{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-fluid/source/css/gitalk.css","path":"css/gitalk.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight-dark.styl","path":"css/highlight-dark.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/fluid.png","path":"img/fluid.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/boot.js","path":"js/boot.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/color-schema.js","path":"js/color-schema.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/events.js","path":"js/events.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/img-lazyload.js","path":"js/img-lazyload.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/leancloud.js","path":"js/leancloud.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/plugins.js","path":"js/plugins.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/umami-view.js","path":"js/umami-view.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/xml/local-search.xml","path":"xml/local-search.xml","modified":0,"renderable":1},{"_id":"source/img/SPVLoc/Network_architecture.png","path":"img/SPVLoc/Network_architecture.png","modified":0,"renderable":0},{"_id":"source/img/SPVLoc/Pose_head.png","path":"img/SPVLoc/Pose_head.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/_posts/SLAM-1.md","hash":"a7d58776b422cb1b4b50122557e7351e49419692","modified":1734668849408},{"_id":"source/_posts/基于视图差的深度估计.md","hash":"1617eabeb34d6f2175a07374fa4d4d739bed811a","modified":1760600379208},{"_id":"source/_posts/操作系统-1.md","hash":"df10f7ae15d9e356eb42bf8786b49db356480c35","modified":1734668849409},{"_id":"source/_posts/操作系统-2.md","hash":"7c43feff87196c36d613e1a55e5f7c1ce6fec81c","modified":1734668849409},{"_id":"source/_posts/数据库-1.md","hash":"96fb863b8ffc9c5e6c3a7cf29d48939c2c463a1d","modified":1734916122460},{"_id":"source/_posts/数据库-2.md","hash":"1e167d14a190f19298e0bee5d43471fa3bf8f819","modified":1734916122460},{"_id":"source/_posts/数据库-3.md","hash":"6c538726116d599b33344767e73584868f8a6ef8","modified":1734916122460},{"_id":"source/_posts/数据库-4.md","hash":"ba3f12e6c6aec86c03d23686d51c3acc49d0da47","modified":1734916122460},{"_id":"source/_posts/数据库-5.md","hash":"c90e95001a9a47b3429c6f6ea79bd77187ae909a","modified":1734916122460},{"_id":"source/_posts/数据库-6.md","hash":"3043dc0a486cff7d29ffcf7a6c7989458d77e557","modified":1734916122460},{"_id":"source/_posts/数据库-7.md","hash":"42ec0c4779b4c59dbdfe2cddaae581f62371590a","modified":1735127961653},{"_id":"source/_posts/数据库-8.md","hash":"56ff22dc7514de1707e20e33c92ff563a0b119e8","modified":1735363969686},{"_id":"source/_posts/软件工程-1.md","hash":"5130e81d656188b210651aa8890eb00ff8b65bd0","modified":1734668849409},{"_id":"source/_posts/软件工程-2.md","hash":"c769cdc4547eb3e7b64638bca89f08bd1c1288ab","modified":1734668849410},{"_id":"source/_posts/软件工程-3.md","hash":"77f7663d47ee6a8726961cee205178f2c52da154","modified":1734668849410},{"_id":"source/_posts/远程连接X11VNC+SSH.md","hash":"f8e16541f0a3d64d64f9050e2399faac52107eb0","modified":1734668849410},{"_id":"source/_posts/项目介绍心得.md","hash":"cf466460595bef278c2f3b5539bdde640dce890d","modified":1734668849410},{"_id":"source/about/index.md","hash":"53a8a778c822d072e741b15d4d6c89fe24431375","modified":1734668849410},{"_id":"source/img/数据库(6)/视图映射.png","hash":"061dfcf7cc696e1986a50fdceca030406f89f138","modified":1734916122468},{"_id":"source/img/软件工程(2)/UML类结构.png","hash":"3422fe834d293eedd06660faa694e5e28584aff5","modified":1734668849433},{"_id":"source/img/软件工程(2)/基本用例与子用例(1).png","hash":"3ed52eea17a6eccd32e2b933f96090d28ee36d15","modified":1734668849438},{"_id":"source/img/软件工程(2)/基本用例与子用例(2).png","hash":"965cd9e7402dc2e2f3762d67edb6279820572815","modified":1734668849438},{"_id":"source/img/数据库(7)/三元关系.png","hash":"e8284e8cd01e820af447f9a8d4fb4b3669effc90","modified":1735125906091},{"_id":"source/img/数据库(7)/复合属性.png","hash":"92562719c6cb72aa6d78fb2b54faf38247dedc53","modified":1735126875689},{"_id":"source/img/数据库(7)/数据库整体设计过程.png","hash":"f06e6db37e1a60fe22ab4ac0b40d54a3282568a8","modified":1735124632178},{"_id":"source/img/数据库(7)/设计过程.png","hash":"3876d15816a18e9276bbdb60b449c0284588b218","modified":1735124632179},{"_id":"source/img/软件工程(2)/UML9个基本图(1).png","hash":"2f1bceaeeafe4502a9d679f1eca1d090403ba898","modified":1734668849429},{"_id":"source/img/软件工程(2)/UML类_继承和实现.png","hash":"c5a7dc3296fe964e171dbb3bb19ba46a9e445a23","modified":1734668849433},{"_id":"source/img/软件工程(2)/UML视图.png","hash":"02536a105266065803a590fc3e0c7ed66761649d","modified":1734668849433},{"_id":"source/img/软件工程(2)/操作契约.png","hash":"82435184a0bc008d9debe51423876d4aa7ac7748","modified":1734668849439},{"_id":"source/img/软件工程(2)/油画关系图.png","hash":"ddb83a5c3c8e9c7e58967f2587e3bac02497b2cd","modified":1734668849439},{"_id":"source/img/软件工程(2)/活动图.png","hash":"ccaf7315032860503c9ac024b5ce98e282526da3","modified":1734668849439},{"_id":"source/img/软件工程(2)/系统顺序图(1).png","hash":"cb82fc4b8a3c714e19497d79d0421ce1b9cdf6da","modified":1734668849443},{"_id":"source/img/数据库(6)/数据库三层次结构.png","hash":"b8b55c9fd57e8fbc13a488f26bc3c69c2fc76e4c","modified":1734916122464},{"_id":"source/img/软件工程(1)/构件组装模型.png","hash":"87ceb3e54f077bed51b0badceba161fbd68154ac","modified":1734668849424},{"_id":"source/img/软件工程(2)/UML类_关联和依赖.png","hash":"05a4370784f3845a7900671480e10108e6af8036","modified":1734668849430},{"_id":"source/img/软件工程(2)/UML类_方法.png","hash":"1c0aa2c9395de0e868dafbf9fa0ed359da3d6469","modified":1734668849431},{"_id":"source/img/软件工程(2)/系统顺序图(2).png","hash":"ce8e6cb13b7d693f20bdbdd13907a882e914e5a8","modified":1734668849443},{"_id":"source/img/操作系统(1)/虚拟机.png","hash":"ced9402d74ece591de3397d341a98efbe93758d1","modified":1734668849413},{"_id":"source/img/数据库(4)/代数关系式.png","hash":"6e6f81f0517e6ce752d85d09280c193aed8440dc","modified":1734916122461},{"_id":"source/img/数据库(5)/表操作顺序.png","hash":"4dd6762c0c088b134367a6f2b4797881bb215e67","modified":1734916122463},{"_id":"source/img/软件工程(1)/快速应用开发模型.png","hash":"aa6704661ee7b36d3f45ae6bba8f90c33aba7e51","modified":1734668849422},{"_id":"source/img/软件工程(1)/演化模型.png","hash":"f9f09b2445315cef690de8296ec835dacc280bdf","modified":1734668849427},{"_id":"source/img/软件工程(1)/软件生命周期.png","hash":"c34e4c7b19ec8cc66887cbd4aea5c3a8faa948e0","modified":1734668849429},{"_id":"source/img/软件工程(2)/UML9个基本图(2).png","hash":"83a752b209354f1ff8087aadf8485a85b6829fc5","modified":1734668849430},{"_id":"source/img/软件工程(2)/UML类_属性.png","hash":"04a43f54e88aa4298c8b85418e91a22c495b1d8e","modified":1734668849430},{"_id":"source/img/软件工程(2)/UML类_组合和聚合.png","hash":"a73959160d378aba7c8970bf3ba23818787fd396","modified":1734668849432},{"_id":"source/img/软件工程(2)/关系类型及其说明以及表示符号.png","hash":"54324cdded8fbe8c9dd82cbcc2b9421f222c7e18","modified":1734668849435},{"_id":"source/img/软件工程(2)/医院挂号处.png","hash":"13c8b13aa8a237613afaa57d55387a90af480168","modified":1734668849437},{"_id":"source/img/软件工程(2)/对象图.png","hash":"45f66022f6f8557763e893962d6fa529fa42a262","modified":1734668849438},{"_id":"source/img/软件工程(2)/状态图.png","hash":"c190598bfd6bfce29ff7522099fd61ed2b315294","modified":1734668849440},{"_id":"source/img/软件工程(2)/组件图.png","hash":"f9327bd4b65cb54347c66c26929f8df4956af057","modified":1734668849444},{"_id":"source/img/软件工程(1)/V模型和W模型.png","hash":"d68f32a9f068b4a4722db4ce8a69bc122a77313f","modified":1734668849421},{"_id":"source/img/软件工程(1)/螺旋模型.png","hash":"41bd09a0bd4c0809ad14242a88dfd718b09fd921","modified":1734668849428},{"_id":"source/img/软件工程(2)/协作图.png","hash":"7a97afc5affe204f543a0a80d3702f4a0aadae68","modified":1734668849437},{"_id":"source/img/软件工程(2)/用例图.png","hash":"9dafd7b2887bec06fdd572ddea2bb96d26b5c712","modified":1734668849441},{"_id":"source/img/软件工程(2)/用例模型基本结构.png","hash":"c074ea9559e1148594fcecb429eb7454140d12e1","modified":1734668849442},{"_id":"source/img/软件工程(2)/类图.png","hash":"4fcabacfbb96613fdddacfc648cdc68d5d3db047","modified":1734668849442},{"_id":"source/img/软件工程(2)/自然界UML图.png","hash":"3f0906562c068e2e76d0a13bc7394428fe8aef69","modified":1734668849444},{"_id":"source/img/软件工程(2)/顺序图.png","hash":"a0ccc15bc127df02d5047886750045dd1083e7e6","modified":1734668849446},{"_id":"source/img/操作系统(1)/操作系统引导.png","hash":"f590a06c604678641abb1f3d971385dc217edf65","modified":1734668849411},{"_id":"source/img/数据库(4)/外键模式图.png","hash":"f757281bbc6d37a43929be63bffdd5bc92b572c9","modified":1734668849418},{"_id":"source/img/数据库(4)/数据库实例.png","hash":"eef5f03b6117840f6d912eacfd620a411c0666fb","modified":1734668849418},{"_id":"source/img/软件工程(1)/增量模型.png","hash":"7a0df1d0d30ef4280cea385ba45284a8dae05d51","modified":1734668849422},{"_id":"source/img/数据库(6)/格式转换_3.png","hash":"874086b911c147dffd3469f3cc2864ec230f20af","modified":1734916122467},{"_id":"source/img/数据库(6)/格式转换_4.png","hash":"6e53d06b2ddd7db0143983623abd365d81d7677b","modified":1734916122468},{"_id":"source/img/数据库(7)/ER_1.png","hash":"60c15d70b0a3bbd040ed57915e0f0031a235ecc5","modified":1735124632178},{"_id":"source/img/软件工程(2)/UML类_类和接口.png","hash":"390028803059bee792c9aa7776facf5ccb3621c5","modified":1734668849432},{"_id":"source/img/软件工程(2)/关联箭头总览.png","hash":"8cd652a5c5a2c21698ce6bddc081e7d08d5b4e8d","modified":1734668849436},{"_id":"source/img/软件工程(2)/部署图.png","hash":"821e8e1d7b781fa2125b4e68f829f5a45259890e","modified":1734668849445},{"_id":"source/img/数据库(5)/数据库关系图.png","hash":"792753ff0181124b452cd05ebae9a8ce82b7d204","modified":1734916122463},{"_id":"source/img/数据库(7)/DBS设计.png","hash":"0777383758fa6b2a1ae2ad81705325fef94332b2","modified":1735124632177},{"_id":"source/img/操作系统(2)/进程的组成.png","hash":"482562701d9a056a8e880562312ff1996e56d5e2","modified":1734668849417},{"_id":"source/img/数据库(4)/模式图.png","hash":"bbec28c777d99642b12c51e698f8988d27131085","modified":1734668849420},{"_id":"source/img/数据库(6)/格式转换_2.png","hash":"8f37489fb213717b1f0b002dec04538a95849320","modified":1734916122466},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1760601227384},{"_id":"source/img/操作系统(2)/进程的特征.png","hash":"d55ab923e2466a7da7abbd5541ffe7b6c34f313d","modified":1734668849417},{"_id":"source/img/数据库(6)/格式转换_1.png","hash":"e9b9ab936d8679e59c643320844aee594fe1e2ff","modified":1734916122465},{"_id":"node_modules/hexo-theme-fluid/LICENSE","hash":"26f9356fd6e84b5a88df6d9014378f41b65ba209","modified":1760601227289},{"_id":"node_modules/hexo-theme-fluid/package.json","hash":"7746460fc2eba7439b494c46aa9b5ded81370819","modified":1760601227367},{"_id":"node_modules/hexo-theme-fluid/languages/de.yml","hash":"58dccef1d98b472dc4e6f4693c2297b0c9c5afba","modified":1760601227385},{"_id":"node_modules/hexo-theme-fluid/README.md","hash":"ff9b0e1fb9dba665af2f1e4a577f8cb9e840464b","modified":1760601227367},{"_id":"node_modules/hexo-theme-fluid/languages/en.yml","hash":"9c580471257f5a32bee701a059a45ea96755dcdc","modified":1760601227385},{"_id":"node_modules/hexo-theme-fluid/languages/eo.yml","hash":"7c1a0c9f6186b6643b19d3980f055329bdb4efa4","modified":1760601227385},{"_id":"node_modules/hexo-theme-fluid/languages/es.yml","hash":"026ddf1a49bf8ddfef6ed86ab4d6af143c1dd95f","modified":1760601227385},{"_id":"node_modules/hexo-theme-fluid/languages/ja.yml","hash":"550b95d3614a64592f02666938d235e9f11e449e","modified":1760601227385},{"_id":"node_modules/hexo-theme-fluid/languages/zh-CN.yml","hash":"a60847136709bb95586a98d9d67b50390a8d2c96","modified":1760601227386},{"_id":"node_modules/hexo-theme-fluid/languages/ru.yml","hash":"93818f8bf07195fb1ebffbb5210e531b0e3a6ec4","modified":1760601227386},{"_id":"node_modules/hexo-theme-fluid/languages/zh-HK.yml","hash":"51c2b4d64c6992a39bfd2586a1bdf5fbbbdf0175","modified":1760601227386},{"_id":"node_modules/hexo-theme-fluid/languages/zh-TW.yml","hash":"e1043de394f6dcf5c0647adcfdefe60637f78426","modified":1760601227386},{"_id":"node_modules/hexo-theme-fluid/layout/about.ejs","hash":"052e9fc19c753f53fdc083c7fb098e3668880140","modified":1760601227301},{"_id":"node_modules/hexo-theme-fluid/layout/archive.ejs","hash":"7c1f44005849791feae4abaa10fae4cb983d3277","modified":1760601227310},{"_id":"node_modules/hexo-theme-fluid/layout/404.ejs","hash":"b84d575c7b7f778b4cb64e89ad3d0aed4a896820","modified":1760601227299},{"_id":"node_modules/hexo-theme-fluid/layout/categories.ejs","hash":"13859726c27b6c79b5876ec174176d0f9c1ee164","modified":1760601227316},{"_id":"node_modules/hexo-theme-fluid/_config.yml","hash":"e62d2e25cae57e8469e3f48c9d17be1fd284a969","modified":1760601227385},{"_id":"node_modules/hexo-theme-fluid/layout/category.ejs","hash":"f099161b738a16a32253f42085b5444f902018ed","modified":1760601227320},{"_id":"node_modules/hexo-theme-fluid/layout/index.ejs","hash":"33c3317cdcee062789de2336dd8d0cc7f86d3650","modified":1760601227338},{"_id":"node_modules/hexo-theme-fluid/layout/layout.ejs","hash":"7e0023474128fbe4d68c467704c41f1712432415","modified":1760601227339},{"_id":"node_modules/hexo-theme-fluid/layout/links.ejs","hash":"1cac32ec4579aaf7b9fa39d317497331d4c5e1dd","modified":1760601227339},{"_id":"node_modules/hexo-theme-fluid/layout/page.ejs","hash":"ed5007a3feb8f14d3d2843271bfb298eb0c56219","modified":1760601227346},{"_id":"node_modules/hexo-theme-fluid/layout/tag.ejs","hash":"9d686364c4d16a1a9219471623af452035c5b966","modified":1760601227350},{"_id":"node_modules/hexo-theme-fluid/layout/post.ejs","hash":"9bf0d357a607a282f3b9cb04525a4df0cc2a8b76","modified":1760601227347},{"_id":"node_modules/hexo-theme-fluid/layout/tags.ejs","hash":"1d06af34b6cf1d8a20d2eb565e309326ceba309f","modified":1760601227351},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/archive-list.ejs","hash":"7520fbf91f762207c2ab06b2c293235cd5b23905","modified":1760601227309},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/category-chains.ejs","hash":"18309584aab83bc4deb20723ebad832149dd2e24","modified":1760601227319},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments.ejs","hash":"d707c47b2638c94e489bc43d4cfd098b7c58447f","modified":1760601227323},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/category-list.ejs","hash":"f8d2f1907450e61968e6d54443e9be8138196a77","modified":1760601227319},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/footer.ejs","hash":"40c8b0852873032e7aaef3f68e8ea08706cdef13","modified":1760601227333},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/css.ejs","hash":"1dadb118d580280524ed0a5f69bd34d234a92276","modified":1760601227326},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/head.ejs","hash":"67be642f99482c07904474f410cfbc2f99003288","modified":1760601227336},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/header.ejs","hash":"0d5e397d30051e5fbabe7b47cfd1f1e6a5820af1","modified":1760601227337},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/markdown-plugins.ejs","hash":"fc4bdf7de0cf1a66d0e5e4fba1b31d6f7ed49468","modified":1760601227340},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/paginator.ejs","hash":"0f38a2c238169edcb63fc46c23bfc529ff3859b7","modified":1760601227346},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/scripts.ejs","hash":"da5810785105e5075861593c7ac22c7aa9665a72","modified":1760601227348},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/default-injects.js","hash":"b2013ae8e189cd07ebc8a2ff48a78e153345210f","modified":1760601227356},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/locals.js","hash":"58d0fec976f6b1d35e7ea03edc45414088acf05c","modified":1760601227362},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/index-generator.js","hash":"9159fc22fa84a7b605dd15fe4104f01fe9c71147","modified":1760601227360},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/post-filter.js","hash":"82bb06686158ebe160a631c79f156cd4fde35656","modified":1760601227364},{"_id":"node_modules/hexo-theme-fluid/scripts/events/index.js","hash":"79de5a379b28cad759a49048351c7f6b8915bd7d","modified":1760601227360},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/search.ejs","hash":"70e1c929e084ca8a2648cedabf29b372511ea2b8","modified":1760601227348},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/local-search.js","hash":"9ac5ddad06e9b0e6015ce531430018182a4bc0fa","modified":1760601227362},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/pages.js","hash":"d3e75f53c59674d171309e50702954671f31f1a4","modified":1760601227363},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/date.js","hash":"9bda6382f61b40a20c24af466fe10c8366ebb74c","modified":1760601227356},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/engine.js","hash":"d3a231d106795ce99cb0bc77eb65f9ae44515933","modified":1760601227357},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/import.js","hash":"ca53e8dbf7d44cfd372cfa79ac60f35a7d5b0076","modified":1760601227359},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/injects.js","hash":"1ad2ae6b11bd8806ee7dd6eb7140d8b54a95d613","modified":1760601227361},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/export-config.js","hash":"8e67b522c47aa250860e3fe2c733f1f958a506c0","modified":1760601227357},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/page.js","hash":"4607607445233b3029ef20ed5e91de0da0a7f9c5","modified":1760601227363},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/scope.js","hash":"d41d9d658fcb54964b388598e996747aadb85b0f","modified":1760601227364},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/url.js","hash":"2a6a8288176d0e0f6ec008056bf2745a86e8943e","modified":1760601227365},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/utils.js","hash":"966689d7c5e4320008285395fbaa2751f6209be5","modified":1760601227365},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/wordcount.js","hash":"4d48c424e47ff9a17a563167ea5f480890267adf","modified":1760601227367},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/button.js","hash":"3eb43a8cdea0a64576ad6b31b4df6c2bf5698d4c","modified":1760601227355},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/checkbox.js","hash":"6eaf53cf4bfc756a65bda18184cf8998a12c861d","modified":1760601227355},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/group-image.js","hash":"4aeebb797026f1df25646a5d69f7fde79b1bcd26","modified":1760601227358},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/label.js","hash":"f05a6d32cca79535b22907dc03edb9d3fa2d8176","modified":1760601227361},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/fold.js","hash":"73e4fd12ce3e47981479391ed354b7d9d3279f70","modified":1760601227357},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/mermaid.js","hash":"75160561e1ef3603b6d2ad2938464ab1cb77fd38","modified":1760601227362},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/note.js","hash":"e3b456a079e5dc0032473b516c865b20f83d2c26","modified":1760601227363},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/crypto.js","hash":"ae4ad8a188ef5b3fa6818b01629fc962b3de8551","modified":1760601227356},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/compare-versions.js","hash":"dbbc928c914fc2bd242cd66aa0c45971aec13a5d","modified":1760601227355},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/object.js","hash":"33b57e4decdc5e75c518859f168c8ba80b2c665b","modified":1760601227363},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/resolve.js","hash":"8c4a8b62aa8608f12f1e9046231dff04859dc3e9","modified":1760601227364},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/url-join.js","hash":"718aab5e7b2059a06b093ca738de420d9afa44ba","modified":1760601227364},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight-dark.styl","hash":"45695ef75c31a4aa57324dd408b7e2327a337018","modified":1760601227376},{"_id":"node_modules/hexo-theme-fluid/source/css/main.styl","hash":"855ae5fe229c51afa57f7645f6997a27a705d7e4","modified":1760601227378},{"_id":"node_modules/hexo-theme-fluid/source/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1760601227297},{"_id":"node_modules/hexo-theme-fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1760601227367},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight.styl","hash":"a9efc52a646a9e585439c768557e3e3c9e3326dc","modified":1760601227377},{"_id":"node_modules/hexo-theme-fluid/source/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1760601227369},{"_id":"node_modules/hexo-theme-fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1760601227369},{"_id":"node_modules/hexo-theme-fluid/source/js/boot.js","hash":"38bd26c6b7acdafda86dda3560e6a3ca488d3c76","modified":1760601227354},{"_id":"node_modules/hexo-theme-fluid/source/js/color-schema.js","hash":"1ef88c881b9f942deadde3d890387b94c617342a","modified":1760601227355},{"_id":"node_modules/hexo-theme-fluid/source/js/events.js","hash":"6869811f67e4c3de3edfa4b08464bb242b97a402","modified":1760601227357},{"_id":"node_modules/hexo-theme-fluid/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1760601227353},{"_id":"node_modules/hexo-theme-fluid/source/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1760601227359},{"_id":"node_modules/hexo-theme-fluid/source/js/leancloud.js","hash":"eff77c7a5c399fcaefda48884980571e15243fc9","modified":1760601227361},{"_id":"node_modules/hexo-theme-fluid/source/js/plugins.js","hash":"c34916291e392a774ff3e85c55badb83e8661297","modified":1760601227363},{"_id":"node_modules/hexo-theme-fluid/source/js/umami-view.js","hash":"33c4b3883fa747604074ad3921606eeeaeb50716","modified":1760601227364},{"_id":"node_modules/hexo-theme-fluid/source/js/local-search.js","hash":"b9945f76f8682f3ec32edfb285b26eb559f7b7e8","modified":1760601227362},{"_id":"node_modules/hexo-theme-fluid/source/js/utils.js","hash":"b82e7c289a66dfd36064470fd41c0e96fc598b43","modified":1760601227366},{"_id":"node_modules/hexo-theme-fluid/source/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1760601227384},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/changyan.ejs","hash":"c9b2d68ed3d375f1953e7007307d2a3f75ed6249","modified":1760601227322},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/cusdis.ejs","hash":"5f9dc012be27040bbe874d0c093c0d53958cc987","modified":1760601227327},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/discuss.ejs","hash":"98d065b58ce06b7d18bff3c974e96fa0f34ae03a","modified":1760601227329},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/disqus.ejs","hash":"aab4a4d24c55231a37db308ae94414319cecdd9b","modified":1760601227330},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/giscus.ejs","hash":"95f8b866b158eff9352c381c243b332a155a5110","modified":1760601227335},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/gitalk.ejs","hash":"843bc141a4545eb20d1c92fb63c85d459b4271ec","modified":1760601227336},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/remark42.ejs","hash":"d4e9532feeb02aed61bd15eda536b5b631454dac","modified":1760601227347},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/twikoo.ejs","hash":"d84bcb5ccd78470a60c067fc914ac0ac67ac8777","modified":1760601227352},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/utterances.ejs","hash":"c7ccf7f28308334a6da6f5425b141a24b5eca0e2","modified":1760601227352},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/valine.ejs","hash":"19ba937553dddd317f827d682661a1066a7b1f30","modified":1760601227353},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/waline.ejs","hash":"3d08c73b77e412d2f06a24d9344565fc7dbc76f8","modified":1760601227353},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/footer/statistics.ejs","hash":"954a29b58d72647d20450da270b5d8fb2e0824f5","modified":1760601227350},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/footer/beian.ejs","hash":"4fb9b5dd3f3e41a586d6af44e5069afe7c81fff2","modified":1760601227314},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/header/banner.ejs","hash":"e07757b59e7b89eea213d0e595cb5932f812fd32","modified":1760601227312},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/header/navigation.ejs","hash":"37d750428772d7c71ba36ce0c2540780d90fadea","modified":1760601227345},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/anchorjs.ejs","hash":"40181442d3a2b8734783a0ad7caf2d2522e3f2ab","modified":1760601227308},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/code-widget.ejs","hash":"3a505cba37942badf62a56bbb8b605b72af330aa","modified":1760601227323},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/fancybox.ejs","hash":"9d1ea2a46b8c8ad8c168594d578f40764818ef13","modified":1760601227332},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/analytics.ejs","hash":"e6dcbf1c2f56314d56bb46b50aca86ff68cacebd","modified":1760601227304},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/encrypt.ejs","hash":"0fff24cf5bf99fbe5c56c292e2eac4a89bf29db4","modified":1760601227331},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/highlight.ejs","hash":"7529dd215b09d3557804333942377b9e20fa554e","modified":1760601227338},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/math.ejs","hash":"dcbf9a381ee76f2f1f75fcbc22c50a502ec85023","modified":1760601227341},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/mermaid.ejs","hash":"03ac02762f801970d1c4e73d6ec8d4c503780e50","modified":1760601227342},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/typed.ejs","hash":"f345374885cd6a334f09a11f59c443b5d577c06c","modified":1760601227352},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/nprogress.ejs","hash":"4c2d39ce816b8a6dcd6b53113c8695f8bd650a23","modified":1760601227346},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/moment.ejs","hash":"4ff3fb1b60ccc95a0af3bbdbd0757fedefc088b5","modified":1760601227344},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/category-bar.ejs","hash":"8772bce97ed297e7a88523f4e939ed6436c22f87","modified":1760601227317},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/copyright.ejs","hash":"cbfa32c5f5973133afd043853b24f8200455cb2d","modified":1760601227325},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/meta-bottom.ejs","hash":"375974ec017696e294dc12469fb0ae257800dc2d","modified":1760601227343},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/meta-top.ejs","hash":"54dd479dbb440126e4ddd9d902229db5afaaae98","modified":1760601227344},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/sidebar-left.ejs","hash":"9992c99b3eb728ad195970e1b84d665f2c8691c4","modified":1760601227349},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/toc.ejs","hash":"635a89060fbf72eeda066fc4bd0a97462f069417","modified":1760601227351},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/sidebar-right.ejs","hash":"d5fcc9b60e02f869a29a8c17a16a6028ecc1e6d8","modified":1760601227349},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/compatible-configs.js","hash":"ef474d1fa5bbafc52619ced0f9dc7eaf2affb363","modified":1760601227356},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/footnote.js","hash":"c19ac8050b82c3676b0332a56099ccfcc36d9d52","modified":1760601227358},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/injects.js","hash":"5ae4b07204683e54b5a1b74e931702bbce2ac23e","modified":1760601227360},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/highlight.js","hash":"a5fe1deccb73b5f578797dbb11038efc15f63ce8","modified":1760601227359},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/hello.js","hash":"bd8376e1cf7892dc2daa58f2f443574be559fdbf","modified":1760601227359},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/lazyload.js","hash":"9ba0d4bc224e22af8a5a48d6ff13e5a0fcfee2a4","modified":1760601227361},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/merge-configs.js","hash":"7c944c43b2ece5dd84859bd9d1fe955d13427387","modified":1760601227362},{"_id":"node_modules/hexo-theme-fluid/source/css/_mixins/base.styl","hash":"542e306ee9494e8a78e44d6d7d409605d94caeb3","modified":1760601227371},{"_id":"node_modules/hexo-theme-fluid/source/css/_functions/base.styl","hash":"2e46f3f4e2c9fe34c1ff1c598738fc7349ae8188","modified":1760601227371},{"_id":"node_modules/hexo-theme-fluid/source/css/_variables/base.styl","hash":"4ed5f0ae105ef4c7dd92eaf652ceda176c38e502","modified":1760601227371},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/pages.styl","hash":"b8e887bc7fb3b765a1f8ec9448eff8603a41984f","modified":1760601227380},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/base.styl","hash":"643284c567665f96915f0b64e59934dda315f74d","modified":1760601227371},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_about/about.styl","hash":"97fe42516ea531fdad771489b68aa8b2a7f6ae46","modified":1760601227369},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_archive/archive.styl","hash":"c475e6681546d30350eaed11f23081ecae80c375","modified":1760601227370},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/color-schema.styl","hash":"85492ef64d7e5f70f0f7e46d570bbc911e686d7e","modified":1760601227374},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/keyframes.styl","hash":"94065ea50f5bef7566d184f2422f6ac20866ba22","modified":1760601227378},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/print.styl","hash":"166afbc596ea4b552bad7290ec372d25ec34db7b","modified":1760601227382},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/inline.styl","hash":"411a3fa3f924a87e00ff04d18b5c83283b049a4d","modified":1760601227377},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/category-chain.styl","hash":"0cdf7ef50dfd0669d3b257821384ff31cd81b7c9","modified":1760601227372},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/category-bar.styl","hash":"cc6df43fef6bb3efecbfdd8b9e467424a1dea581","modified":1760601227372},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_index/index.styl","hash":"25fb6fa4c783b847c632584c49a7e1593cdb2f5d","modified":1760601227377},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/category-list.styl","hash":"7edfe1b571ecca7d08f5f4dbcf76f4ffdcfbf0b5","modified":1760601227373},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_links/links.styl","hash":"5c7f2044e3f1da05a3229537c06bd879836f8d6e","modified":1760601227378},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/comment.styl","hash":"780f3788e7357bcd3f3262d781cb91bb53976a93","modified":1760601227374},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/highlight.styl","hash":"4df764d298fe556e501db4afc2b05686fe6ebcfb","modified":1760601227376},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/post-page.styl","hash":"7eee3f78296a3c81849a5415d1d43dcc6e03e6aa","modified":1760601227381},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/markdown.styl","hash":"1e3d3a82721e7c10bcfcecec6d81cf2979039452","modified":1760601227378},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_tag/tags.styl","hash":"65bfc01c76abc927fa1a23bf2422892b0d566c3f","modified":1760601227384},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/post-tag.styl","hash":"c96d36aa8fe20f0c3c1a29ee2473cd8064b10f73","modified":1760601227381},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/banner.styl","hash":"7a0bd629bc234fc75e3cc8e3715ffada92f09e73","modified":1760601227370},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/anchorjs.styl","hash":"e0cebda4a6f499aff75e71417d88caa7ceb13b94","modified":1760601227370},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/board.styl","hash":"4397037fc3f0033dbe546c33cd9dbdabd8cb1632","modified":1760601227372},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/code-widget.styl","hash":"b66ab013f0f37d724a149b85b3c7432afcf460ad","modified":1760601227373},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/copyright.styl","hash":"26f71a9cd60d96bb0cb5bbdf58150b8e524d9707","modified":1760601227374},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/footnote.styl","hash":"ae9289cc89649af2042907f8a003303b987f3404","modified":1760601227375},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/footer.styl","hash":"2caaca71dd1ff63d583099ed817677dd267b457e","modified":1760601227375},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/header.styl","hash":"d42b748f2f49ef32aafb1a21d75991d2459da927","modified":1760601227376},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/modal.styl","hash":"adf6c1e5c8e1fb41c77ce6e2258001df61245aa2","modified":1760601227379},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/pagination.styl","hash":"8bb1b68e5f3552cb48c2ffa31edbc53646a8fb4c","modified":1760601227380},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/ngrogress.styl","hash":"5d225357b4a58d46118e6616377168336ed44cb2","modified":1760601227379},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/noscript.styl","hash":"0cf2f2bb44f456150d428016675d5876a9d2e2aa","modified":1760601227380},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/qrcode.styl","hash":"78704a94c0436097abfb0e0a57abeb3429c749b7","modified":1760601227382},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/scroll-btn.styl","hash":"f0e429a27fa8a7658fcbddbb4d4dbe4afa12499a","modified":1760601227383},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/search.styl","hash":"10f7e91a91e681fb9fe46f9df7707b9ef78707c8","modified":1760601227383},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/toc.styl","hash":"9e7452aa2372153f25d7a4675c9d36d281a65d24","modified":1760601227384},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/livere.ejs","hash":"2264758fed57542a7389c7aa9f00f1aefa17eb87","modified":1760601227340},{"_id":"source/img/软件工程(2)/关系箭头总览.png","hash":"ca11f897027c1b8e21ce21b01dbce0002f40f90b","modified":1734668849434},{"_id":"source/img/操作系统(2)/PCB.png","hash":"08f82fdf91488e84e11cf5122f6706f879f5c31b","modified":1734668849414},{"_id":"source/img/软件工程(1)/模型开发特定及适用场合.png","hash":"a83fbb1bbfd032b77d5be956448879cd2216ccbf","modified":1734668849427},{"_id":"node_modules/hexo-theme-fluid/source/img/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1760601227368},{"_id":"public/local-search.xml","hash":"7f8b7d60c547bfdce94d4b2e6864acbc15e7db72","modified":1760976774512},{"_id":"public/about/index.html","hash":"ccb2ada52b6127594e14de32c47809a44ae41533","modified":1760601283460},{"_id":"public/2024/12/27/数据库-8/index.html","hash":"11e3f7899dcea70ab1f46c4d2b97d621d68f4d87","modified":1760601283460},{"_id":"public/2024/12/22/数据库-7/index.html","hash":"2288c67693d057c2cc89580aa28d275001c3abfb","modified":1760601283460},{"_id":"public/2024/12/22/数据库-6/index.html","hash":"0216081f96ed8b480cba3a547effd969ca878608","modified":1760601283460},{"_id":"public/2024/12/21/数据库-5/index.html","hash":"de8ca382e3cc96d97f8b5a33390740045dea424f","modified":1760601283460},{"_id":"public/2024/12/20/数据库-4/index.html","hash":"d38024b305dd252753fb2d4925ce1a701f1e7004","modified":1760601283460},{"_id":"public/2024/11/01/项目介绍心得/index.html","hash":"ca8ca00ab82c576436a179c152a510baf213188a","modified":1760601283460},{"_id":"public/2024/11/01/数据库-3/index.html","hash":"1d24657b875c6de7b058d13c2fc7ebab2eb07c58","modified":1760601283460},{"_id":"public/2024/10/31/软件工程-3/index.html","hash":"aa0ea6ab956ec01b1d5bde5681f85a9bfafa0b33","modified":1760601283460},{"_id":"public/2024/10/24/操作系统-2/index.html","hash":"fbc2269f6e763f3e4fe1446b47a09a56cb37e7f6","modified":1760601283460},{"_id":"public/2024/10/23/软件工程-2/index.html","hash":"e25360a0c09ac1e7d823c1c82e8fd3575c4f2b3c","modified":1760601283460},{"_id":"public/2024/10/23/基于视图差的深度估计/index.html","hash":"a7bb53cdea683784ee64f9408565cbe49800a318","modified":1760601283460},{"_id":"public/2024/10/23/软件工程-1/index.html","hash":"e8ce35aa524dc8aa28258051a4731820f1701787","modified":1760601283460},{"_id":"public/2024/10/18/操作系统-1/index.html","hash":"6df272111e6d096025d1cc8457df51668bf65832","modified":1760601283460},{"_id":"public/2024/10/17/远程连接X11VNC+SSH/index.html","hash":"901f446afbcf9ccac04b5caefb09a20807cec9c1","modified":1760601283460},{"_id":"public/2024/10/05/数据库-2/index.html","hash":"955bc8cd75d25a7fe2a3854dc0af5ef08598732e","modified":1760601283460},{"_id":"public/2024/10/03/SLAM-1/index.html","hash":"d75a551fc01c31effc596464202b59b42f622dc8","modified":1760976774512},{"_id":"public/2024/10/01/数据库-1/index.html","hash":"b5abefcb4fb2d8425f8e7fadf222d93d4f24e7f1","modified":1760601283460},{"_id":"public/archives/index.html","hash":"ef288694e5882c71fcec9d28523abaf33be2f1ef","modified":1760976774512},{"_id":"public/archives/page/2/index.html","hash":"707adad91896b66a57f1375ea71b25bc3acc46f8","modified":1760601283460},{"_id":"public/archives/2024/index.html","hash":"b0222d9765803ee3053a6968820b6d552c7072fa","modified":1760976774512},{"_id":"public/archives/2024/page/2/index.html","hash":"92123b22b29dd4bb925d6770819b917a9fb09d9f","modified":1760601283460},{"_id":"public/archives/2024/10/index.html","hash":"95b9e11bc1da0542f0231ced6babc975c1e6e98d","modified":1760976774512},{"_id":"public/archives/2024/11/index.html","hash":"8f3c600d1053a237f6ac72b40b7597538fcf5cc8","modified":1760601283460},{"_id":"public/archives/2024/12/index.html","hash":"a773742a6e73949d460c183031caec10869f5b97","modified":1760601283460},{"_id":"public/index.html","hash":"3e7d11895e92a44d020f19c85d9149b6388f400c","modified":1760976774512},{"_id":"public/page/2/index.html","hash":"a0901835b1091a5b551de0250b3d5d12afe74edb","modified":1760601283460},{"_id":"public/tags/三维视觉/index.html","hash":"8293aea460e515d0d37042df56c1e68d4e0c0b27","modified":1760976774512},{"_id":"public/tags/SLAM/index.html","hash":"ea2437e7b954971ed2f1403c8760a28bf4cb2529","modified":1760601283460},{"_id":"public/tags/学习笔记/index.html","hash":"c51a120714c4c57862f9a6a8adad85b95fc75b01","modified":1760976774512},{"_id":"public/tags/学习笔记/page/2/index.html","hash":"0ea1a60b14e9b826f8b67a85d33a51e99afcb788","modified":1760601283460},{"_id":"public/tags/深度估计/index.html","hash":"c5c710992c6912a56a15efee7f175ae61a64e1f4","modified":1760976774512},{"_id":"public/tags/论文阅读总结/index.html","hash":"12c2e1eaa7bbb5ad52932019ef162a48d46c8794","modified":1760601283460},{"_id":"public/tags/数据库/index.html","hash":"7d083a571d7ebc4f658d381f4e4295c58dae8480","modified":1760601283460},{"_id":"public/tags/Archlinux/index.html","hash":"8fe24d137cde1cf21cec259e636512eb7c02d1f9","modified":1760601283460},{"_id":"public/404.html","hash":"4c853b6ae3f2a18b773ae3179cc48aa350f2fa2d","modified":1760601283460},{"_id":"public/tags/index.html","hash":"780fc7ef13b0ba452e86b39fc61701faefa123a5","modified":1760976774512},{"_id":"public/categories/index.html","hash":"684e84d7f0501250153fe8ae2099fd0563a47067","modified":1760601283460},{"_id":"public/links/index.html","hash":"fb496eb182c5e84f1b93030e9561a03b1bc175f6","modified":1760601283460},{"_id":"public/img/数据库(6)/视图映射.png","hash":"061dfcf7cc696e1986a50fdceca030406f89f138","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML类结构.png","hash":"3422fe834d293eedd06660faa694e5e28584aff5","modified":1760601283460},{"_id":"public/img/软件工程(2)/基本用例与子用例(1).png","hash":"3ed52eea17a6eccd32e2b933f96090d28ee36d15","modified":1760601283460},{"_id":"public/img/软件工程(2)/基本用例与子用例(2).png","hash":"965cd9e7402dc2e2f3762d67edb6279820572815","modified":1760601283460},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1760601283460},{"_id":"public/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1760601283460},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1760601283460},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1760601283460},{"_id":"public/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1760601283460},{"_id":"public/img/数据库(7)/三元关系.png","hash":"e8284e8cd01e820af447f9a8d4fb4b3669effc90","modified":1760601283460},{"_id":"public/img/数据库(7)/复合属性.png","hash":"92562719c6cb72aa6d78fb2b54faf38247dedc53","modified":1760601283460},{"_id":"public/img/数据库(7)/数据库整体设计过程.png","hash":"f06e6db37e1a60fe22ab4ac0b40d54a3282568a8","modified":1760601283460},{"_id":"public/img/数据库(7)/设计过程.png","hash":"3876d15816a18e9276bbdb60b449c0284588b218","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML9个基本图(1).png","hash":"2f1bceaeeafe4502a9d679f1eca1d090403ba898","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML类_继承和实现.png","hash":"c5a7dc3296fe964e171dbb3bb19ba46a9e445a23","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML视图.png","hash":"02536a105266065803a590fc3e0c7ed66761649d","modified":1760601283460},{"_id":"public/img/软件工程(2)/操作契约.png","hash":"82435184a0bc008d9debe51423876d4aa7ac7748","modified":1760601283460},{"_id":"public/img/软件工程(2)/油画关系图.png","hash":"ddb83a5c3c8e9c7e58967f2587e3bac02497b2cd","modified":1760601283460},{"_id":"public/img/软件工程(2)/活动图.png","hash":"ccaf7315032860503c9ac024b5ce98e282526da3","modified":1760601283460},{"_id":"public/img/软件工程(2)/系统顺序图(1).png","hash":"cb82fc4b8a3c714e19497d79d0421ce1b9cdf6da","modified":1760601283460},{"_id":"public/css/highlight.css","hash":"04d4ddbb5e1d1007447c2fe293ee05aae9b9563e","modified":1760601283460},{"_id":"public/css/highlight-dark.css","hash":"902294bada4323c0f51502d67cba8c3a0298952f","modified":1760601283460},{"_id":"public/css/main.css","hash":"14ebd9b515085666cee29bbcbe362ad3604ab62a","modified":1760601283460},{"_id":"public/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1760601283460},{"_id":"public/js/boot.js","hash":"38bd26c6b7acdafda86dda3560e6a3ca488d3c76","modified":1760601283460},{"_id":"public/js/color-schema.js","hash":"1ef88c881b9f942deadde3d890387b94c617342a","modified":1760601283460},{"_id":"public/js/events.js","hash":"6869811f67e4c3de3edfa4b08464bb242b97a402","modified":1760601283460},{"_id":"public/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1760601283460},{"_id":"public/js/leancloud.js","hash":"eff77c7a5c399fcaefda48884980571e15243fc9","modified":1760601283460},{"_id":"public/js/local-search.js","hash":"b9945f76f8682f3ec32edfb285b26eb559f7b7e8","modified":1760601283460},{"_id":"public/js/plugins.js","hash":"c34916291e392a774ff3e85c55badb83e8661297","modified":1760601283460},{"_id":"public/js/umami-view.js","hash":"33c4b3883fa747604074ad3921606eeeaeb50716","modified":1760601283460},{"_id":"public/js/utils.js","hash":"b82e7c289a66dfd36064470fd41c0e96fc598b43","modified":1760601283460},{"_id":"public/img/数据库(6)/数据库三层次结构.png","hash":"b8b55c9fd57e8fbc13a488f26bc3c69c2fc76e4c","modified":1760601283460},{"_id":"public/img/软件工程(1)/构件组装模型.png","hash":"87ceb3e54f077bed51b0badceba161fbd68154ac","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML类_关联和依赖.png","hash":"05a4370784f3845a7900671480e10108e6af8036","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML类_方法.png","hash":"1c0aa2c9395de0e868dafbf9fa0ed359da3d6469","modified":1760601283460},{"_id":"public/img/软件工程(2)/系统顺序图(2).png","hash":"ce8e6cb13b7d693f20bdbdd13907a882e914e5a8","modified":1760601283460},{"_id":"public/img/操作系统(1)/虚拟机.png","hash":"ced9402d74ece591de3397d341a98efbe93758d1","modified":1760601283460},{"_id":"public/img/数据库(4)/代数关系式.png","hash":"6e6f81f0517e6ce752d85d09280c193aed8440dc","modified":1760601283460},{"_id":"public/img/数据库(5)/表操作顺序.png","hash":"4dd6762c0c088b134367a6f2b4797881bb215e67","modified":1760601283460},{"_id":"public/img/软件工程(1)/快速应用开发模型.png","hash":"aa6704661ee7b36d3f45ae6bba8f90c33aba7e51","modified":1760601283460},{"_id":"public/img/软件工程(1)/演化模型.png","hash":"f9f09b2445315cef690de8296ec835dacc280bdf","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML9个基本图(2).png","hash":"83a752b209354f1ff8087aadf8485a85b6829fc5","modified":1760601283460},{"_id":"public/img/软件工程(1)/软件生命周期.png","hash":"c34e4c7b19ec8cc66887cbd4aea5c3a8faa948e0","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML类_属性.png","hash":"04a43f54e88aa4298c8b85418e91a22c495b1d8e","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML类_组合和聚合.png","hash":"a73959160d378aba7c8970bf3ba23818787fd396","modified":1760601283460},{"_id":"public/img/软件工程(2)/关系类型及其说明以及表示符号.png","hash":"54324cdded8fbe8c9dd82cbcc2b9421f222c7e18","modified":1760601283460},{"_id":"public/img/软件工程(2)/医院挂号处.png","hash":"13c8b13aa8a237613afaa57d55387a90af480168","modified":1760601283460},{"_id":"public/img/软件工程(2)/对象图.png","hash":"45f66022f6f8557763e893962d6fa529fa42a262","modified":1760601283460},{"_id":"public/img/软件工程(2)/状态图.png","hash":"c190598bfd6bfce29ff7522099fd61ed2b315294","modified":1760601283460},{"_id":"public/img/软件工程(2)/组件图.png","hash":"f9327bd4b65cb54347c66c26929f8df4956af057","modified":1760601283460},{"_id":"public/img/软件工程(1)/V模型和W模型.png","hash":"d68f32a9f068b4a4722db4ce8a69bc122a77313f","modified":1760601283460},{"_id":"public/img/软件工程(1)/螺旋模型.png","hash":"41bd09a0bd4c0809ad14242a88dfd718b09fd921","modified":1760601283460},{"_id":"public/img/软件工程(2)/协作图.png","hash":"7a97afc5affe204f543a0a80d3702f4a0aadae68","modified":1760601283460},{"_id":"public/img/软件工程(2)/用例图.png","hash":"9dafd7b2887bec06fdd572ddea2bb96d26b5c712","modified":1760601283460},{"_id":"public/img/软件工程(2)/用例模型基本结构.png","hash":"c074ea9559e1148594fcecb429eb7454140d12e1","modified":1760601283460},{"_id":"public/img/软件工程(2)/类图.png","hash":"4fcabacfbb96613fdddacfc648cdc68d5d3db047","modified":1760601283460},{"_id":"public/img/软件工程(2)/自然界UML图.png","hash":"3f0906562c068e2e76d0a13bc7394428fe8aef69","modified":1760601283460},{"_id":"public/img/软件工程(2)/顺序图.png","hash":"a0ccc15bc127df02d5047886750045dd1083e7e6","modified":1760601283460},{"_id":"public/img/操作系统(1)/操作系统引导.png","hash":"f590a06c604678641abb1f3d971385dc217edf65","modified":1760601283460},{"_id":"public/img/数据库(4)/外键模式图.png","hash":"f757281bbc6d37a43929be63bffdd5bc92b572c9","modified":1760601283460},{"_id":"public/img/数据库(4)/数据库实例.png","hash":"eef5f03b6117840f6d912eacfd620a411c0666fb","modified":1760601283460},{"_id":"public/img/软件工程(1)/增量模型.png","hash":"7a0df1d0d30ef4280cea385ba45284a8dae05d51","modified":1760601283460},{"_id":"public/img/数据库(6)/格式转换_3.png","hash":"874086b911c147dffd3469f3cc2864ec230f20af","modified":1760601283460},{"_id":"public/img/数据库(6)/格式转换_4.png","hash":"6e53d06b2ddd7db0143983623abd365d81d7677b","modified":1760601283460},{"_id":"public/img/数据库(7)/ER_1.png","hash":"60c15d70b0a3bbd040ed57915e0f0031a235ecc5","modified":1760601283460},{"_id":"public/img/软件工程(2)/UML类_类和接口.png","hash":"390028803059bee792c9aa7776facf5ccb3621c5","modified":1760601283460},{"_id":"public/img/软件工程(2)/关联箭头总览.png","hash":"8cd652a5c5a2c21698ce6bddc081e7d08d5b4e8d","modified":1760601283460},{"_id":"public/img/软件工程(2)/部署图.png","hash":"821e8e1d7b781fa2125b4e68f829f5a45259890e","modified":1760601283460},{"_id":"public/img/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1760601283460},{"_id":"public/img/数据库(5)/数据库关系图.png","hash":"792753ff0181124b452cd05ebae9a8ce82b7d204","modified":1760601283460},{"_id":"public/img/数据库(7)/DBS设计.png","hash":"0777383758fa6b2a1ae2ad81705325fef94332b2","modified":1760601283460},{"_id":"public/img/操作系统(2)/进程的组成.png","hash":"482562701d9a056a8e880562312ff1996e56d5e2","modified":1760601283460},{"_id":"public/img/数据库(4)/模式图.png","hash":"bbec28c777d99642b12c51e698f8988d27131085","modified":1760601283460},{"_id":"public/img/数据库(6)/格式转换_2.png","hash":"8f37489fb213717b1f0b002dec04538a95849320","modified":1760601283460},{"_id":"public/img/操作系统(2)/进程的特征.png","hash":"d55ab923e2466a7da7abbd5541ffe7b6c34f313d","modified":1760601283460},{"_id":"public/img/数据库(6)/格式转换_1.png","hash":"e9b9ab936d8679e59c643320844aee594fe1e2ff","modified":1760601283460},{"_id":"public/img/软件工程(2)/关系箭头总览.png","hash":"ca11f897027c1b8e21ce21b01dbce0002f40f90b","modified":1760601283460},{"_id":"public/img/操作系统(2)/PCB.png","hash":"08f82fdf91488e84e11cf5122f6706f879f5c31b","modified":1760601283460},{"_id":"public/img/软件工程(1)/模型开发特定及适用场合.png","hash":"a83fbb1bbfd032b77d5be956448879cd2216ccbf","modified":1760601283460},{"_id":"source/_posts/SPVLoc.md","hash":"80ec3a080d2137655550ddba4c187e5b0ee797eb","modified":1760977510928},{"_id":"source/_posts/基于多步视差的深度估计.md","hash":"870019819e3d137974ca08e59502483981805113","modified":1760935131819},{"_id":"source/img/SPVLoc/Pose_head.png","hash":"51181c40b12155d02f87282bfc0d13bd595d042c","modified":1760883553979},{"_id":"source/img/SPVLoc/Network_architecture.png","hash":"3c3aceb0f8a3c63d4b48b8e666576932ef008076","modified":1760881385402},{"_id":"public/2025/10/17/SPVLoc/index.html","hash":"7ea424e60f5b8396528ae35ea010a7a45b9abeb1","modified":1760976774512},{"_id":"public/2025/10/16/基于多步视差的深度估计/index.html","hash":"05a174e09ee036ca9d405d3edaed967552c5a0dc","modified":1760976774512},{"_id":"public/archives/2025/index.html","hash":"7ea4124cd9db9a965adc7c3b49039e4b3d4c436f","modified":1760976774512},{"_id":"public/archives/2025/10/index.html","hash":"5213d026b1dfc4b99e10e4db38446e1095dc204c","modified":1760976774512},{"_id":"public/img/SPVLoc/Pose_head.png","hash":"51181c40b12155d02f87282bfc0d13bd595d042c","modified":1760976774512},{"_id":"public/img/SPVLoc/Network_architecture.png","hash":"3c3aceb0f8a3c63d4b48b8e666576932ef008076","modified":1760976774512}],"Category":[],"Data":[],"Page":[{"title":"about","date":"2024-09-30T11:49:36.000Z","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-09-30 19:49:36\nlayout: about\n---\n","updated":"2024-12-20T04:27:29.410Z","path":"about/index.html","comments":1,"_id":"cmgt4jot100006d6e35zg355t","content":"","excerpt":"","more":""}],"Post":[{"title":"SLAM(1)","date":"2024-10-03T02:45:51.000Z","_content":"本文主要介绍SLAM最基础的纯视觉版本 ORB-SLAM。该版本SLAM通过单目相机，提取坐标中的特征点，循环检测以实现较为精准的定位。\n### 补充知识\n#### 词袋模型\n对于一张图片进行词袋模型建立，需要一下三步:\n1. 对于图像进行整体特征检测，投射入特征空间，然后进行聚类。\n2. 对于聚类产生的类，取每个类的中心为一个单词。\n3. 每次增加的图像增加的特征点加入特征空间，然后根据其对于各个类的距离进行分类.\n   \n由此实现对每张图的词袋表示。\n#### 基于词袋模型的图像检索\n\n对于图片的词袋模型搜索，常常采用TF-IDF(Term frequency(词频因子)-inverse document frequency(逆文档频率因子))\n- 文档 *i* 中出现频率越高的词，其重要性越高。\n- 数据库中包含单词 *j* 的文档越多，该单词对于文档的区分性越低，重要性也就越低。\n$$\n    t_j=\\frac{n_{ij}}{n_i} \\log(\\frac{N}{n_j})\n$$\n在整个图像搜索过程中，采用倒排索引方法，索引值是特征编号，而不是图像编号。相对于图像所以，单词的数量在一定阈值后不会随着图像的增加而增加。故而其索引数量从图像数量N变成了特征点数量 $n_j$ 。同时在检索之后一般进行空间验证，以保证获取的点具有几何一致性，最终获取重排结果。\n\n#### 优化问题\n**ORB_SLAM**中的典型优化函数：\n1. **GlobalBundleAdjustment** 除第一帧摄像机位置外，全部的地图点和关键帧相机优化(初始化)\n2. **LocalBundleAdjustment** 优化部分地图点和关键帧的位姿\n3. **PoseOptimization** 仅优化单帧摄像位姿(3D-2D重投影最小化、运动跟踪、参考帧跟踪、局部地图跟踪、重定位)\n4. **OptimizeEssentialGraph** 本质图优化全部相机位姿(回环检测)\n5. **OptimizeSim3** 优化两个关键帧和关键帧共有的地图点(回环检测)\n#### 生成树\n对于连通图进行遍历，过程中所经过的边和顶点的组合可以看做是一个普通树，通常成为生成树。\n连通图中的生成树必须满足一下两个条件：\n1. 包含连通图中的所有顶点\n2. 任意两条顶点之间有且只有一条通路\n\n### SLAM介绍\n**Simultaneous Localization and Mapping (SLAM)**\n- Localization：传感器的位置和姿态\n- Mapping：地图构建\n- SLAM：同时定位和建图\n- 应用场景：定位，导航，避障，重建，交互\n\n在整个地图构建的过程中，根据地图的特点可以分为：拓扑地图、2D栅格地图、3D网格地图、3D点云地图\n  \n### ORB-SLAM详解\n在**ORB-SLAM**中\n- 一般会同时运行三个线程：\n  - 跟踪：确定当前帧位姿\n  - 建图：完成局部地图构建\n  - 回环修整：回环检测以及基于回环信息修正系统漂移\n- 数据中心中，会存储\n  1. 视觉词典\n  2. 关键帧词袋数据库。\n- 数据中心存储的数据包含\n  1. 地图点\n       - 世界坐标系下的3D坐标\n       - 观测方向，即所有可以观测到该特征点的视图所产生的观测方向均值\n       - 最具表达性的ORB特征描述子\n       - 该点能被观测到的最大距离和最小距离\n  2. 关键帧\n       - 摄像机位姿\n       - 内参数\n       - 全部ORB特征描述符，是否有地图点对应\n  3. 共视图\n       - 一种无向有全图，节点为关键帧，如果两个阶段共享的地图数大于阈值(至少15个)则存在一条边，其权重设置为共享地图的个数\n  4. 本质图\n       - 共视图的子图，保留所有节点，边数量相较于共视图更少，其作用是加速会还矫正的计算\n        $本质图 = 生成树 + 共视图边权重超过100的边 + 回环边$\n#### 跟踪\n**目标**: 构建初始的地图点云，第一个摄像机作为世界坐标系。\n  1. 计算特征点匹配，匹配数要大于预定义门限\n  2. 同时计算基础矩阵F与单应矩阵H(注：匹配点可能来自同一平面)\n  3. 分别计算F和H误差，选择最合理的矩阵\n  4. 对F或者H进行分解，三角化重构出初始地图点\n  5. 使用**GlobalBA**进行优化，即关键帧位姿与重构的地图点都参与优化\n\n**基于前一帧的估计**：\n  1. 如果前一帧跟踪成功，利用上一帧的运动模型估计当前帧的相机位姿\n  2. 依据估计的R和T，将前一帧观测到的地图点投影到当前帧\n  3. 如果成功找到足够多的点，则依照2D-3D对应点计算相机位姿(EPnP)；如果失败则转到全局重定位。  \n  4. 在全局重定位中，计算当前帧的词袋，在数据库中进行检索，找到匹配度大于某个门限的候选参考关键帧，并对于每个候选参考关键帧进行特征点匹配和PnP计算操作，实现迭代优化，求解计算机位姿\n  5. \n**局部地图跟踪：**\n  1. 寻找候选“局部地图点”\n     - 建立共视图中与当前帧相练的关键帧集合$K_1$以及与$K_1$相连的关键帧集合$K_2$\n     - $K_1+K_2$看到的全部地图点构成\"局部地图点\"集合\n     - $K_1$与当前帧相似度最高的设为$K_{ref}$\n  2. 筛选地图点\n     - 无法引申到当前帧的地图点舍弃\n     - 当前点视图方向和该地图点的平均视图方向$n$满足$v*n < \\cos(60^\\circ)$ \n     - 计算地图点到该帧摄像机中心的距离$d$,如果$d \\notin [d_{min}, d_{max}],则该地图点舍弃$\n     - 计算尺度$\\frac{d}{d_{min}}$，在该帧中，对于所有未匹配的ORB特征描述子，检测在该尺度下是否有3D点的投影与其匹配，若匹配则将该未匹配的特征描述子与3D点建立联系\n  3. 使用所有未被筛选掉的2D-3D点对应当前帧进行位姿优化\n\n**关键帧需要满足的条件：**\n  1. 距离上次全局重定位已经经历了超过20帧(刚重定位的帧位姿可能不够准确)\n  2.  局部地图线程空闲\n  3.  或者距离上次关键帧插入超过了20帧(场景变化)\n  4.  当前至少有50个关键点(该帧特征充足，可信度高)\n  5.  当前帧和$K_{ref}$共视点相似比小于90%\n\n#### 建图\n**关键帧插入：**\n- 更新共视图，增加新的节点$K_i$，更新共视图链接关系\n- 更新生成树的节点关系\n- 计算该新增关键帧的词袋表示，为新建地图点做准备\n\n**地图点剔除**\n- 最近增加的地图点必须通过如下测试，才可以留在地图中\n  - 实际观测比例大于理论的25%\n  - 地图点被创建后，再经过三个关键帧都可以观测到它\n- 通过测试过的地图点，只在如下情况时会被删除\n  - 任何时间下观测它的关键帧个数小于3(通常发生在删除关键帧时) \n\n**对于当前帧$K_I$中没有被特征匹配的特征点进行如下处理：**\n  - 在共视图中选取共视程度最高且基线宽度大于某阈值的关键帧进行特征点匹配和三角化，对重构结果进行尺度、重投影误差、视差(不易过小)、正向深度检验。\n  - 通过检验的点再投影到其他视图中，能匹配成功则建立“地图点-关键帧特征点”的对应关系。\n\n**优化局部地图**\n  - 当前帧(位姿)与共视关键帧(位姿)，及其它们看到的所有地图点(空间坐标)，参与局部BA优化。\n  - 所有能看到上述地图点单与当前关键帧没有共视关系的关键帧参与局部BA，但它们的位姿在优化过程中固定不变。  \n\n**局部关键帧剔除**\n当场景不发生变化时，控制关键帧的数量。如果当前90%的地图点都可以被至少其他三个关键帧在相同或者更精细的尺度观测到，则该关键帧被剔除。\n\n#### 回环修整\n**回环候选帧检测**\n  1. 回环候选关键帧产生\n    - 在共视图中计算$K_i$与所邻居的BOW向量相似性，保存最低分数，记作$s_{min}$\n    - 分数低于$s_{min}$的帧不做备选，与当前帧$K_i$直接相连的帧不做备选\n  2. 确认回环关键帧\n   \n**对于每一个候选回环关键帧$K_j$**\n  1. 计算其与当前关键帧$K_i$的特征匹配，建立3D-3D点对应\n  2. 利用RANSAC方法估计其与当前帧的相似变换$S_{ij}$(7个自由度)\n  3. 利用$S_{ij}$搜索两帧之间更多点对应\n  4. 基于点对应优化$S_{ij}$直至有足够多内点，则接受当前候选回环关键帧，并记为$K_i$，其相似变换记为$S_{jl}$\n\n**回环融合**\n  1. 基于相似性矩阵$S_{il}$修正当前帧$K_i$(及其共视帧位姿)\n  2. 特征点匹配与地图匹配融合：将回环帧$K_i$及其共视帧可见的地图点投影到当前帧$K_i$及其共视关键帧，寻找匹配并融合\n  3. 连接关系更新:融合过程中涉及到的关键帧的边均需要更新，包括回环边\n  - 每条边的误差为:\n    $$\n    e_{ij} = \\log(S_{ij}S_{jw}S_{iw}^{-1})\n    $$\n  - 本质图上的总误差为：\n    $$\n    C = \\sum_{i,j}{e_{i,j}^T\\Lambda{e_{ij}}}    \\\\\n    其中，{\\Lambda}_{i,j}为单位矩阵\n    $$\n\n\n","source":"_posts/SLAM-1.md","raw":"---\ntitle: SLAM(1)\ndate: 2024-10-03 10:45:51\ntags:\n    - 三维视觉\n    - SLAM\n    - 学习笔记\n---\n本文主要介绍SLAM最基础的纯视觉版本 ORB-SLAM。该版本SLAM通过单目相机，提取坐标中的特征点，循环检测以实现较为精准的定位。\n### 补充知识\n#### 词袋模型\n对于一张图片进行词袋模型建立，需要一下三步:\n1. 对于图像进行整体特征检测，投射入特征空间，然后进行聚类。\n2. 对于聚类产生的类，取每个类的中心为一个单词。\n3. 每次增加的图像增加的特征点加入特征空间，然后根据其对于各个类的距离进行分类.\n   \n由此实现对每张图的词袋表示。\n#### 基于词袋模型的图像检索\n\n对于图片的词袋模型搜索，常常采用TF-IDF(Term frequency(词频因子)-inverse document frequency(逆文档频率因子))\n- 文档 *i* 中出现频率越高的词，其重要性越高。\n- 数据库中包含单词 *j* 的文档越多，该单词对于文档的区分性越低，重要性也就越低。\n$$\n    t_j=\\frac{n_{ij}}{n_i} \\log(\\frac{N}{n_j})\n$$\n在整个图像搜索过程中，采用倒排索引方法，索引值是特征编号，而不是图像编号。相对于图像所以，单词的数量在一定阈值后不会随着图像的增加而增加。故而其索引数量从图像数量N变成了特征点数量 $n_j$ 。同时在检索之后一般进行空间验证，以保证获取的点具有几何一致性，最终获取重排结果。\n\n#### 优化问题\n**ORB_SLAM**中的典型优化函数：\n1. **GlobalBundleAdjustment** 除第一帧摄像机位置外，全部的地图点和关键帧相机优化(初始化)\n2. **LocalBundleAdjustment** 优化部分地图点和关键帧的位姿\n3. **PoseOptimization** 仅优化单帧摄像位姿(3D-2D重投影最小化、运动跟踪、参考帧跟踪、局部地图跟踪、重定位)\n4. **OptimizeEssentialGraph** 本质图优化全部相机位姿(回环检测)\n5. **OptimizeSim3** 优化两个关键帧和关键帧共有的地图点(回环检测)\n#### 生成树\n对于连通图进行遍历，过程中所经过的边和顶点的组合可以看做是一个普通树，通常成为生成树。\n连通图中的生成树必须满足一下两个条件：\n1. 包含连通图中的所有顶点\n2. 任意两条顶点之间有且只有一条通路\n\n### SLAM介绍\n**Simultaneous Localization and Mapping (SLAM)**\n- Localization：传感器的位置和姿态\n- Mapping：地图构建\n- SLAM：同时定位和建图\n- 应用场景：定位，导航，避障，重建，交互\n\n在整个地图构建的过程中，根据地图的特点可以分为：拓扑地图、2D栅格地图、3D网格地图、3D点云地图\n  \n### ORB-SLAM详解\n在**ORB-SLAM**中\n- 一般会同时运行三个线程：\n  - 跟踪：确定当前帧位姿\n  - 建图：完成局部地图构建\n  - 回环修整：回环检测以及基于回环信息修正系统漂移\n- 数据中心中，会存储\n  1. 视觉词典\n  2. 关键帧词袋数据库。\n- 数据中心存储的数据包含\n  1. 地图点\n       - 世界坐标系下的3D坐标\n       - 观测方向，即所有可以观测到该特征点的视图所产生的观测方向均值\n       - 最具表达性的ORB特征描述子\n       - 该点能被观测到的最大距离和最小距离\n  2. 关键帧\n       - 摄像机位姿\n       - 内参数\n       - 全部ORB特征描述符，是否有地图点对应\n  3. 共视图\n       - 一种无向有全图，节点为关键帧，如果两个阶段共享的地图数大于阈值(至少15个)则存在一条边，其权重设置为共享地图的个数\n  4. 本质图\n       - 共视图的子图，保留所有节点，边数量相较于共视图更少，其作用是加速会还矫正的计算\n        $本质图 = 生成树 + 共视图边权重超过100的边 + 回环边$\n#### 跟踪\n**目标**: 构建初始的地图点云，第一个摄像机作为世界坐标系。\n  1. 计算特征点匹配，匹配数要大于预定义门限\n  2. 同时计算基础矩阵F与单应矩阵H(注：匹配点可能来自同一平面)\n  3. 分别计算F和H误差，选择最合理的矩阵\n  4. 对F或者H进行分解，三角化重构出初始地图点\n  5. 使用**GlobalBA**进行优化，即关键帧位姿与重构的地图点都参与优化\n\n**基于前一帧的估计**：\n  1. 如果前一帧跟踪成功，利用上一帧的运动模型估计当前帧的相机位姿\n  2. 依据估计的R和T，将前一帧观测到的地图点投影到当前帧\n  3. 如果成功找到足够多的点，则依照2D-3D对应点计算相机位姿(EPnP)；如果失败则转到全局重定位。  \n  4. 在全局重定位中，计算当前帧的词袋，在数据库中进行检索，找到匹配度大于某个门限的候选参考关键帧，并对于每个候选参考关键帧进行特征点匹配和PnP计算操作，实现迭代优化，求解计算机位姿\n  5. \n**局部地图跟踪：**\n  1. 寻找候选“局部地图点”\n     - 建立共视图中与当前帧相练的关键帧集合$K_1$以及与$K_1$相连的关键帧集合$K_2$\n     - $K_1+K_2$看到的全部地图点构成\"局部地图点\"集合\n     - $K_1$与当前帧相似度最高的设为$K_{ref}$\n  2. 筛选地图点\n     - 无法引申到当前帧的地图点舍弃\n     - 当前点视图方向和该地图点的平均视图方向$n$满足$v*n < \\cos(60^\\circ)$ \n     - 计算地图点到该帧摄像机中心的距离$d$,如果$d \\notin [d_{min}, d_{max}],则该地图点舍弃$\n     - 计算尺度$\\frac{d}{d_{min}}$，在该帧中，对于所有未匹配的ORB特征描述子，检测在该尺度下是否有3D点的投影与其匹配，若匹配则将该未匹配的特征描述子与3D点建立联系\n  3. 使用所有未被筛选掉的2D-3D点对应当前帧进行位姿优化\n\n**关键帧需要满足的条件：**\n  1. 距离上次全局重定位已经经历了超过20帧(刚重定位的帧位姿可能不够准确)\n  2.  局部地图线程空闲\n  3.  或者距离上次关键帧插入超过了20帧(场景变化)\n  4.  当前至少有50个关键点(该帧特征充足，可信度高)\n  5.  当前帧和$K_{ref}$共视点相似比小于90%\n\n#### 建图\n**关键帧插入：**\n- 更新共视图，增加新的节点$K_i$，更新共视图链接关系\n- 更新生成树的节点关系\n- 计算该新增关键帧的词袋表示，为新建地图点做准备\n\n**地图点剔除**\n- 最近增加的地图点必须通过如下测试，才可以留在地图中\n  - 实际观测比例大于理论的25%\n  - 地图点被创建后，再经过三个关键帧都可以观测到它\n- 通过测试过的地图点，只在如下情况时会被删除\n  - 任何时间下观测它的关键帧个数小于3(通常发生在删除关键帧时) \n\n**对于当前帧$K_I$中没有被特征匹配的特征点进行如下处理：**\n  - 在共视图中选取共视程度最高且基线宽度大于某阈值的关键帧进行特征点匹配和三角化，对重构结果进行尺度、重投影误差、视差(不易过小)、正向深度检验。\n  - 通过检验的点再投影到其他视图中，能匹配成功则建立“地图点-关键帧特征点”的对应关系。\n\n**优化局部地图**\n  - 当前帧(位姿)与共视关键帧(位姿)，及其它们看到的所有地图点(空间坐标)，参与局部BA优化。\n  - 所有能看到上述地图点单与当前关键帧没有共视关系的关键帧参与局部BA，但它们的位姿在优化过程中固定不变。  \n\n**局部关键帧剔除**\n当场景不发生变化时，控制关键帧的数量。如果当前90%的地图点都可以被至少其他三个关键帧在相同或者更精细的尺度观测到，则该关键帧被剔除。\n\n#### 回环修整\n**回环候选帧检测**\n  1. 回环候选关键帧产生\n    - 在共视图中计算$K_i$与所邻居的BOW向量相似性，保存最低分数，记作$s_{min}$\n    - 分数低于$s_{min}$的帧不做备选，与当前帧$K_i$直接相连的帧不做备选\n  2. 确认回环关键帧\n   \n**对于每一个候选回环关键帧$K_j$**\n  1. 计算其与当前关键帧$K_i$的特征匹配，建立3D-3D点对应\n  2. 利用RANSAC方法估计其与当前帧的相似变换$S_{ij}$(7个自由度)\n  3. 利用$S_{ij}$搜索两帧之间更多点对应\n  4. 基于点对应优化$S_{ij}$直至有足够多内点，则接受当前候选回环关键帧，并记为$K_i$，其相似变换记为$S_{jl}$\n\n**回环融合**\n  1. 基于相似性矩阵$S_{il}$修正当前帧$K_i$(及其共视帧位姿)\n  2. 特征点匹配与地图匹配融合：将回环帧$K_i$及其共视帧可见的地图点投影到当前帧$K_i$及其共视关键帧，寻找匹配并融合\n  3. 连接关系更新:融合过程中涉及到的关键帧的边均需要更新，包括回环边\n  - 每条边的误差为:\n    $$\n    e_{ij} = \\log(S_{ij}S_{jw}S_{iw}^{-1})\n    $$\n  - 本质图上的总误差为：\n    $$\n    C = \\sum_{i,j}{e_{i,j}^T\\Lambda{e_{ij}}}    \\\\\n    其中，{\\Lambda}_{i,j}为单位矩阵\n    $$\n\n\n","slug":"SLAM-1","published":1,"updated":"2024-12-20T04:27:29.408Z","comments":1,"layout":"post","photos":[],"_id":"cmgt4jot200016d6e9boq76ca","content":"<p>本文主要介绍SLAM最基础的纯视觉版本 ORB-SLAM。该版本SLAM通过单目相机，提取坐标中的特征点，循环检测以实现较为精准的定位。</p>\n<h3 id=\"补充知识\"><a href=\"#补充知识\" class=\"headerlink\" title=\"补充知识\"></a>补充知识</h3><h4 id=\"词袋模型\"><a href=\"#词袋模型\" class=\"headerlink\" title=\"词袋模型\"></a>词袋模型</h4><p>对于一张图片进行词袋模型建立，需要一下三步:</p>\n<ol>\n<li>对于图像进行整体特征检测，投射入特征空间，然后进行聚类。</li>\n<li>对于聚类产生的类，取每个类的中心为一个单词。</li>\n<li>每次增加的图像增加的特征点加入特征空间，然后根据其对于各个类的距离进行分类.</li>\n</ol>\n<p>由此实现对每张图的词袋表示。</p>\n<h4 id=\"基于词袋模型的图像检索\"><a href=\"#基于词袋模型的图像检索\" class=\"headerlink\" title=\"基于词袋模型的图像检索\"></a>基于词袋模型的图像检索</h4><p>对于图片的词袋模型搜索，常常采用TF-IDF(Term frequency(词频因子)-inverse document frequency(逆文档频率因子))</p>\n<ul>\n<li>文档 <em>i</em> 中出现频率越高的词，其重要性越高。</li>\n<li>数据库中包含单词 <em>j</em> 的文档越多，该单词对于文档的区分性越低，重要性也就越低。<br>$$<br>  t_j&#x3D;\\frac{n_{ij}}{n_i} \\log(\\frac{N}{n_j})<br>$$<br>在整个图像搜索过程中，采用倒排索引方法，索引值是特征编号，而不是图像编号。相对于图像所以，单词的数量在一定阈值后不会随着图像的增加而增加。故而其索引数量从图像数量N变成了特征点数量 $n_j$ 。同时在检索之后一般进行空间验证，以保证获取的点具有几何一致性，最终获取重排结果。</li>\n</ul>\n<h4 id=\"优化问题\"><a href=\"#优化问题\" class=\"headerlink\" title=\"优化问题\"></a>优化问题</h4><p><strong>ORB_SLAM</strong>中的典型优化函数：</p>\n<ol>\n<li><strong>GlobalBundleAdjustment</strong> 除第一帧摄像机位置外，全部的地图点和关键帧相机优化(初始化)</li>\n<li><strong>LocalBundleAdjustment</strong> 优化部分地图点和关键帧的位姿</li>\n<li><strong>PoseOptimization</strong> 仅优化单帧摄像位姿(3D-2D重投影最小化、运动跟踪、参考帧跟踪、局部地图跟踪、重定位)</li>\n<li><strong>OptimizeEssentialGraph</strong> 本质图优化全部相机位姿(回环检测)</li>\n<li><strong>OptimizeSim3</strong> 优化两个关键帧和关键帧共有的地图点(回环检测)</li>\n</ol>\n<h4 id=\"生成树\"><a href=\"#生成树\" class=\"headerlink\" title=\"生成树\"></a>生成树</h4><p>对于连通图进行遍历，过程中所经过的边和顶点的组合可以看做是一个普通树，通常成为生成树。<br>连通图中的生成树必须满足一下两个条件：</p>\n<ol>\n<li>包含连通图中的所有顶点</li>\n<li>任意两条顶点之间有且只有一条通路</li>\n</ol>\n<h3 id=\"SLAM介绍\"><a href=\"#SLAM介绍\" class=\"headerlink\" title=\"SLAM介绍\"></a>SLAM介绍</h3><p><strong>Simultaneous Localization and Mapping (SLAM)</strong></p>\n<ul>\n<li>Localization：传感器的位置和姿态</li>\n<li>Mapping：地图构建</li>\n<li>SLAM：同时定位和建图</li>\n<li>应用场景：定位，导航，避障，重建，交互</li>\n</ul>\n<p>在整个地图构建的过程中，根据地图的特点可以分为：拓扑地图、2D栅格地图、3D网格地图、3D点云地图</p>\n<h3 id=\"ORB-SLAM详解\"><a href=\"#ORB-SLAM详解\" class=\"headerlink\" title=\"ORB-SLAM详解\"></a>ORB-SLAM详解</h3><p>在<strong>ORB-SLAM</strong>中</p>\n<ul>\n<li>一般会同时运行三个线程：<ul>\n<li>跟踪：确定当前帧位姿</li>\n<li>建图：完成局部地图构建</li>\n<li>回环修整：回环检测以及基于回环信息修正系统漂移</li>\n</ul>\n</li>\n<li>数据中心中，会存储<ol>\n<li>视觉词典</li>\n<li>关键帧词袋数据库。</li>\n</ol>\n</li>\n<li>数据中心存储的数据包含<ol>\n<li>地图点<ul>\n<li>世界坐标系下的3D坐标</li>\n<li>观测方向，即所有可以观测到该特征点的视图所产生的观测方向均值</li>\n<li>最具表达性的ORB特征描述子</li>\n<li>该点能被观测到的最大距离和最小距离</li>\n</ul>\n</li>\n<li>关键帧<ul>\n<li>摄像机位姿</li>\n<li>内参数</li>\n<li>全部ORB特征描述符，是否有地图点对应</li>\n</ul>\n</li>\n<li>共视图<ul>\n<li>一种无向有全图，节点为关键帧，如果两个阶段共享的地图数大于阈值(至少15个)则存在一条边，其权重设置为共享地图的个数</li>\n</ul>\n</li>\n<li>本质图<ul>\n<li>共视图的子图，保留所有节点，边数量相较于共视图更少，其作用是加速会还矫正的计算<br>   $本质图 &#x3D; 生成树 + 共视图边权重超过100的边 + 回环边$</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h4 id=\"跟踪\"><a href=\"#跟踪\" class=\"headerlink\" title=\"跟踪\"></a>跟踪</h4><p><strong>目标</strong>: 构建初始的地图点云，第一个摄像机作为世界坐标系。</p>\n<ol>\n<li>计算特征点匹配，匹配数要大于预定义门限</li>\n<li>同时计算基础矩阵F与单应矩阵H(注：匹配点可能来自同一平面)</li>\n<li>分别计算F和H误差，选择最合理的矩阵</li>\n<li>对F或者H进行分解，三角化重构出初始地图点</li>\n<li>使用<strong>GlobalBA</strong>进行优化，即关键帧位姿与重构的地图点都参与优化</li>\n</ol>\n<p><strong>基于前一帧的估计</strong>：</p>\n<ol>\n<li>如果前一帧跟踪成功，利用上一帧的运动模型估计当前帧的相机位姿</li>\n<li>依据估计的R和T，将前一帧观测到的地图点投影到当前帧</li>\n<li>如果成功找到足够多的点，则依照2D-3D对应点计算相机位姿(EPnP)；如果失败则转到全局重定位。  </li>\n<li>在全局重定位中，计算当前帧的词袋，在数据库中进行检索，找到匹配度大于某个门限的候选参考关键帧，并对于每个候选参考关键帧进行特征点匹配和PnP计算操作，实现迭代优化，求解计算机位姿</li>\n<li><strong>局部地图跟踪：</strong></li>\n<li>寻找候选“局部地图点”<ul>\n<li>建立共视图中与当前帧相练的关键帧集合$K_1$以及与$K_1$相连的关键帧集合$K_2$</li>\n<li>$K_1+K_2$看到的全部地图点构成”局部地图点”集合</li>\n<li>$K_1$与当前帧相似度最高的设为$K_{ref}$</li>\n</ul>\n</li>\n<li>筛选地图点<ul>\n<li>无法引申到当前帧的地图点舍弃</li>\n<li>当前点视图方向和该地图点的平均视图方向$n$满足$v*n &lt; \\cos(60^\\circ)$ </li>\n<li>计算地图点到该帧摄像机中心的距离$d$,如果$d \\notin [d_{min}, d_{max}],则该地图点舍弃$</li>\n<li>计算尺度$\\frac{d}{d_{min}}$，在该帧中，对于所有未匹配的ORB特征描述子，检测在该尺度下是否有3D点的投影与其匹配，若匹配则将该未匹配的特征描述子与3D点建立联系</li>\n</ul>\n</li>\n<li>使用所有未被筛选掉的2D-3D点对应当前帧进行位姿优化</li>\n</ol>\n<p><strong>关键帧需要满足的条件：</strong></p>\n<ol>\n<li>距离上次全局重定位已经经历了超过20帧(刚重定位的帧位姿可能不够准确)</li>\n<li>局部地图线程空闲</li>\n<li>或者距离上次关键帧插入超过了20帧(场景变化)</li>\n<li>当前至少有50个关键点(该帧特征充足，可信度高)</li>\n<li>当前帧和$K_{ref}$共视点相似比小于90%</li>\n</ol>\n<h4 id=\"建图\"><a href=\"#建图\" class=\"headerlink\" title=\"建图\"></a>建图</h4><p><strong>关键帧插入：</strong></p>\n<ul>\n<li>更新共视图，增加新的节点$K_i$，更新共视图链接关系</li>\n<li>更新生成树的节点关系</li>\n<li>计算该新增关键帧的词袋表示，为新建地图点做准备</li>\n</ul>\n<p><strong>地图点剔除</strong></p>\n<ul>\n<li>最近增加的地图点必须通过如下测试，才可以留在地图中<ul>\n<li>实际观测比例大于理论的25%</li>\n<li>地图点被创建后，再经过三个关键帧都可以观测到它</li>\n</ul>\n</li>\n<li>通过测试过的地图点，只在如下情况时会被删除<ul>\n<li>任何时间下观测它的关键帧个数小于3(通常发生在删除关键帧时)</li>\n</ul>\n</li>\n</ul>\n<p><strong>对于当前帧$K_I$中没有被特征匹配的特征点进行如下处理：</strong></p>\n<ul>\n<li>在共视图中选取共视程度最高且基线宽度大于某阈值的关键帧进行特征点匹配和三角化，对重构结果进行尺度、重投影误差、视差(不易过小)、正向深度检验。</li>\n<li>通过检验的点再投影到其他视图中，能匹配成功则建立“地图点-关键帧特征点”的对应关系。</li>\n</ul>\n<p><strong>优化局部地图</strong></p>\n<ul>\n<li>当前帧(位姿)与共视关键帧(位姿)，及其它们看到的所有地图点(空间坐标)，参与局部BA优化。</li>\n<li>所有能看到上述地图点单与当前关键帧没有共视关系的关键帧参与局部BA，但它们的位姿在优化过程中固定不变。</li>\n</ul>\n<p><strong>局部关键帧剔除</strong><br>当场景不发生变化时，控制关键帧的数量。如果当前90%的地图点都可以被至少其他三个关键帧在相同或者更精细的尺度观测到，则该关键帧被剔除。</p>\n<h4 id=\"回环修整\"><a href=\"#回环修整\" class=\"headerlink\" title=\"回环修整\"></a>回环修整</h4><p><strong>回环候选帧检测</strong></p>\n<ol>\n<li>回环候选关键帧产生<br>- 在共视图中计算$K_i$与所邻居的BOW向量相似性，保存最低分数，记作$s_{min}$<br>- 分数低于$s_{min}$的帧不做备选，与当前帧$K_i$直接相连的帧不做备选</li>\n<li>确认回环关键帧</li>\n</ol>\n<p><strong>对于每一个候选回环关键帧$K_j$</strong></p>\n<ol>\n<li>计算其与当前关键帧$K_i$的特征匹配，建立3D-3D点对应</li>\n<li>利用RANSAC方法估计其与当前帧的相似变换$S_{ij}$(7个自由度)</li>\n<li>利用$S_{ij}$搜索两帧之间更多点对应</li>\n<li>基于点对应优化$S_{ij}$直至有足够多内点，则接受当前候选回环关键帧，并记为$K_i$，其相似变换记为$S_{jl}$</li>\n</ol>\n<p><strong>回环融合</strong></p>\n<ol>\n<li>基于相似性矩阵$S_{il}$修正当前帧$K_i$(及其共视帧位姿)</li>\n<li>特征点匹配与地图匹配融合：将回环帧$K_i$及其共视帧可见的地图点投影到当前帧$K_i$及其共视关键帧，寻找匹配并融合</li>\n<li>连接关系更新:融合过程中涉及到的关键帧的边均需要更新，包括回环边</li>\n</ol>\n<ul>\n<li>每条边的误差为:<br>$$<br>e_{ij} &#x3D; \\log(S_{ij}S_{jw}S_{iw}^{-1})<br>$$</li>\n<li>本质图上的总误差为：<br>$$<br>C &#x3D; \\sum_{i,j}{e_{i,j}^T\\Lambda{e_{ij}}}    \\<br>其中，{\\Lambda}_{i,j}为单位矩阵<br>$$</li>\n</ul>\n","excerpt":"","more":"<p>本文主要介绍SLAM最基础的纯视觉版本 ORB-SLAM。该版本SLAM通过单目相机，提取坐标中的特征点，循环检测以实现较为精准的定位。</p>\n<h3 id=\"补充知识\"><a href=\"#补充知识\" class=\"headerlink\" title=\"补充知识\"></a>补充知识</h3><h4 id=\"词袋模型\"><a href=\"#词袋模型\" class=\"headerlink\" title=\"词袋模型\"></a>词袋模型</h4><p>对于一张图片进行词袋模型建立，需要一下三步:</p>\n<ol>\n<li>对于图像进行整体特征检测，投射入特征空间，然后进行聚类。</li>\n<li>对于聚类产生的类，取每个类的中心为一个单词。</li>\n<li>每次增加的图像增加的特征点加入特征空间，然后根据其对于各个类的距离进行分类.</li>\n</ol>\n<p>由此实现对每张图的词袋表示。</p>\n<h4 id=\"基于词袋模型的图像检索\"><a href=\"#基于词袋模型的图像检索\" class=\"headerlink\" title=\"基于词袋模型的图像检索\"></a>基于词袋模型的图像检索</h4><p>对于图片的词袋模型搜索，常常采用TF-IDF(Term frequency(词频因子)-inverse document frequency(逆文档频率因子))</p>\n<ul>\n<li>文档 <em>i</em> 中出现频率越高的词，其重要性越高。</li>\n<li>数据库中包含单词 <em>j</em> 的文档越多，该单词对于文档的区分性越低，重要性也就越低。<br>$$<br>  t_j&#x3D;\\frac{n_{ij}}{n_i} \\log(\\frac{N}{n_j})<br>$$<br>在整个图像搜索过程中，采用倒排索引方法，索引值是特征编号，而不是图像编号。相对于图像所以，单词的数量在一定阈值后不会随着图像的增加而增加。故而其索引数量从图像数量N变成了特征点数量 $n_j$ 。同时在检索之后一般进行空间验证，以保证获取的点具有几何一致性，最终获取重排结果。</li>\n</ul>\n<h4 id=\"优化问题\"><a href=\"#优化问题\" class=\"headerlink\" title=\"优化问题\"></a>优化问题</h4><p><strong>ORB_SLAM</strong>中的典型优化函数：</p>\n<ol>\n<li><strong>GlobalBundleAdjustment</strong> 除第一帧摄像机位置外，全部的地图点和关键帧相机优化(初始化)</li>\n<li><strong>LocalBundleAdjustment</strong> 优化部分地图点和关键帧的位姿</li>\n<li><strong>PoseOptimization</strong> 仅优化单帧摄像位姿(3D-2D重投影最小化、运动跟踪、参考帧跟踪、局部地图跟踪、重定位)</li>\n<li><strong>OptimizeEssentialGraph</strong> 本质图优化全部相机位姿(回环检测)</li>\n<li><strong>OptimizeSim3</strong> 优化两个关键帧和关键帧共有的地图点(回环检测)</li>\n</ol>\n<h4 id=\"生成树\"><a href=\"#生成树\" class=\"headerlink\" title=\"生成树\"></a>生成树</h4><p>对于连通图进行遍历，过程中所经过的边和顶点的组合可以看做是一个普通树，通常成为生成树。<br>连通图中的生成树必须满足一下两个条件：</p>\n<ol>\n<li>包含连通图中的所有顶点</li>\n<li>任意两条顶点之间有且只有一条通路</li>\n</ol>\n<h3 id=\"SLAM介绍\"><a href=\"#SLAM介绍\" class=\"headerlink\" title=\"SLAM介绍\"></a>SLAM介绍</h3><p><strong>Simultaneous Localization and Mapping (SLAM)</strong></p>\n<ul>\n<li>Localization：传感器的位置和姿态</li>\n<li>Mapping：地图构建</li>\n<li>SLAM：同时定位和建图</li>\n<li>应用场景：定位，导航，避障，重建，交互</li>\n</ul>\n<p>在整个地图构建的过程中，根据地图的特点可以分为：拓扑地图、2D栅格地图、3D网格地图、3D点云地图</p>\n<h3 id=\"ORB-SLAM详解\"><a href=\"#ORB-SLAM详解\" class=\"headerlink\" title=\"ORB-SLAM详解\"></a>ORB-SLAM详解</h3><p>在<strong>ORB-SLAM</strong>中</p>\n<ul>\n<li>一般会同时运行三个线程：<ul>\n<li>跟踪：确定当前帧位姿</li>\n<li>建图：完成局部地图构建</li>\n<li>回环修整：回环检测以及基于回环信息修正系统漂移</li>\n</ul>\n</li>\n<li>数据中心中，会存储<ol>\n<li>视觉词典</li>\n<li>关键帧词袋数据库。</li>\n</ol>\n</li>\n<li>数据中心存储的数据包含<ol>\n<li>地图点<ul>\n<li>世界坐标系下的3D坐标</li>\n<li>观测方向，即所有可以观测到该特征点的视图所产生的观测方向均值</li>\n<li>最具表达性的ORB特征描述子</li>\n<li>该点能被观测到的最大距离和最小距离</li>\n</ul>\n</li>\n<li>关键帧<ul>\n<li>摄像机位姿</li>\n<li>内参数</li>\n<li>全部ORB特征描述符，是否有地图点对应</li>\n</ul>\n</li>\n<li>共视图<ul>\n<li>一种无向有全图，节点为关键帧，如果两个阶段共享的地图数大于阈值(至少15个)则存在一条边，其权重设置为共享地图的个数</li>\n</ul>\n</li>\n<li>本质图<ul>\n<li>共视图的子图，保留所有节点，边数量相较于共视图更少，其作用是加速会还矫正的计算<br>   $本质图 &#x3D; 生成树 + 共视图边权重超过100的边 + 回环边$</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h4 id=\"跟踪\"><a href=\"#跟踪\" class=\"headerlink\" title=\"跟踪\"></a>跟踪</h4><p><strong>目标</strong>: 构建初始的地图点云，第一个摄像机作为世界坐标系。</p>\n<ol>\n<li>计算特征点匹配，匹配数要大于预定义门限</li>\n<li>同时计算基础矩阵F与单应矩阵H(注：匹配点可能来自同一平面)</li>\n<li>分别计算F和H误差，选择最合理的矩阵</li>\n<li>对F或者H进行分解，三角化重构出初始地图点</li>\n<li>使用<strong>GlobalBA</strong>进行优化，即关键帧位姿与重构的地图点都参与优化</li>\n</ol>\n<p><strong>基于前一帧的估计</strong>：</p>\n<ol>\n<li>如果前一帧跟踪成功，利用上一帧的运动模型估计当前帧的相机位姿</li>\n<li>依据估计的R和T，将前一帧观测到的地图点投影到当前帧</li>\n<li>如果成功找到足够多的点，则依照2D-3D对应点计算相机位姿(EPnP)；如果失败则转到全局重定位。  </li>\n<li>在全局重定位中，计算当前帧的词袋，在数据库中进行检索，找到匹配度大于某个门限的候选参考关键帧，并对于每个候选参考关键帧进行特征点匹配和PnP计算操作，实现迭代优化，求解计算机位姿</li>\n<li><strong>局部地图跟踪：</strong></li>\n<li>寻找候选“局部地图点”<ul>\n<li>建立共视图中与当前帧相练的关键帧集合$K_1$以及与$K_1$相连的关键帧集合$K_2$</li>\n<li>$K_1+K_2$看到的全部地图点构成”局部地图点”集合</li>\n<li>$K_1$与当前帧相似度最高的设为$K_{ref}$</li>\n</ul>\n</li>\n<li>筛选地图点<ul>\n<li>无法引申到当前帧的地图点舍弃</li>\n<li>当前点视图方向和该地图点的平均视图方向$n$满足$v*n &lt; \\cos(60^\\circ)$ </li>\n<li>计算地图点到该帧摄像机中心的距离$d$,如果$d \\notin [d_{min}, d_{max}],则该地图点舍弃$</li>\n<li>计算尺度$\\frac{d}{d_{min}}$，在该帧中，对于所有未匹配的ORB特征描述子，检测在该尺度下是否有3D点的投影与其匹配，若匹配则将该未匹配的特征描述子与3D点建立联系</li>\n</ul>\n</li>\n<li>使用所有未被筛选掉的2D-3D点对应当前帧进行位姿优化</li>\n</ol>\n<p><strong>关键帧需要满足的条件：</strong></p>\n<ol>\n<li>距离上次全局重定位已经经历了超过20帧(刚重定位的帧位姿可能不够准确)</li>\n<li>局部地图线程空闲</li>\n<li>或者距离上次关键帧插入超过了20帧(场景变化)</li>\n<li>当前至少有50个关键点(该帧特征充足，可信度高)</li>\n<li>当前帧和$K_{ref}$共视点相似比小于90%</li>\n</ol>\n<h4 id=\"建图\"><a href=\"#建图\" class=\"headerlink\" title=\"建图\"></a>建图</h4><p><strong>关键帧插入：</strong></p>\n<ul>\n<li>更新共视图，增加新的节点$K_i$，更新共视图链接关系</li>\n<li>更新生成树的节点关系</li>\n<li>计算该新增关键帧的词袋表示，为新建地图点做准备</li>\n</ul>\n<p><strong>地图点剔除</strong></p>\n<ul>\n<li>最近增加的地图点必须通过如下测试，才可以留在地图中<ul>\n<li>实际观测比例大于理论的25%</li>\n<li>地图点被创建后，再经过三个关键帧都可以观测到它</li>\n</ul>\n</li>\n<li>通过测试过的地图点，只在如下情况时会被删除<ul>\n<li>任何时间下观测它的关键帧个数小于3(通常发生在删除关键帧时)</li>\n</ul>\n</li>\n</ul>\n<p><strong>对于当前帧$K_I$中没有被特征匹配的特征点进行如下处理：</strong></p>\n<ul>\n<li>在共视图中选取共视程度最高且基线宽度大于某阈值的关键帧进行特征点匹配和三角化，对重构结果进行尺度、重投影误差、视差(不易过小)、正向深度检验。</li>\n<li>通过检验的点再投影到其他视图中，能匹配成功则建立“地图点-关键帧特征点”的对应关系。</li>\n</ul>\n<p><strong>优化局部地图</strong></p>\n<ul>\n<li>当前帧(位姿)与共视关键帧(位姿)，及其它们看到的所有地图点(空间坐标)，参与局部BA优化。</li>\n<li>所有能看到上述地图点单与当前关键帧没有共视关系的关键帧参与局部BA，但它们的位姿在优化过程中固定不变。</li>\n</ul>\n<p><strong>局部关键帧剔除</strong><br>当场景不发生变化时，控制关键帧的数量。如果当前90%的地图点都可以被至少其他三个关键帧在相同或者更精细的尺度观测到，则该关键帧被剔除。</p>\n<h4 id=\"回环修整\"><a href=\"#回环修整\" class=\"headerlink\" title=\"回环修整\"></a>回环修整</h4><p><strong>回环候选帧检测</strong></p>\n<ol>\n<li>回环候选关键帧产生<br>- 在共视图中计算$K_i$与所邻居的BOW向量相似性，保存最低分数，记作$s_{min}$<br>- 分数低于$s_{min}$的帧不做备选，与当前帧$K_i$直接相连的帧不做备选</li>\n<li>确认回环关键帧</li>\n</ol>\n<p><strong>对于每一个候选回环关键帧$K_j$</strong></p>\n<ol>\n<li>计算其与当前关键帧$K_i$的特征匹配，建立3D-3D点对应</li>\n<li>利用RANSAC方法估计其与当前帧的相似变换$S_{ij}$(7个自由度)</li>\n<li>利用$S_{ij}$搜索两帧之间更多点对应</li>\n<li>基于点对应优化$S_{ij}$直至有足够多内点，则接受当前候选回环关键帧，并记为$K_i$，其相似变换记为$S_{jl}$</li>\n</ol>\n<p><strong>回环融合</strong></p>\n<ol>\n<li>基于相似性矩阵$S_{il}$修正当前帧$K_i$(及其共视帧位姿)</li>\n<li>特征点匹配与地图匹配融合：将回环帧$K_i$及其共视帧可见的地图点投影到当前帧$K_i$及其共视关键帧，寻找匹配并融合</li>\n<li>连接关系更新:融合过程中涉及到的关键帧的边均需要更新，包括回环边</li>\n</ol>\n<ul>\n<li>每条边的误差为:<br>$$<br>e_{ij} &#x3D; \\log(S_{ij}S_{jw}S_{iw}^{-1})<br>$$</li>\n<li>本质图上的总误差为：<br>$$<br>C &#x3D; \\sum_{i,j}{e_{i,j}^T\\Lambda{e_{ij}}}    \\<br>其中，{\\Lambda}_{i,j}为单位矩阵<br>$$</li>\n</ul>\n"},{"title":"基于多步视差的深度估计","data":"2025-10-16T16:00:00.000Z","_content":"本文章的方法源于论文《Accurate Multiple View 3D Reconstruction Using Patch-Based Stereo for Large-Scale Scenes》\n\n## 基础方法介绍\n\n在整体的针对基于多步视差获取深度估计的算法中，完整算法步骤分为如下四步：\n\n1. 立体像对选择 （Stereo Pair Selection）\n2. 深度图计算 （Depth-Map Computation）\n3. 深度图优化 （Depth-Map Refinement）\n4. 深度图融合 （Depth-Map Merging）\n\n其是一个用于单目相机连续或相关帧之间的深度估计算法，实际上正好符合对于SLAM定位技术中的基本需求。\n\n## 立体像对选择 （Stereo Pair Selection）\n\n`立体像对的选取`不仅关系到立体匹配的精度，还将直接影响最终多视图三维重建MVS（区别与SFM，为稠密建图）的效果。在实际的方法测试中，立体像的选取对于街景这种高结构化场景非常简单，但是对于其他普遍性的无序场景，需要额外的设计。理想的参考图需要满足两个条件：\n\n1. 与目标图像的拍摄视角合适（视角适中）\n2. 具有适中的基线距离\n   - 过短会导致三位重建精度下降\n   - 过长则可能造成场景共同覆盖区域不足\n\n事实上，该选择方法可以直接参考SFM中的图像对选取，通过计算两图像图像共视点夹角之间的平均值或中位值进行判断，对于图像$i$，计算其与其他图像之间的夹角$\\theta_{i,j}$选取其中夹角在$5^\\circ$至$60^\\circ$之间的图像对。\n对于满足夹角的图像，计算每对图像光学中心点之间的距离$d_{i,j}$，计算其中中位数$d$，剔除$d_{i,j}>2d$ 或 $d_{i,j}<0.05d$ 的异常图像。若剩余图像不足 $k_1$ 张（在该论文中设 $k_1$=10），则全部视为图像$i$的邻域图像集合N(i)；否则按 $\\theta_{i,j} \\dot d_{i,j}$ 升序排列，取前 $k_1$ 张作为 $N(i)$ 。最终从 $N(i)$ 中选取乘积最小的图像作为第 $i$ 张图像的参考图像，构成立体像对。\n\n## 深度图计算 （Depth-Map Computation）\n\n对于深度图计算的核心思想是：对于输入的每个像素点，尝试找到一个最佳的支撑平面，使其与参考图像之间的聚合匹配成本最小。支撑平面 $\nf$ 的本质是场景表面的局部切平面，该平面由关联相机坐标系中的三维点 $x_i$ 及其法向量 $n_i$ 表示。\n\n在图像集中的第 $i$ 张输入图像 $I_i$中，给定其参考图 $I_j$ 以及其对应的相机参数组合 $\\{K_i, R_i, C_i\\}$ 和 $\\{K_j, R_j, C_j\\}$，其中 $K$ 代表内参矩阵，$R$ 为旋转矩阵，$C$ 表示相机中心），我们首先为 $Ii$ 中的每个像素点 $p$ 随机分配一个三维平面。设像素点 $p$ 的齐次坐标为：\n\n$$\\mathbf{p} = \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}$$\n\n三维点 $X_i$ 必须位于点 $p$ 的观测射线上，我们在深度范围 $\\lambda \\in [\\lambda_{min}, \\lambda_{max}]$ 内随机选择一个深度 $\\lambda$ ，然后在相机 $C_i$ 的坐标系中计算 $X_i$ ：\n$$X_i = \\lambda K_i^{-1} p$$\n然后我们在相机$C_i$的球面坐标系中随机分配平面的法向量，如下所示：\n\n$$\\mathbf{n_i} = \\begin{bmatrix} \\cos\\theta\\sin\\phi \\newline \n                                 \\sin\\theta\\sin\\phi \\newline  \n                                 \\cos\\phi \\end{bmatrix}$$\n\n其中，$\\theta$ 是范围 $[0°, 360°]$ 内的随机角度， $\\phi$ 是范围 $[0°, 60°]$ 内的随机角度。通过球面坐标系获取的法向量方式分布更加均匀。这些范围设置基于一个简单假设：当图像块法向量 $n_i$ 与相机 $C_i$ 坐标系的z轴之间夹角低于某个阈值时，该图像块在图像 $I_i$ 中是可见的（论文中将此阈值设置为60°）。\n\n这个分配的操作本质是给每个像素一个基础的猜测值，该操作也大概率能为图像中的每个场景平面保留至少一个有效猜测值。一旦完成了图像 $I_i$ 的深度图计算，在后期处理其参考图 $I_j$ 的深度图时就能改进其纯随机的初始化方法。此时，$I_i$ 深度图中各像素的深度值和面片法线可以通过视点变换映射到 $I_j$，作为计算 $I_j$ 深度图的初始估计值；而 $I_i$ 与 $I_j$ 之间不存在映射关系的 $I_j$ 像素点仍需保持随机初始化状态。通过这种方式，我们能为 $I_j$ 中每个经视点变换的像素赋予比随机猜测更优的初始平面，因为该平面在立体图像对 $I_i$ 和 $I_j$ 之间具有一致性。\n\n接下来基于平面诱导单应矩阵的方法进行深度匹配估计\n\n### 深度匹配估计\n\n#### 1. 单应矩阵计算（关键操作）\n\n- 输入：两个相机的内外参 $\\{K_i, R_i, C_i\\}$、$\\{K_j, R_j, C_j\\}$ 和一个平面参数 $f_p = \\{X_i, \\mathbf{n}_i\\}$（定义在相机 $C_i$ 坐标系）。\n- 操作：\n\n  将世界坐标系对齐到相机 $C_i$，标准化投影矩阵：\n\n  $$\n     P_i = K_i[I \\mid \\mathbf{0}], \\quad P_j = K_j[R_jR_i^{-1} \\mid R_j(C_i - C_j)]\n  $$\n\n  计算平面诱导的单应矩阵 $H_{ij}$：\n\n  $$\n     H_{ij} = K_j \\left( R_jR_i^{-1} + \\frac{R_j(C_i - C_j)\\mathbf{n}_i^\\top}{\\mathbf{n}_i^\\top X_i} \\right) K_i^{-1}\n  $$\n\n- 作用：通过单应将图像 $I_i$ 的像素映射到图像 $I_j$ 的对应位置。\n\n#### 2. 归一化互相关（NCC）匹配代价聚合\n\n- 输入：当前像素 $p$、单应矩阵 $H_{ij}$、局部窗口 $B$（如 $7 \\times 7$ 邻域）。\n- 操作：\n\n  对窗口 $B$ 内每个像素 $q$，用 $H_{ij}$ 计算其在参考图像 $I_j$ 中的对应点 $H_{ij}(q)$。\n计算 $q$ 和 $H_{ij}(q)$ 的 NCC 分数（比较信号差异性，即窗口 $B$ 中像素差异与在参考图上对应点间的差异）：\n\n\n\n  $$\n     \\text{NCC} = \\frac{\\sum (q - \\bar{q})(H_{ij}(q) - \\overline{H_{ij}(q)})}{\\sqrt{\\sum (q - \\bar{q})^2 \\sum (H_{ij}(q) - \\overline{H_{ij}(q)})^2}}\n  $$\n\n  聚合代价 $m(p, f_p) = 1 - \\text{NCC}$（NCC 范围 $[-1, 1]$，代价越小匹配越好）。\n\n- 作用：衡量当前像素 $p$ 与平面假设 $f_p$ 的光度一致性。\n\n#### 3. 简化设计的理由\n\n高分辨率图像：提供了足够可靠的匹配，NCC 简单高效。\n后续优化：不可靠匹配会被深度图优化（如滤波、CRF）剔除，最终结果与复杂方法接近。\n\n### 空间传播\n\n通过解该最优化问题后，对于该图像通过正向和反向遍历图像中的各个像素来优化其对应的平面。\n每次迭代中，每个像素会进行两种操作：空间传播与随机赋值。空间传播通过比对相邻像素平面与当前像素平面的匹配代价，若满足条件 $m(p, f_{pN}) < m(p, f_p)$ ，则将更优的邻域平面$f_{pN}$ 传播至当前像素 $p$（即令 $f_p$ = $f_pN$）。该操作依赖高分辨率图像中相邻像素共享相似3D平面的特性。理论上，仅需经过奇偶次空间传播：奇数轮次选取左、上、左上邻域，偶数轮次选取右、下、右下邻域，就能将优质平面猜测快速传播至整个区域。\n\n### 随机赋值\n\n在完成空间传递后，采用随机分配方法进一步优化平面参数 $f_p$。该方法的原理是通过随机测试多组平面参数，从而降低匹配代价。具体实现包含五个步骤：\n\n1. 参数扰动\n  随机生成新参数：\n\n  $$\n     \\lambda' \\in [\\lambda - \\Delta\\lambda, \\lambda + \\Delta\\lambda], \\quad\n     \\theta' \\in [\\theta - \\Delta\\theta, \\theta + \\Delta\\theta], \\quad\n     \\phi' \\in [\\phi - \\Delta\\phi, \\phi + \\Delta\\phi]\n  $$\n\n2. 更新平面\n  计算新平面参数 $f'_p = \\{X'_i, \\mathbf{n}'_i\\}$ \n3. 代价判断：若新代价更低（$m(p, f'_p) < m(p, f_p)$），则接受更新：\n\n  $$\n     (\\lambda, \\theta, \\phi) \\leftarrow (\\lambda', \\theta', \\phi')\n  $$\n\n4. 缩小搜索范围\n  将范围减半：\n\n  $$\n     \\Delta\\lambda \\leftarrow \\Delta\\lambda/2, \\quad\n     \\Delta\\theta \\leftarrow \\Delta\\theta/2, \\quad\n     \\Delta\\phi \\leftarrow \\Delta\\phi/2\n  $$\n\n5. 迭代终止\n  重复上述步骤 $k_3 = 6$ 次。\n\n经过空间传播和随机分配处理后，我们会从深度图中剔除那些聚合匹配代价超过阈值 $τ_1$（本文设定$τ_1=0.3$）的不可靠点。\n\n\n## 深度图优化 （Depth-Map Refinement）\n\n由于原始深度图在公共区域可能因深度误差不一致，需通过优化过程加强相邻视图间的约束。具体步骤如下：\n\n1. 3D反投影\n  对图像 $I_i$ 中的每个点 $p$，根据深度 $\\lambda$ 和相机参数将其反投影到3D空间：\n\n  $$\n     \\mathbf{X} = \\lambda R_i^\\top K_i^{-1} p + C_i \\tag{7}\n  $$\n  其中 $p$ 是齐次坐标（由式1定义），$\\mathbf{X}$ 为世界坐标系下的3D点。\n\n2. 邻视图投影\n  将 $\\mathbf{X}$ 投影到 $I_i$ 的相邻视图 $N(i)$（来自立体像对选择阶段）。对第 $k$ 个邻视图 $N_k$：\n  真实深度：$d(\\mathbf{X}, N_k)$（$\\mathbf{X}$ 在相机 $N_k$ 中的深度）\n  估计深度：$\\lambda(\\mathbf{X}, N_k)$（通过 $N_k$ 的深度图计算）\n\n3. 一致性判定\n  若满足深度一致性：\n  $$\n   \\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} < \\tau_2 \\quad (\\tau_2 = 0.01)\n  $$\n  则称 $\\mathbf{X}$ 在 $I_i$ 和 $N_k$ 中一致。\n\n4. 可靠性过滤\n  当 $\\mathbf{X}$ 在至少 $k_4 = 2$ 个邻视图中一致时，保留 $I_i$ 深度图中对应像素 $p$；否则剔除。\n\n- 优化效果：该方法可去除多数误差，最终生成各视角下干净的深度图。\n\n## 深度图融合 （Depth-Map Merging）\n\n针对多视图中，存在视角重合，所以要进行深度图融合保证深度一致性。通过邻视图深度测试进一步去冗：\n\n1. 冗余检测流程\n   对相机 $C_i$ 深度图中的每个像素 $p$ 反投影为3D点 $\\mathbf{X}$ ：\n   $$\n      \\mathbf{X} = \\lambda R_i^\\top K_i^{-1} p + C_i\n   $$\n2. 深度比较规则\n   - 遮挡情况：\n     若邻视图中的真实深度 $d(\\mathbf{X}, N_k)$ 小于其深度图的估计值 $\\lambda(\\mathbf{X}, N_k)$，判定为遮挡，剔除邻视图中的对应点。\n   - 冗余情况（如邻视图 $N_4$）：\n     若两者深度接近（$\\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} < \\tau$），判定为重复点，剔除邻视图中的对应点。\n\n3. 生成点云\n   - 合并所有深度图的3D反投影点，得到密集点云。\n   - 稀疏化控制：\n     若需降低点云密度，可仅保留深度图中稀疏位置（如 $(2n, 2n)$ 像素）的点，使点云规模降至$1/4$。\n\n## 总结\n\n老套的深度估计方法，但是平衡了速度与精度，相当经典的算法，推荐作为**CV**研究的前置学习内容。\n","source":"_posts/基于多步视差的深度估计.md","raw":"---\ntitle: 基于多步视差的深度估计\ndata:  2025-10-17 \ntags:\n    - 三维视觉\n    - 深度估计\n    - 学习笔记\n---\n本文章的方法源于论文《Accurate Multiple View 3D Reconstruction Using Patch-Based Stereo for Large-Scale Scenes》\n\n## 基础方法介绍\n\n在整体的针对基于多步视差获取深度估计的算法中，完整算法步骤分为如下四步：\n\n1. 立体像对选择 （Stereo Pair Selection）\n2. 深度图计算 （Depth-Map Computation）\n3. 深度图优化 （Depth-Map Refinement）\n4. 深度图融合 （Depth-Map Merging）\n\n其是一个用于单目相机连续或相关帧之间的深度估计算法，实际上正好符合对于SLAM定位技术中的基本需求。\n\n## 立体像对选择 （Stereo Pair Selection）\n\n`立体像对的选取`不仅关系到立体匹配的精度，还将直接影响最终多视图三维重建MVS（区别与SFM，为稠密建图）的效果。在实际的方法测试中，立体像的选取对于街景这种高结构化场景非常简单，但是对于其他普遍性的无序场景，需要额外的设计。理想的参考图需要满足两个条件：\n\n1. 与目标图像的拍摄视角合适（视角适中）\n2. 具有适中的基线距离\n   - 过短会导致三位重建精度下降\n   - 过长则可能造成场景共同覆盖区域不足\n\n事实上，该选择方法可以直接参考SFM中的图像对选取，通过计算两图像图像共视点夹角之间的平均值或中位值进行判断，对于图像$i$，计算其与其他图像之间的夹角$\\theta_{i,j}$选取其中夹角在$5^\\circ$至$60^\\circ$之间的图像对。\n对于满足夹角的图像，计算每对图像光学中心点之间的距离$d_{i,j}$，计算其中中位数$d$，剔除$d_{i,j}>2d$ 或 $d_{i,j}<0.05d$ 的异常图像。若剩余图像不足 $k_1$ 张（在该论文中设 $k_1$=10），则全部视为图像$i$的邻域图像集合N(i)；否则按 $\\theta_{i,j} \\dot d_{i,j}$ 升序排列，取前 $k_1$ 张作为 $N(i)$ 。最终从 $N(i)$ 中选取乘积最小的图像作为第 $i$ 张图像的参考图像，构成立体像对。\n\n## 深度图计算 （Depth-Map Computation）\n\n对于深度图计算的核心思想是：对于输入的每个像素点，尝试找到一个最佳的支撑平面，使其与参考图像之间的聚合匹配成本最小。支撑平面 $\nf$ 的本质是场景表面的局部切平面，该平面由关联相机坐标系中的三维点 $x_i$ 及其法向量 $n_i$ 表示。\n\n在图像集中的第 $i$ 张输入图像 $I_i$中，给定其参考图 $I_j$ 以及其对应的相机参数组合 $\\{K_i, R_i, C_i\\}$ 和 $\\{K_j, R_j, C_j\\}$，其中 $K$ 代表内参矩阵，$R$ 为旋转矩阵，$C$ 表示相机中心），我们首先为 $Ii$ 中的每个像素点 $p$ 随机分配一个三维平面。设像素点 $p$ 的齐次坐标为：\n\n$$\\mathbf{p} = \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}$$\n\n三维点 $X_i$ 必须位于点 $p$ 的观测射线上，我们在深度范围 $\\lambda \\in [\\lambda_{min}, \\lambda_{max}]$ 内随机选择一个深度 $\\lambda$ ，然后在相机 $C_i$ 的坐标系中计算 $X_i$ ：\n$$X_i = \\lambda K_i^{-1} p$$\n然后我们在相机$C_i$的球面坐标系中随机分配平面的法向量，如下所示：\n\n$$\\mathbf{n_i} = \\begin{bmatrix} \\cos\\theta\\sin\\phi \\newline \n                                 \\sin\\theta\\sin\\phi \\newline  \n                                 \\cos\\phi \\end{bmatrix}$$\n\n其中，$\\theta$ 是范围 $[0°, 360°]$ 内的随机角度， $\\phi$ 是范围 $[0°, 60°]$ 内的随机角度。通过球面坐标系获取的法向量方式分布更加均匀。这些范围设置基于一个简单假设：当图像块法向量 $n_i$ 与相机 $C_i$ 坐标系的z轴之间夹角低于某个阈值时，该图像块在图像 $I_i$ 中是可见的（论文中将此阈值设置为60°）。\n\n这个分配的操作本质是给每个像素一个基础的猜测值，该操作也大概率能为图像中的每个场景平面保留至少一个有效猜测值。一旦完成了图像 $I_i$ 的深度图计算，在后期处理其参考图 $I_j$ 的深度图时就能改进其纯随机的初始化方法。此时，$I_i$ 深度图中各像素的深度值和面片法线可以通过视点变换映射到 $I_j$，作为计算 $I_j$ 深度图的初始估计值；而 $I_i$ 与 $I_j$ 之间不存在映射关系的 $I_j$ 像素点仍需保持随机初始化状态。通过这种方式，我们能为 $I_j$ 中每个经视点变换的像素赋予比随机猜测更优的初始平面，因为该平面在立体图像对 $I_i$ 和 $I_j$ 之间具有一致性。\n\n接下来基于平面诱导单应矩阵的方法进行深度匹配估计\n\n### 深度匹配估计\n\n#### 1. 单应矩阵计算（关键操作）\n\n- 输入：两个相机的内外参 $\\{K_i, R_i, C_i\\}$、$\\{K_j, R_j, C_j\\}$ 和一个平面参数 $f_p = \\{X_i, \\mathbf{n}_i\\}$（定义在相机 $C_i$ 坐标系）。\n- 操作：\n\n  将世界坐标系对齐到相机 $C_i$，标准化投影矩阵：\n\n  $$\n     P_i = K_i[I \\mid \\mathbf{0}], \\quad P_j = K_j[R_jR_i^{-1} \\mid R_j(C_i - C_j)]\n  $$\n\n  计算平面诱导的单应矩阵 $H_{ij}$：\n\n  $$\n     H_{ij} = K_j \\left( R_jR_i^{-1} + \\frac{R_j(C_i - C_j)\\mathbf{n}_i^\\top}{\\mathbf{n}_i^\\top X_i} \\right) K_i^{-1}\n  $$\n\n- 作用：通过单应将图像 $I_i$ 的像素映射到图像 $I_j$ 的对应位置。\n\n#### 2. 归一化互相关（NCC）匹配代价聚合\n\n- 输入：当前像素 $p$、单应矩阵 $H_{ij}$、局部窗口 $B$（如 $7 \\times 7$ 邻域）。\n- 操作：\n\n  对窗口 $B$ 内每个像素 $q$，用 $H_{ij}$ 计算其在参考图像 $I_j$ 中的对应点 $H_{ij}(q)$。\n计算 $q$ 和 $H_{ij}(q)$ 的 NCC 分数（比较信号差异性，即窗口 $B$ 中像素差异与在参考图上对应点间的差异）：\n\n\n\n  $$\n     \\text{NCC} = \\frac{\\sum (q - \\bar{q})(H_{ij}(q) - \\overline{H_{ij}(q)})}{\\sqrt{\\sum (q - \\bar{q})^2 \\sum (H_{ij}(q) - \\overline{H_{ij}(q)})^2}}\n  $$\n\n  聚合代价 $m(p, f_p) = 1 - \\text{NCC}$（NCC 范围 $[-1, 1]$，代价越小匹配越好）。\n\n- 作用：衡量当前像素 $p$ 与平面假设 $f_p$ 的光度一致性。\n\n#### 3. 简化设计的理由\n\n高分辨率图像：提供了足够可靠的匹配，NCC 简单高效。\n后续优化：不可靠匹配会被深度图优化（如滤波、CRF）剔除，最终结果与复杂方法接近。\n\n### 空间传播\n\n通过解该最优化问题后，对于该图像通过正向和反向遍历图像中的各个像素来优化其对应的平面。\n每次迭代中，每个像素会进行两种操作：空间传播与随机赋值。空间传播通过比对相邻像素平面与当前像素平面的匹配代价，若满足条件 $m(p, f_{pN}) < m(p, f_p)$ ，则将更优的邻域平面$f_{pN}$ 传播至当前像素 $p$（即令 $f_p$ = $f_pN$）。该操作依赖高分辨率图像中相邻像素共享相似3D平面的特性。理论上，仅需经过奇偶次空间传播：奇数轮次选取左、上、左上邻域，偶数轮次选取右、下、右下邻域，就能将优质平面猜测快速传播至整个区域。\n\n### 随机赋值\n\n在完成空间传递后，采用随机分配方法进一步优化平面参数 $f_p$。该方法的原理是通过随机测试多组平面参数，从而降低匹配代价。具体实现包含五个步骤：\n\n1. 参数扰动\n  随机生成新参数：\n\n  $$\n     \\lambda' \\in [\\lambda - \\Delta\\lambda, \\lambda + \\Delta\\lambda], \\quad\n     \\theta' \\in [\\theta - \\Delta\\theta, \\theta + \\Delta\\theta], \\quad\n     \\phi' \\in [\\phi - \\Delta\\phi, \\phi + \\Delta\\phi]\n  $$\n\n2. 更新平面\n  计算新平面参数 $f'_p = \\{X'_i, \\mathbf{n}'_i\\}$ \n3. 代价判断：若新代价更低（$m(p, f'_p) < m(p, f_p)$），则接受更新：\n\n  $$\n     (\\lambda, \\theta, \\phi) \\leftarrow (\\lambda', \\theta', \\phi')\n  $$\n\n4. 缩小搜索范围\n  将范围减半：\n\n  $$\n     \\Delta\\lambda \\leftarrow \\Delta\\lambda/2, \\quad\n     \\Delta\\theta \\leftarrow \\Delta\\theta/2, \\quad\n     \\Delta\\phi \\leftarrow \\Delta\\phi/2\n  $$\n\n5. 迭代终止\n  重复上述步骤 $k_3 = 6$ 次。\n\n经过空间传播和随机分配处理后，我们会从深度图中剔除那些聚合匹配代价超过阈值 $τ_1$（本文设定$τ_1=0.3$）的不可靠点。\n\n\n## 深度图优化 （Depth-Map Refinement）\n\n由于原始深度图在公共区域可能因深度误差不一致，需通过优化过程加强相邻视图间的约束。具体步骤如下：\n\n1. 3D反投影\n  对图像 $I_i$ 中的每个点 $p$，根据深度 $\\lambda$ 和相机参数将其反投影到3D空间：\n\n  $$\n     \\mathbf{X} = \\lambda R_i^\\top K_i^{-1} p + C_i \\tag{7}\n  $$\n  其中 $p$ 是齐次坐标（由式1定义），$\\mathbf{X}$ 为世界坐标系下的3D点。\n\n2. 邻视图投影\n  将 $\\mathbf{X}$ 投影到 $I_i$ 的相邻视图 $N(i)$（来自立体像对选择阶段）。对第 $k$ 个邻视图 $N_k$：\n  真实深度：$d(\\mathbf{X}, N_k)$（$\\mathbf{X}$ 在相机 $N_k$ 中的深度）\n  估计深度：$\\lambda(\\mathbf{X}, N_k)$（通过 $N_k$ 的深度图计算）\n\n3. 一致性判定\n  若满足深度一致性：\n  $$\n   \\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} < \\tau_2 \\quad (\\tau_2 = 0.01)\n  $$\n  则称 $\\mathbf{X}$ 在 $I_i$ 和 $N_k$ 中一致。\n\n4. 可靠性过滤\n  当 $\\mathbf{X}$ 在至少 $k_4 = 2$ 个邻视图中一致时，保留 $I_i$ 深度图中对应像素 $p$；否则剔除。\n\n- 优化效果：该方法可去除多数误差，最终生成各视角下干净的深度图。\n\n## 深度图融合 （Depth-Map Merging）\n\n针对多视图中，存在视角重合，所以要进行深度图融合保证深度一致性。通过邻视图深度测试进一步去冗：\n\n1. 冗余检测流程\n   对相机 $C_i$ 深度图中的每个像素 $p$ 反投影为3D点 $\\mathbf{X}$ ：\n   $$\n      \\mathbf{X} = \\lambda R_i^\\top K_i^{-1} p + C_i\n   $$\n2. 深度比较规则\n   - 遮挡情况：\n     若邻视图中的真实深度 $d(\\mathbf{X}, N_k)$ 小于其深度图的估计值 $\\lambda(\\mathbf{X}, N_k)$，判定为遮挡，剔除邻视图中的对应点。\n   - 冗余情况（如邻视图 $N_4$）：\n     若两者深度接近（$\\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} < \\tau$），判定为重复点，剔除邻视图中的对应点。\n\n3. 生成点云\n   - 合并所有深度图的3D反投影点，得到密集点云。\n   - 稀疏化控制：\n     若需降低点云密度，可仅保留深度图中稀疏位置（如 $(2n, 2n)$ 像素）的点，使点云规模降至$1/4$。\n\n## 总结\n\n老套的深度估计方法，但是平衡了速度与精度，相当经典的算法，推荐作为**CV**研究的前置学习内容。\n","slug":"基于多步视差的深度估计","published":1,"date":"2025-10-16T08:12:12.008Z","updated":"2025-10-20T04:38:51.819Z","comments":1,"layout":"post","photos":[],"_id":"cmgzc3rli00007a6efl2chtsj","content":"<p>本文章的方法源于论文《Accurate Multiple View 3D Reconstruction Using Patch-Based Stereo for Large-Scale Scenes》</p>\n<h2 id=\"基础方法介绍\"><a href=\"#基础方法介绍\" class=\"headerlink\" title=\"基础方法介绍\"></a>基础方法介绍</h2><p>在整体的针对基于多步视差获取深度估计的算法中，完整算法步骤分为如下四步：</p>\n<ol>\n<li>立体像对选择 （Stereo Pair Selection）</li>\n<li>深度图计算 （Depth-Map Computation）</li>\n<li>深度图优化 （Depth-Map Refinement）</li>\n<li>深度图融合 （Depth-Map Merging）</li>\n</ol>\n<p>其是一个用于单目相机连续或相关帧之间的深度估计算法，实际上正好符合对于SLAM定位技术中的基本需求。</p>\n<h2 id=\"立体像对选择-（Stereo-Pair-Selection）\"><a href=\"#立体像对选择-（Stereo-Pair-Selection）\" class=\"headerlink\" title=\"立体像对选择 （Stereo Pair Selection）\"></a>立体像对选择 （Stereo Pair Selection）</h2><p><code>立体像对的选取</code>不仅关系到立体匹配的精度，还将直接影响最终多视图三维重建MVS（区别与SFM，为稠密建图）的效果。在实际的方法测试中，立体像的选取对于街景这种高结构化场景非常简单，但是对于其他普遍性的无序场景，需要额外的设计。理想的参考图需要满足两个条件：</p>\n<ol>\n<li>与目标图像的拍摄视角合适（视角适中）</li>\n<li>具有适中的基线距离<ul>\n<li>过短会导致三位重建精度下降</li>\n<li>过长则可能造成场景共同覆盖区域不足</li>\n</ul>\n</li>\n</ol>\n<p>事实上，该选择方法可以直接参考SFM中的图像对选取，通过计算两图像图像共视点夹角之间的平均值或中位值进行判断，对于图像$i$，计算其与其他图像之间的夹角$\\theta_{i,j}$选取其中夹角在$5^\\circ$至$60^\\circ$之间的图像对。<br>对于满足夹角的图像，计算每对图像光学中心点之间的距离$d_{i,j}$，计算其中中位数$d$，剔除$d_{i,j}&gt;2d$ 或 $d_{i,j}&lt;0.05d$ 的异常图像。若剩余图像不足 $k_1$ 张（在该论文中设 $k_1$&#x3D;10），则全部视为图像$i$的邻域图像集合N(i)；否则按 $\\theta_{i,j} \\dot d_{i,j}$ 升序排列，取前 $k_1$ 张作为 $N(i)$ 。最终从 $N(i)$ 中选取乘积最小的图像作为第 $i$ 张图像的参考图像，构成立体像对。</p>\n<h2 id=\"深度图计算-（Depth-Map-Computation）\"><a href=\"#深度图计算-（Depth-Map-Computation）\" class=\"headerlink\" title=\"深度图计算 （Depth-Map Computation）\"></a>深度图计算 （Depth-Map Computation）</h2><p>对于深度图计算的核心思想是：对于输入的每个像素点，尝试找到一个最佳的支撑平面，使其与参考图像之间的聚合匹配成本最小。支撑平面 $<br>f$ 的本质是场景表面的局部切平面，该平面由关联相机坐标系中的三维点 $x_i$ 及其法向量 $n_i$ 表示。</p>\n<p>在图像集中的第 $i$ 张输入图像 $I_i$中，给定其参考图 $I_j$ 以及其对应的相机参数组合 ${K_i, R_i, C_i}$ 和 ${K_j, R_j, C_j}$，其中 $K$ 代表内参矩阵，$R$ 为旋转矩阵，$C$ 表示相机中心），我们首先为 $Ii$ 中的每个像素点 $p$ 随机分配一个三维平面。设像素点 $p$ 的齐次坐标为：</p>\n<p>$$\\mathbf{p} &#x3D; \\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix}$$</p>\n<p>三维点 $X_i$ 必须位于点 $p$ 的观测射线上，我们在深度范围 $\\lambda \\in [\\lambda_{min}, \\lambda_{max}]$ 内随机选择一个深度 $\\lambda$ ，然后在相机 $C_i$ 的坐标系中计算 $X_i$ ：<br>$$X_i &#x3D; \\lambda K_i^{-1} p$$<br>然后我们在相机$C_i$的球面坐标系中随机分配平面的法向量，如下所示：</p>\n<p>$$\\mathbf{n_i} &#x3D; \\begin{bmatrix} \\cos\\theta\\sin\\phi \\newline<br>                                 \\sin\\theta\\sin\\phi \\newline<br>                                 \\cos\\phi \\end{bmatrix}$$</p>\n<p>其中，$\\theta$ 是范围 $[0°, 360°]$ 内的随机角度， $\\phi$ 是范围 $[0°, 60°]$ 内的随机角度。通过球面坐标系获取的法向量方式分布更加均匀。这些范围设置基于一个简单假设：当图像块法向量 $n_i$ 与相机 $C_i$ 坐标系的z轴之间夹角低于某个阈值时，该图像块在图像 $I_i$ 中是可见的（论文中将此阈值设置为60°）。</p>\n<p>这个分配的操作本质是给每个像素一个基础的猜测值，该操作也大概率能为图像中的每个场景平面保留至少一个有效猜测值。一旦完成了图像 $I_i$ 的深度图计算，在后期处理其参考图 $I_j$ 的深度图时就能改进其纯随机的初始化方法。此时，$I_i$ 深度图中各像素的深度值和面片法线可以通过视点变换映射到 $I_j$，作为计算 $I_j$ 深度图的初始估计值；而 $I_i$ 与 $I_j$ 之间不存在映射关系的 $I_j$ 像素点仍需保持随机初始化状态。通过这种方式，我们能为 $I_j$ 中每个经视点变换的像素赋予比随机猜测更优的初始平面，因为该平面在立体图像对 $I_i$ 和 $I_j$ 之间具有一致性。</p>\n<p>接下来基于平面诱导单应矩阵的方法进行深度匹配估计</p>\n<h3 id=\"深度匹配估计\"><a href=\"#深度匹配估计\" class=\"headerlink\" title=\"深度匹配估计\"></a>深度匹配估计</h3><h4 id=\"1-单应矩阵计算（关键操作）\"><a href=\"#1-单应矩阵计算（关键操作）\" class=\"headerlink\" title=\"1. 单应矩阵计算（关键操作）\"></a>1. 单应矩阵计算（关键操作）</h4><ul>\n<li><p>输入：两个相机的内外参 ${K_i, R_i, C_i}$、${K_j, R_j, C_j}$ 和一个平面参数 $f_p &#x3D; {X_i, \\mathbf{n}_i}$（定义在相机 $C_i$ 坐标系）。</p>\n</li>\n<li><p>操作：</p>\n<p>将世界坐标系对齐到相机 $C_i$，标准化投影矩阵：</p>\n<p>$$<br>   P_i &#x3D; K_i[I \\mid \\mathbf{0}], \\quad P_j &#x3D; K_j[R_jR_i^{-1} \\mid R_j(C_i - C_j)]<br>$$</p>\n<p>计算平面诱导的单应矩阵 $H_{ij}$：</p>\n<p>$$<br>   H_{ij} &#x3D; K_j \\left( R_jR_i^{-1} + \\frac{R_j(C_i - C_j)\\mathbf{n}_i^\\top}{\\mathbf{n}_i^\\top X_i} \\right) K_i^{-1}<br>$$</p>\n</li>\n<li><p>作用：通过单应将图像 $I_i$ 的像素映射到图像 $I_j$ 的对应位置。</p>\n</li>\n</ul>\n<h4 id=\"2-归一化互相关（NCC）匹配代价聚合\"><a href=\"#2-归一化互相关（NCC）匹配代价聚合\" class=\"headerlink\" title=\"2. 归一化互相关（NCC）匹配代价聚合\"></a>2. 归一化互相关（NCC）匹配代价聚合</h4><ul>\n<li><p>输入：当前像素 $p$、单应矩阵 $H_{ij}$、局部窗口 $B$（如 $7 \\times 7$ 邻域）。</p>\n</li>\n<li><p>操作：</p>\n<p>对窗口 $B$ 内每个像素 $q$，用 $H_{ij}$ 计算其在参考图像 $I_j$ 中的对应点 $H_{ij}(q)$。</p>\n</li>\n</ul>\n<p>计算 $q$ 和 $H_{ij}(q)$ 的 NCC 分数（比较信号差异性，即窗口 $B$ 中像素差异与在参考图上对应点间的差异）：</p>\n<p>  $$<br>     \\text{NCC} &#x3D; \\frac{\\sum (q - \\bar{q})(H_{ij}(q) - \\overline{H_{ij}(q)})}{\\sqrt{\\sum (q - \\bar{q})^2 \\sum (H_{ij}(q) - \\overline{H_{ij}(q)})^2}}<br>  $$</p>\n<p>  聚合代价 $m(p, f_p) &#x3D; 1 - \\text{NCC}$（NCC 范围 $[-1, 1]$，代价越小匹配越好）。</p>\n<ul>\n<li>作用：衡量当前像素 $p$ 与平面假设 $f_p$ 的光度一致性。</li>\n</ul>\n<h4 id=\"3-简化设计的理由\"><a href=\"#3-简化设计的理由\" class=\"headerlink\" title=\"3. 简化设计的理由\"></a>3. 简化设计的理由</h4><p>高分辨率图像：提供了足够可靠的匹配，NCC 简单高效。<br>后续优化：不可靠匹配会被深度图优化（如滤波、CRF）剔除，最终结果与复杂方法接近。</p>\n<h3 id=\"空间传播\"><a href=\"#空间传播\" class=\"headerlink\" title=\"空间传播\"></a>空间传播</h3><p>通过解该最优化问题后，对于该图像通过正向和反向遍历图像中的各个像素来优化其对应的平面。<br>每次迭代中，每个像素会进行两种操作：空间传播与随机赋值。空间传播通过比对相邻像素平面与当前像素平面的匹配代价，若满足条件 $m(p, f_{pN}) &lt; m(p, f_p)$ ，则将更优的邻域平面$f_{pN}$ 传播至当前像素 $p$（即令 $f_p$ &#x3D; $f_pN$）。该操作依赖高分辨率图像中相邻像素共享相似3D平面的特性。理论上，仅需经过奇偶次空间传播：奇数轮次选取左、上、左上邻域，偶数轮次选取右、下、右下邻域，就能将优质平面猜测快速传播至整个区域。</p>\n<h3 id=\"随机赋值\"><a href=\"#随机赋值\" class=\"headerlink\" title=\"随机赋值\"></a>随机赋值</h3><p>在完成空间传递后，采用随机分配方法进一步优化平面参数 $f_p$。该方法的原理是通过随机测试多组平面参数，从而降低匹配代价。具体实现包含五个步骤：</p>\n<ol>\n<li>参数扰动<br>  随机生成新参数：</li>\n</ol>\n<p>  $$<br>     \\lambda’ \\in [\\lambda - \\Delta\\lambda, \\lambda + \\Delta\\lambda], \\quad<br>     \\theta’ \\in [\\theta - \\Delta\\theta, \\theta + \\Delta\\theta], \\quad<br>     \\phi’ \\in [\\phi - \\Delta\\phi, \\phi + \\Delta\\phi]<br>  $$</p>\n<ol start=\"2\">\n<li>更新平面<br>  计算新平面参数 $f’_p &#x3D; {X’_i, \\mathbf{n}’_i}$ </li>\n<li>代价判断：若新代价更低（$m(p, f’_p) &lt; m(p, f_p)$），则接受更新：</li>\n</ol>\n<p>  $$<br>     (\\lambda, \\theta, \\phi) \\leftarrow (\\lambda’, \\theta’, \\phi’)<br>  $$</p>\n<ol start=\"4\">\n<li>缩小搜索范围<br>  将范围减半：</li>\n</ol>\n<p>  $$<br>     \\Delta\\lambda \\leftarrow \\Delta\\lambda&#x2F;2, \\quad<br>     \\Delta\\theta \\leftarrow \\Delta\\theta&#x2F;2, \\quad<br>     \\Delta\\phi \\leftarrow \\Delta\\phi&#x2F;2<br>  $$</p>\n<ol start=\"5\">\n<li>迭代终止<br>  重复上述步骤 $k_3 &#x3D; 6$ 次。</li>\n</ol>\n<p>经过空间传播和随机分配处理后，我们会从深度图中剔除那些聚合匹配代价超过阈值 $τ_1$（本文设定$τ_1&#x3D;0.3$）的不可靠点。</p>\n<h2 id=\"深度图优化-（Depth-Map-Refinement）\"><a href=\"#深度图优化-（Depth-Map-Refinement）\" class=\"headerlink\" title=\"深度图优化 （Depth-Map Refinement）\"></a>深度图优化 （Depth-Map Refinement）</h2><p>由于原始深度图在公共区域可能因深度误差不一致，需通过优化过程加强相邻视图间的约束。具体步骤如下：</p>\n<ol>\n<li>3D反投影<br>  对图像 $I_i$ 中的每个点 $p$，根据深度 $\\lambda$ 和相机参数将其反投影到3D空间：</li>\n</ol>\n<p>  $$<br>     \\mathbf{X} &#x3D; \\lambda R_i^\\top K_i^{-1} p + C_i \\tag{7}<br>  $$<br>  其中 $p$ 是齐次坐标（由式1定义），$\\mathbf{X}$ 为世界坐标系下的3D点。</p>\n<ol start=\"2\">\n<li><p>邻视图投影<br>  将 $\\mathbf{X}$ 投影到 $I_i$ 的相邻视图 $N(i)$（来自立体像对选择阶段）。对第 $k$ 个邻视图 $N_k$：<br>  真实深度：$d(\\mathbf{X}, N_k)$（$\\mathbf{X}$ 在相机 $N_k$ 中的深度）<br>  估计深度：$\\lambda(\\mathbf{X}, N_k)$（通过 $N_k$ 的深度图计算）</p>\n</li>\n<li><p>一致性判定<br>  若满足深度一致性：<br>  $$<br>\\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} &lt; \\tau_2 \\quad (\\tau_2 &#x3D; 0.01)<br>  $$<br>  则称 $\\mathbf{X}$ 在 $I_i$ 和 $N_k$ 中一致。</p>\n</li>\n<li><p>可靠性过滤<br>  当 $\\mathbf{X}$ 在至少 $k_4 &#x3D; 2$ 个邻视图中一致时，保留 $I_i$ 深度图中对应像素 $p$；否则剔除。</p>\n</li>\n</ol>\n<ul>\n<li>优化效果：该方法可去除多数误差，最终生成各视角下干净的深度图。</li>\n</ul>\n<h2 id=\"深度图融合-（Depth-Map-Merging）\"><a href=\"#深度图融合-（Depth-Map-Merging）\" class=\"headerlink\" title=\"深度图融合 （Depth-Map Merging）\"></a>深度图融合 （Depth-Map Merging）</h2><p>针对多视图中，存在视角重合，所以要进行深度图融合保证深度一致性。通过邻视图深度测试进一步去冗：</p>\n<ol>\n<li><p>冗余检测流程<br>对相机 $C_i$ 深度图中的每个像素 $p$ 反投影为3D点 $\\mathbf{X}$ ：<br>$$<br>   \\mathbf{X} &#x3D; \\lambda R_i^\\top K_i^{-1} p + C_i<br>$$</p>\n</li>\n<li><p>深度比较规则</p>\n<ul>\n<li>遮挡情况：<br>若邻视图中的真实深度 $d(\\mathbf{X}, N_k)$ 小于其深度图的估计值 $\\lambda(\\mathbf{X}, N_k)$，判定为遮挡，剔除邻视图中的对应点。</li>\n<li>冗余情况（如邻视图 $N_4$）：<br>若两者深度接近（$\\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} &lt; \\tau$），判定为重复点，剔除邻视图中的对应点。</li>\n</ul>\n</li>\n<li><p>生成点云</p>\n<ul>\n<li>合并所有深度图的3D反投影点，得到密集点云。</li>\n<li>稀疏化控制：<br>若需降低点云密度，可仅保留深度图中稀疏位置（如 $(2n, 2n)$ 像素）的点，使点云规模降至$1&#x2F;4$。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>老套的深度估计方法，但是平衡了速度与精度，相当经典的算法，推荐作为<strong>CV</strong>研究的前置学习内容。</p>\n","excerpt":"","more":"<p>本文章的方法源于论文《Accurate Multiple View 3D Reconstruction Using Patch-Based Stereo for Large-Scale Scenes》</p>\n<h2 id=\"基础方法介绍\"><a href=\"#基础方法介绍\" class=\"headerlink\" title=\"基础方法介绍\"></a>基础方法介绍</h2><p>在整体的针对基于多步视差获取深度估计的算法中，完整算法步骤分为如下四步：</p>\n<ol>\n<li>立体像对选择 （Stereo Pair Selection）</li>\n<li>深度图计算 （Depth-Map Computation）</li>\n<li>深度图优化 （Depth-Map Refinement）</li>\n<li>深度图融合 （Depth-Map Merging）</li>\n</ol>\n<p>其是一个用于单目相机连续或相关帧之间的深度估计算法，实际上正好符合对于SLAM定位技术中的基本需求。</p>\n<h2 id=\"立体像对选择-（Stereo-Pair-Selection）\"><a href=\"#立体像对选择-（Stereo-Pair-Selection）\" class=\"headerlink\" title=\"立体像对选择 （Stereo Pair Selection）\"></a>立体像对选择 （Stereo Pair Selection）</h2><p><code>立体像对的选取</code>不仅关系到立体匹配的精度，还将直接影响最终多视图三维重建MVS（区别与SFM，为稠密建图）的效果。在实际的方法测试中，立体像的选取对于街景这种高结构化场景非常简单，但是对于其他普遍性的无序场景，需要额外的设计。理想的参考图需要满足两个条件：</p>\n<ol>\n<li>与目标图像的拍摄视角合适（视角适中）</li>\n<li>具有适中的基线距离<ul>\n<li>过短会导致三位重建精度下降</li>\n<li>过长则可能造成场景共同覆盖区域不足</li>\n</ul>\n</li>\n</ol>\n<p>事实上，该选择方法可以直接参考SFM中的图像对选取，通过计算两图像图像共视点夹角之间的平均值或中位值进行判断，对于图像$i$，计算其与其他图像之间的夹角$\\theta_{i,j}$选取其中夹角在$5^\\circ$至$60^\\circ$之间的图像对。<br>对于满足夹角的图像，计算每对图像光学中心点之间的距离$d_{i,j}$，计算其中中位数$d$，剔除$d_{i,j}&gt;2d$ 或 $d_{i,j}&lt;0.05d$ 的异常图像。若剩余图像不足 $k_1$ 张（在该论文中设 $k_1$&#x3D;10），则全部视为图像$i$的邻域图像集合N(i)；否则按 $\\theta_{i,j} \\dot d_{i,j}$ 升序排列，取前 $k_1$ 张作为 $N(i)$ 。最终从 $N(i)$ 中选取乘积最小的图像作为第 $i$ 张图像的参考图像，构成立体像对。</p>\n<h2 id=\"深度图计算-（Depth-Map-Computation）\"><a href=\"#深度图计算-（Depth-Map-Computation）\" class=\"headerlink\" title=\"深度图计算 （Depth-Map Computation）\"></a>深度图计算 （Depth-Map Computation）</h2><p>对于深度图计算的核心思想是：对于输入的每个像素点，尝试找到一个最佳的支撑平面，使其与参考图像之间的聚合匹配成本最小。支撑平面 $<br>f$ 的本质是场景表面的局部切平面，该平面由关联相机坐标系中的三维点 $x_i$ 及其法向量 $n_i$ 表示。</p>\n<p>在图像集中的第 $i$ 张输入图像 $I_i$中，给定其参考图 $I_j$ 以及其对应的相机参数组合 ${K_i, R_i, C_i}$ 和 ${K_j, R_j, C_j}$，其中 $K$ 代表内参矩阵，$R$ 为旋转矩阵，$C$ 表示相机中心），我们首先为 $Ii$ 中的每个像素点 $p$ 随机分配一个三维平面。设像素点 $p$ 的齐次坐标为：</p>\n<p>$$\\mathbf{p} &#x3D; \\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix}$$</p>\n<p>三维点 $X_i$ 必须位于点 $p$ 的观测射线上，我们在深度范围 $\\lambda \\in [\\lambda_{min}, \\lambda_{max}]$ 内随机选择一个深度 $\\lambda$ ，然后在相机 $C_i$ 的坐标系中计算 $X_i$ ：<br>$$X_i &#x3D; \\lambda K_i^{-1} p$$<br>然后我们在相机$C_i$的球面坐标系中随机分配平面的法向量，如下所示：</p>\n<p>$$\\mathbf{n_i} &#x3D; \\begin{bmatrix} \\cos\\theta\\sin\\phi \\newline<br>                                 \\sin\\theta\\sin\\phi \\newline<br>                                 \\cos\\phi \\end{bmatrix}$$</p>\n<p>其中，$\\theta$ 是范围 $[0°, 360°]$ 内的随机角度， $\\phi$ 是范围 $[0°, 60°]$ 内的随机角度。通过球面坐标系获取的法向量方式分布更加均匀。这些范围设置基于一个简单假设：当图像块法向量 $n_i$ 与相机 $C_i$ 坐标系的z轴之间夹角低于某个阈值时，该图像块在图像 $I_i$ 中是可见的（论文中将此阈值设置为60°）。</p>\n<p>这个分配的操作本质是给每个像素一个基础的猜测值，该操作也大概率能为图像中的每个场景平面保留至少一个有效猜测值。一旦完成了图像 $I_i$ 的深度图计算，在后期处理其参考图 $I_j$ 的深度图时就能改进其纯随机的初始化方法。此时，$I_i$ 深度图中各像素的深度值和面片法线可以通过视点变换映射到 $I_j$，作为计算 $I_j$ 深度图的初始估计值；而 $I_i$ 与 $I_j$ 之间不存在映射关系的 $I_j$ 像素点仍需保持随机初始化状态。通过这种方式，我们能为 $I_j$ 中每个经视点变换的像素赋予比随机猜测更优的初始平面，因为该平面在立体图像对 $I_i$ 和 $I_j$ 之间具有一致性。</p>\n<p>接下来基于平面诱导单应矩阵的方法进行深度匹配估计</p>\n<h3 id=\"深度匹配估计\"><a href=\"#深度匹配估计\" class=\"headerlink\" title=\"深度匹配估计\"></a>深度匹配估计</h3><h4 id=\"1-单应矩阵计算（关键操作）\"><a href=\"#1-单应矩阵计算（关键操作）\" class=\"headerlink\" title=\"1. 单应矩阵计算（关键操作）\"></a>1. 单应矩阵计算（关键操作）</h4><ul>\n<li><p>输入：两个相机的内外参 ${K_i, R_i, C_i}$、${K_j, R_j, C_j}$ 和一个平面参数 $f_p &#x3D; {X_i, \\mathbf{n}_i}$（定义在相机 $C_i$ 坐标系）。</p>\n</li>\n<li><p>操作：</p>\n<p>将世界坐标系对齐到相机 $C_i$，标准化投影矩阵：</p>\n<p>$$<br>   P_i &#x3D; K_i[I \\mid \\mathbf{0}], \\quad P_j &#x3D; K_j[R_jR_i^{-1} \\mid R_j(C_i - C_j)]<br>$$</p>\n<p>计算平面诱导的单应矩阵 $H_{ij}$：</p>\n<p>$$<br>   H_{ij} &#x3D; K_j \\left( R_jR_i^{-1} + \\frac{R_j(C_i - C_j)\\mathbf{n}_i^\\top}{\\mathbf{n}_i^\\top X_i} \\right) K_i^{-1}<br>$$</p>\n</li>\n<li><p>作用：通过单应将图像 $I_i$ 的像素映射到图像 $I_j$ 的对应位置。</p>\n</li>\n</ul>\n<h4 id=\"2-归一化互相关（NCC）匹配代价聚合\"><a href=\"#2-归一化互相关（NCC）匹配代价聚合\" class=\"headerlink\" title=\"2. 归一化互相关（NCC）匹配代价聚合\"></a>2. 归一化互相关（NCC）匹配代价聚合</h4><ul>\n<li><p>输入：当前像素 $p$、单应矩阵 $H_{ij}$、局部窗口 $B$（如 $7 \\times 7$ 邻域）。</p>\n</li>\n<li><p>操作：</p>\n<p>对窗口 $B$ 内每个像素 $q$，用 $H_{ij}$ 计算其在参考图像 $I_j$ 中的对应点 $H_{ij}(q)$。</p>\n</li>\n</ul>\n<p>计算 $q$ 和 $H_{ij}(q)$ 的 NCC 分数（比较信号差异性，即窗口 $B$ 中像素差异与在参考图上对应点间的差异）：</p>\n<p>  $$<br>     \\text{NCC} &#x3D; \\frac{\\sum (q - \\bar{q})(H_{ij}(q) - \\overline{H_{ij}(q)})}{\\sqrt{\\sum (q - \\bar{q})^2 \\sum (H_{ij}(q) - \\overline{H_{ij}(q)})^2}}<br>  $$</p>\n<p>  聚合代价 $m(p, f_p) &#x3D; 1 - \\text{NCC}$（NCC 范围 $[-1, 1]$，代价越小匹配越好）。</p>\n<ul>\n<li>作用：衡量当前像素 $p$ 与平面假设 $f_p$ 的光度一致性。</li>\n</ul>\n<h4 id=\"3-简化设计的理由\"><a href=\"#3-简化设计的理由\" class=\"headerlink\" title=\"3. 简化设计的理由\"></a>3. 简化设计的理由</h4><p>高分辨率图像：提供了足够可靠的匹配，NCC 简单高效。<br>后续优化：不可靠匹配会被深度图优化（如滤波、CRF）剔除，最终结果与复杂方法接近。</p>\n<h3 id=\"空间传播\"><a href=\"#空间传播\" class=\"headerlink\" title=\"空间传播\"></a>空间传播</h3><p>通过解该最优化问题后，对于该图像通过正向和反向遍历图像中的各个像素来优化其对应的平面。<br>每次迭代中，每个像素会进行两种操作：空间传播与随机赋值。空间传播通过比对相邻像素平面与当前像素平面的匹配代价，若满足条件 $m(p, f_{pN}) &lt; m(p, f_p)$ ，则将更优的邻域平面$f_{pN}$ 传播至当前像素 $p$（即令 $f_p$ &#x3D; $f_pN$）。该操作依赖高分辨率图像中相邻像素共享相似3D平面的特性。理论上，仅需经过奇偶次空间传播：奇数轮次选取左、上、左上邻域，偶数轮次选取右、下、右下邻域，就能将优质平面猜测快速传播至整个区域。</p>\n<h3 id=\"随机赋值\"><a href=\"#随机赋值\" class=\"headerlink\" title=\"随机赋值\"></a>随机赋值</h3><p>在完成空间传递后，采用随机分配方法进一步优化平面参数 $f_p$。该方法的原理是通过随机测试多组平面参数，从而降低匹配代价。具体实现包含五个步骤：</p>\n<ol>\n<li>参数扰动<br>  随机生成新参数：</li>\n</ol>\n<p>  $$<br>     \\lambda’ \\in [\\lambda - \\Delta\\lambda, \\lambda + \\Delta\\lambda], \\quad<br>     \\theta’ \\in [\\theta - \\Delta\\theta, \\theta + \\Delta\\theta], \\quad<br>     \\phi’ \\in [\\phi - \\Delta\\phi, \\phi + \\Delta\\phi]<br>  $$</p>\n<ol start=\"2\">\n<li>更新平面<br>  计算新平面参数 $f’_p &#x3D; {X’_i, \\mathbf{n}’_i}$ </li>\n<li>代价判断：若新代价更低（$m(p, f’_p) &lt; m(p, f_p)$），则接受更新：</li>\n</ol>\n<p>  $$<br>     (\\lambda, \\theta, \\phi) \\leftarrow (\\lambda’, \\theta’, \\phi’)<br>  $$</p>\n<ol start=\"4\">\n<li>缩小搜索范围<br>  将范围减半：</li>\n</ol>\n<p>  $$<br>     \\Delta\\lambda \\leftarrow \\Delta\\lambda&#x2F;2, \\quad<br>     \\Delta\\theta \\leftarrow \\Delta\\theta&#x2F;2, \\quad<br>     \\Delta\\phi \\leftarrow \\Delta\\phi&#x2F;2<br>  $$</p>\n<ol start=\"5\">\n<li>迭代终止<br>  重复上述步骤 $k_3 &#x3D; 6$ 次。</li>\n</ol>\n<p>经过空间传播和随机分配处理后，我们会从深度图中剔除那些聚合匹配代价超过阈值 $τ_1$（本文设定$τ_1&#x3D;0.3$）的不可靠点。</p>\n<h2 id=\"深度图优化-（Depth-Map-Refinement）\"><a href=\"#深度图优化-（Depth-Map-Refinement）\" class=\"headerlink\" title=\"深度图优化 （Depth-Map Refinement）\"></a>深度图优化 （Depth-Map Refinement）</h2><p>由于原始深度图在公共区域可能因深度误差不一致，需通过优化过程加强相邻视图间的约束。具体步骤如下：</p>\n<ol>\n<li>3D反投影<br>  对图像 $I_i$ 中的每个点 $p$，根据深度 $\\lambda$ 和相机参数将其反投影到3D空间：</li>\n</ol>\n<p>  $$<br>     \\mathbf{X} &#x3D; \\lambda R_i^\\top K_i^{-1} p + C_i \\tag{7}<br>  $$<br>  其中 $p$ 是齐次坐标（由式1定义），$\\mathbf{X}$ 为世界坐标系下的3D点。</p>\n<ol start=\"2\">\n<li><p>邻视图投影<br>  将 $\\mathbf{X}$ 投影到 $I_i$ 的相邻视图 $N(i)$（来自立体像对选择阶段）。对第 $k$ 个邻视图 $N_k$：<br>  真实深度：$d(\\mathbf{X}, N_k)$（$\\mathbf{X}$ 在相机 $N_k$ 中的深度）<br>  估计深度：$\\lambda(\\mathbf{X}, N_k)$（通过 $N_k$ 的深度图计算）</p>\n</li>\n<li><p>一致性判定<br>  若满足深度一致性：<br>  $$<br>\\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} &lt; \\tau_2 \\quad (\\tau_2 &#x3D; 0.01)<br>  $$<br>  则称 $\\mathbf{X}$ 在 $I_i$ 和 $N_k$ 中一致。</p>\n</li>\n<li><p>可靠性过滤<br>  当 $\\mathbf{X}$ 在至少 $k_4 &#x3D; 2$ 个邻视图中一致时，保留 $I_i$ 深度图中对应像素 $p$；否则剔除。</p>\n</li>\n</ol>\n<ul>\n<li>优化效果：该方法可去除多数误差，最终生成各视角下干净的深度图。</li>\n</ul>\n<h2 id=\"深度图融合-（Depth-Map-Merging）\"><a href=\"#深度图融合-（Depth-Map-Merging）\" class=\"headerlink\" title=\"深度图融合 （Depth-Map Merging）\"></a>深度图融合 （Depth-Map Merging）</h2><p>针对多视图中，存在视角重合，所以要进行深度图融合保证深度一致性。通过邻视图深度测试进一步去冗：</p>\n<ol>\n<li><p>冗余检测流程<br>对相机 $C_i$ 深度图中的每个像素 $p$ 反投影为3D点 $\\mathbf{X}$ ：<br>$$<br>   \\mathbf{X} &#x3D; \\lambda R_i^\\top K_i^{-1} p + C_i<br>$$</p>\n</li>\n<li><p>深度比较规则</p>\n<ul>\n<li>遮挡情况：<br>若邻视图中的真实深度 $d(\\mathbf{X}, N_k)$ 小于其深度图的估计值 $\\lambda(\\mathbf{X}, N_k)$，判定为遮挡，剔除邻视图中的对应点。</li>\n<li>冗余情况（如邻视图 $N_4$）：<br>若两者深度接近（$\\frac{|d(\\mathbf{X}, N_k) - \\lambda(\\mathbf{X}, N_k)|}{\\lambda(\\mathbf{X}, N_k)} &lt; \\tau$），判定为重复点，剔除邻视图中的对应点。</li>\n</ul>\n</li>\n<li><p>生成点云</p>\n<ul>\n<li>合并所有深度图的3D反投影点，得到密集点云。</li>\n<li>稀疏化控制：<br>若需降低点云密度，可仅保留深度图中稀疏位置（如 $(2n, 2n)$ 像素）的点，使点云规模降至$1&#x2F;4$。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>老套的深度估计方法，但是平衡了速度与精度，相当经典的算法，推荐作为<strong>CV</strong>研究的前置学习内容。</p>\n"},{"title":"SPVLoc：基于语义全景视口匹配的相机定位技术 —— 面向未知环境的六维位姿估计","data":"2025-10-17T16:00:00.000Z","mathjax":true,"_content":"本文为对于论文《SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments》 的翻译与精炼，该论文发表于2024年的 `ECCV`上。\n\n## 基础摘要\n\n通过透视相机拍摄RGB图像，在室内环境全景语义布局表征中，实现对于视口的定位。该全景图仅仅包含室内的近似结构以及门窗标注的无纹理3D参考模型渲染生成。文章中，通过简单的卷积网络结构实现了图像-全景图匹配，并最终达成了图像-模型匹配。基于视口分类打分后对参考全景图进行排序，为查询图片找到最佳匹配项，进而估算选定全景图与查询图像之间的相对位姿。\n\n整体分析而言，该论文方法分为四个模块：\n\n1. 语义全景视口匹配（Semantic Panoramic Viewport Matching）\n2. 基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）\n3. 优化\n4. 推理\n\n## 方法介绍\n\n**SPVLoc** 方法本质上是通过给定一个二维的RGB平面图在一个无纹理的语义三位建模场景（室内）中推导出相机的六维位姿定位。模型使用的三位模型是 `Structured3D` 标注格式，其可直接从建筑模型中生成，也可以通过AI工具基于平面图自动推导获取。\n\nSPVLoc首先通过跨域图像-全景图匹配，利用语义场景模型在渲染的全景图中估算图像的视口位置。接着根据分类准确度对这些预测结果进行排名，并针对匹配度最高的参考全景图执行相对6D姿态回归。\n\n### 语义全景视口匹配（Semantic Panoramic Viewport Matching）\n\n#### 问题定义\n\n本论文的目的是为给定查询的RGB图像，获取其相对于语义三维模型的6D位姿。而本论文对此将整体任务转变为了跨域图像-全景图匹配任务。\n\n该模型通过一组N个等矩形参考布局 $L=\\{L_i|i=1,...N\\}$ 表示，每个布局单通道中包含语义标签。每个渲染图完整记录了特定视点周围的全景环境。视口 $V$ 是从全景图中截取的独立区域，精确定位了附近相机所能拍摄的实际视野范围。视角 $V_{P_i}$ 的透视畸变包含 $L_i$ 与 $I_q$ 之间的相对姿态偏移 $P_i$ 的信息。通过评分和排序视口有助于选出最优全景图 $L^*$。绝对姿态 $P$ 由 $P^*$ 与 $P_i$ 串联获取。\n\n#### 视口预测\n\n视口掩码 $V_P$ 是二值图，用于标示等距柱状投影图像中各点在透视相机视角下的可见性。其计算过程设计将全景图的所有3D点投影至透视图，该投影基于训练阶段一直的底层布局完成。{V_P} 掩码通过以下方式确定：结合3D相机位姿信息，并剔除被遮挡的部分，识别能投影到像素透视图的像素点。\n\n此外，二维边界框 $V_{bb}$ 是包围 $V_P$ 的最小轴对称矩形框。该边界框在 $360^{\\circ}$ 全景图中采用循环定义的方式，允许右侧溢出的部分从全景图左侧重新进入。每个预测的边界框都关联一个置信度分数 $C_{bb}$，用于量化该边界框作为有效的匹配概率。网络的目标是为每对输入的予以全景图和查询RGB图像预测 $V_P$ 、 $V_{bb}$ 和 $C_{bb}$ 。当给定一组全景图像时，具有最高综合置信度分数 $C_{bb}$ 的预测边界框即为最优观测视点。\n\n#### 神经网络结构\n\n神经网络具体结构如下图\n\n![Network architecture](../img/SPVLoc/Network_architecture.png)\n\n详细结构分析如下：\n\n1. **图像编码**：查询图像 $I_q$ 通过 `EfficientNet-S` 主干网络编码，保留第四层和最后一次下采样层的特征输出。两组特征经空间尺寸归一至 $7 \\times 7$后拼接为张量 $F$。\n2. **全景编码**：语义全景L通过 `DenseNet-121` 主干编码（延伸至第五下采样块）提取特征 $R$。图像特征通过深度互相关操（标注为 $⋆d$ ）作引导全景分支预测视口信息：\n   - 特征 $F$ 经两个连续卷积块（含卷积+批归一化+ELU激活）处理，生成 $3\\times3$ 滤波器组 $\\hat{F}$ 用于执行 $R⋆d\\hat{F}$\n   - 对 $F$ 进行全局最大池化压缩后，与R进行相乘和相减运算，生成三重图像信息注入特征.\n3. **特征融合**：各注入特征经独立卷积块处理后拼接，最终通过卷积块生成互相关特征 $R*$\n4. **特征解码**：解码器预测全景场景中透视相机视口的投影范围。\n   - 全景图像中的视口通过边界框进行检测，采用类似 $RetinaNet$ 的设计框架，包含边界框分类头(`BBox Class.`)和回归头(`BBox Reg.`) 。\n   - 边界框分类头(`BBox Class.`) 采用焦点损失函数 $\\mathcal{L}^{VP}_1$ 进行优化，而边界框回归头(`BBox Reg.`)则使用平滑L1损失函数 ${\\mathcal{L}\\_{VP2}}$ 。边界框分类头为预测结果提供置信度评分，确保概率最高的边界框即为最优视口估计。\n   - 此外，通过另一个卷积解码器头预测透视相机的精确视窗区域$V_P$，并采用二元交叉熵损失函数 ${\\mathcal{L}\\_{VP3}}$ 进行优化。\n5. **透视监督**：编码后的图像特征IE连接到卷积解码器，输出基础语义图和法线图，以确保提取出与图像内容紧密对应的特征。语义图通过交叉熵损失 $\\mathcal{L}\\_{VP4}$ 进行学习，法线图则采用余弦损失 ${\\mathcal{L}\\_{VP5}}$ 进行优化。\n\n### 基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）\n\n关联特征（记为 $R^\\star$）不仅编码了查询图像 $I_q$ 在局部全景图 $L_i$ 视窗内的信息，还应推导出两者之间的相对位姿偏移 $P_i$。由于合成生成的 $L$ 已知 $L_i$ 的精确位置，因此确定 $I_q$ 相对于 $L_i$ 的位置即等价于确定其绝对位置。所有渲染的全景图均具有统一朝向（水平对齐且指向正北），故旋转差值直接对应相机的绝对旋转。\n\n位姿估计头（Pose Head）通过处理 $R^\\star$ 来预测相机相对于全景渲染的位姿偏移（如下图所示）。\n\n![Pose_head](../img/SPVLoc/Pose_head.png)\n\n首先使用三个卷积块（前两个接最大池化层）压缩特征，展平后的特征形成一维嵌入 $r^\\star \\in \\mathbb{R}^C$（对于256×128的全景图，$C=640$）。可选地，为处理训练与测试时相机的不同视场角，$r^\\star$ 会经归一化后乘以水平视场角（弧度制）。结果输入四层MLP，输出查询图像位置的3D平移偏移量 $\\hat{t_i} \\in \\mathbb{R}^3$ 以及旋转分量（以单位四元数的对数形式表示 $\\hat{r_i} = \\log(\\hat{q})_{ij} \\in \\mathbb{R}^3$  [41]）。\nMLP通过两种损失函数优化：$\\mathcal{L}_{P1}$ 评估相对平移精度，$\\mathcal{L}_{P2}$ 评估最终旋转估计准确性。二者均采用L1距离计算。\n\n### 优化\n\n训练损失由两部分组成：MLP的两个损失函数和视口估计的五个损失函数。由于涉及七项损失，我们采用多任务学习策略，通过为每项损失分配不同权重实现平衡。利用指数映射并基于各任务的同方差不确定性（homoscedastic uncertainty）和可学习权重因子进行加权。最终总损失表达为：\n\n$$\\mathcal{L} = \\sum_{i=1}^{2} \\mathcal{L}^{P}_i e^{-\\beta_i} + \\beta_i + \\sum_{i=1}^{5} \\mathcal{L}^{VP}_i e^{-\\gamma_i} + \\gamma_i$$\n\n其中：\n- $\\beta_i$ 和 $\\gamma_i$ 是可学习的对数权重参数\n- 指数项$e^{-\\beta_i}$和$e^{-\\gamma_i}$动态调整各项损失的贡献度\n- 附加的$\\beta_i$和$\\gamma_i$项用于防止权重过度衰减\n\n### 推理\n\n在全景定位过程中，虚拟相机的位置通过二维固定网格（覆盖于平面图上）确定。网格参数包括：\n\n- 高度参数：虚拟相机高度固定为 $h_{\\text{pano}}$\n- 网格范围：由 $xy_{\\text{pano}}$ 定义\n  为确保有效的参考图像，仅对房间地板以上的区域进行采样。网格范围 $xy_{\\text{pano}}$ 应尽可能扩大，以减少全景参考图数量（因每次推理均需执行特征关联与边界框回归计算）。\n\n#### 位姿估计流程\n\n1. 参考帧筛选：选取边界框分类得分最高的前 $n$ 个参考位置\n2. 绝对位姿计算：\n   - 位姿估计头（Pose Head）输出相对位姿偏移\n   - 叠加所选参考位置的坐标，获得最终绝对平移量\n3. 预生成优化：\n   - 为平面图预渲染全景参考图及其编码特征 $R$\n   - 仅存储 $R$ 可大幅简化推理流程\n\n#### 位姿优化（Refinement）\n\n在不改动网络架构或训练流程的前提下，SPVLoc 可通过迭代提升位姿精度：\n\n1. 首轮估计：获取初始位姿结果\n2. 二次渲染：在估计位置生成新参考全景图，重新执行相对位姿回归\n3. 精度增益：由于此时初始估计已接近真实目标，第二轮的相对位姿回归问题更简单，从而输出更精确的优化位姿\n\n\n测试\n$\\mathcal{L}_{VP1}$","source":"_posts/SPVLoc.md","raw":"---\ntitle: SPVLoc：基于语义全景视口匹配的相机定位技术 —— 面向未知环境的六维位姿估计\ndata:  2025-10-18 \nmathjax: true\ntags:\n    - 三维视觉\n    - 深度估计\n    - 学习笔记\n---\n本文为对于论文《SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments》 的翻译与精炼，该论文发表于2024年的 `ECCV`上。\n\n## 基础摘要\n\n通过透视相机拍摄RGB图像，在室内环境全景语义布局表征中，实现对于视口的定位。该全景图仅仅包含室内的近似结构以及门窗标注的无纹理3D参考模型渲染生成。文章中，通过简单的卷积网络结构实现了图像-全景图匹配，并最终达成了图像-模型匹配。基于视口分类打分后对参考全景图进行排序，为查询图片找到最佳匹配项，进而估算选定全景图与查询图像之间的相对位姿。\n\n整体分析而言，该论文方法分为四个模块：\n\n1. 语义全景视口匹配（Semantic Panoramic Viewport Matching）\n2. 基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）\n3. 优化\n4. 推理\n\n## 方法介绍\n\n**SPVLoc** 方法本质上是通过给定一个二维的RGB平面图在一个无纹理的语义三位建模场景（室内）中推导出相机的六维位姿定位。模型使用的三位模型是 `Structured3D` 标注格式，其可直接从建筑模型中生成，也可以通过AI工具基于平面图自动推导获取。\n\nSPVLoc首先通过跨域图像-全景图匹配，利用语义场景模型在渲染的全景图中估算图像的视口位置。接着根据分类准确度对这些预测结果进行排名，并针对匹配度最高的参考全景图执行相对6D姿态回归。\n\n### 语义全景视口匹配（Semantic Panoramic Viewport Matching）\n\n#### 问题定义\n\n本论文的目的是为给定查询的RGB图像，获取其相对于语义三维模型的6D位姿。而本论文对此将整体任务转变为了跨域图像-全景图匹配任务。\n\n该模型通过一组N个等矩形参考布局 $L=\\{L_i|i=1,...N\\}$ 表示，每个布局单通道中包含语义标签。每个渲染图完整记录了特定视点周围的全景环境。视口 $V$ 是从全景图中截取的独立区域，精确定位了附近相机所能拍摄的实际视野范围。视角 $V_{P_i}$ 的透视畸变包含 $L_i$ 与 $I_q$ 之间的相对姿态偏移 $P_i$ 的信息。通过评分和排序视口有助于选出最优全景图 $L^*$。绝对姿态 $P$ 由 $P^*$ 与 $P_i$ 串联获取。\n\n#### 视口预测\n\n视口掩码 $V_P$ 是二值图，用于标示等距柱状投影图像中各点在透视相机视角下的可见性。其计算过程设计将全景图的所有3D点投影至透视图，该投影基于训练阶段一直的底层布局完成。{V_P} 掩码通过以下方式确定：结合3D相机位姿信息，并剔除被遮挡的部分，识别能投影到像素透视图的像素点。\n\n此外，二维边界框 $V_{bb}$ 是包围 $V_P$ 的最小轴对称矩形框。该边界框在 $360^{\\circ}$ 全景图中采用循环定义的方式，允许右侧溢出的部分从全景图左侧重新进入。每个预测的边界框都关联一个置信度分数 $C_{bb}$，用于量化该边界框作为有效的匹配概率。网络的目标是为每对输入的予以全景图和查询RGB图像预测 $V_P$ 、 $V_{bb}$ 和 $C_{bb}$ 。当给定一组全景图像时，具有最高综合置信度分数 $C_{bb}$ 的预测边界框即为最优观测视点。\n\n#### 神经网络结构\n\n神经网络具体结构如下图\n\n![Network architecture](../img/SPVLoc/Network_architecture.png)\n\n详细结构分析如下：\n\n1. **图像编码**：查询图像 $I_q$ 通过 `EfficientNet-S` 主干网络编码，保留第四层和最后一次下采样层的特征输出。两组特征经空间尺寸归一至 $7 \\times 7$后拼接为张量 $F$。\n2. **全景编码**：语义全景L通过 `DenseNet-121` 主干编码（延伸至第五下采样块）提取特征 $R$。图像特征通过深度互相关操（标注为 $⋆d$ ）作引导全景分支预测视口信息：\n   - 特征 $F$ 经两个连续卷积块（含卷积+批归一化+ELU激活）处理，生成 $3\\times3$ 滤波器组 $\\hat{F}$ 用于执行 $R⋆d\\hat{F}$\n   - 对 $F$ 进行全局最大池化压缩后，与R进行相乘和相减运算，生成三重图像信息注入特征.\n3. **特征融合**：各注入特征经独立卷积块处理后拼接，最终通过卷积块生成互相关特征 $R*$\n4. **特征解码**：解码器预测全景场景中透视相机视口的投影范围。\n   - 全景图像中的视口通过边界框进行检测，采用类似 $RetinaNet$ 的设计框架，包含边界框分类头(`BBox Class.`)和回归头(`BBox Reg.`) 。\n   - 边界框分类头(`BBox Class.`) 采用焦点损失函数 $\\mathcal{L}^{VP}_1$ 进行优化，而边界框回归头(`BBox Reg.`)则使用平滑L1损失函数 ${\\mathcal{L}\\_{VP2}}$ 。边界框分类头为预测结果提供置信度评分，确保概率最高的边界框即为最优视口估计。\n   - 此外，通过另一个卷积解码器头预测透视相机的精确视窗区域$V_P$，并采用二元交叉熵损失函数 ${\\mathcal{L}\\_{VP3}}$ 进行优化。\n5. **透视监督**：编码后的图像特征IE连接到卷积解码器，输出基础语义图和法线图，以确保提取出与图像内容紧密对应的特征。语义图通过交叉熵损失 $\\mathcal{L}\\_{VP4}$ 进行学习，法线图则采用余弦损失 ${\\mathcal{L}\\_{VP5}}$ 进行优化。\n\n### 基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）\n\n关联特征（记为 $R^\\star$）不仅编码了查询图像 $I_q$ 在局部全景图 $L_i$ 视窗内的信息，还应推导出两者之间的相对位姿偏移 $P_i$。由于合成生成的 $L$ 已知 $L_i$ 的精确位置，因此确定 $I_q$ 相对于 $L_i$ 的位置即等价于确定其绝对位置。所有渲染的全景图均具有统一朝向（水平对齐且指向正北），故旋转差值直接对应相机的绝对旋转。\n\n位姿估计头（Pose Head）通过处理 $R^\\star$ 来预测相机相对于全景渲染的位姿偏移（如下图所示）。\n\n![Pose_head](../img/SPVLoc/Pose_head.png)\n\n首先使用三个卷积块（前两个接最大池化层）压缩特征，展平后的特征形成一维嵌入 $r^\\star \\in \\mathbb{R}^C$（对于256×128的全景图，$C=640$）。可选地，为处理训练与测试时相机的不同视场角，$r^\\star$ 会经归一化后乘以水平视场角（弧度制）。结果输入四层MLP，输出查询图像位置的3D平移偏移量 $\\hat{t_i} \\in \\mathbb{R}^3$ 以及旋转分量（以单位四元数的对数形式表示 $\\hat{r_i} = \\log(\\hat{q})_{ij} \\in \\mathbb{R}^3$  [41]）。\nMLP通过两种损失函数优化：$\\mathcal{L}_{P1}$ 评估相对平移精度，$\\mathcal{L}_{P2}$ 评估最终旋转估计准确性。二者均采用L1距离计算。\n\n### 优化\n\n训练损失由两部分组成：MLP的两个损失函数和视口估计的五个损失函数。由于涉及七项损失，我们采用多任务学习策略，通过为每项损失分配不同权重实现平衡。利用指数映射并基于各任务的同方差不确定性（homoscedastic uncertainty）和可学习权重因子进行加权。最终总损失表达为：\n\n$$\\mathcal{L} = \\sum_{i=1}^{2} \\mathcal{L}^{P}_i e^{-\\beta_i} + \\beta_i + \\sum_{i=1}^{5} \\mathcal{L}^{VP}_i e^{-\\gamma_i} + \\gamma_i$$\n\n其中：\n- $\\beta_i$ 和 $\\gamma_i$ 是可学习的对数权重参数\n- 指数项$e^{-\\beta_i}$和$e^{-\\gamma_i}$动态调整各项损失的贡献度\n- 附加的$\\beta_i$和$\\gamma_i$项用于防止权重过度衰减\n\n### 推理\n\n在全景定位过程中，虚拟相机的位置通过二维固定网格（覆盖于平面图上）确定。网格参数包括：\n\n- 高度参数：虚拟相机高度固定为 $h_{\\text{pano}}$\n- 网格范围：由 $xy_{\\text{pano}}$ 定义\n  为确保有效的参考图像，仅对房间地板以上的区域进行采样。网格范围 $xy_{\\text{pano}}$ 应尽可能扩大，以减少全景参考图数量（因每次推理均需执行特征关联与边界框回归计算）。\n\n#### 位姿估计流程\n\n1. 参考帧筛选：选取边界框分类得分最高的前 $n$ 个参考位置\n2. 绝对位姿计算：\n   - 位姿估计头（Pose Head）输出相对位姿偏移\n   - 叠加所选参考位置的坐标，获得最终绝对平移量\n3. 预生成优化：\n   - 为平面图预渲染全景参考图及其编码特征 $R$\n   - 仅存储 $R$ 可大幅简化推理流程\n\n#### 位姿优化（Refinement）\n\n在不改动网络架构或训练流程的前提下，SPVLoc 可通过迭代提升位姿精度：\n\n1. 首轮估计：获取初始位姿结果\n2. 二次渲染：在估计位置生成新参考全景图，重新执行相对位姿回归\n3. 精度增益：由于此时初始估计已接近真实目标，第二轮的相对位姿回归问题更简单，从而输出更精确的优化位姿\n\n\n测试\n$\\mathcal{L}_{VP1}$","slug":"SPVLoc","published":1,"date":"2025-10-17T13:16:33.557Z","updated":"2025-10-20T16:25:10.928Z","_id":"cmgzc3rlk00017a6ehi1y6db9","comments":1,"layout":"post","photos":[],"content":"<p>本文为对于论文《SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments》 的翻译与精炼，该论文发表于2024年的 <code>ECCV</code>上。</p>\n<h2 id=\"基础摘要\"><a href=\"#基础摘要\" class=\"headerlink\" title=\"基础摘要\"></a>基础摘要</h2><p>通过透视相机拍摄RGB图像，在室内环境全景语义布局表征中，实现对于视口的定位。该全景图仅仅包含室内的近似结构以及门窗标注的无纹理3D参考模型渲染生成。文章中，通过简单的卷积网络结构实现了图像-全景图匹配，并最终达成了图像-模型匹配。基于视口分类打分后对参考全景图进行排序，为查询图片找到最佳匹配项，进而估算选定全景图与查询图像之间的相对位姿。</p>\n<p>整体分析而言，该论文方法分为四个模块：</p>\n<ol>\n<li>语义全景视口匹配（Semantic Panoramic Viewport Matching）</li>\n<li>基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）</li>\n<li>优化</li>\n<li>推理</li>\n</ol>\n<h2 id=\"方法介绍\"><a href=\"#方法介绍\" class=\"headerlink\" title=\"方法介绍\"></a>方法介绍</h2><p><strong>SPVLoc</strong> 方法本质上是通过给定一个二维的RGB平面图在一个无纹理的语义三位建模场景（室内）中推导出相机的六维位姿定位。模型使用的三位模型是 <code>Structured3D</code> 标注格式，其可直接从建筑模型中生成，也可以通过AI工具基于平面图自动推导获取。</p>\n<p>SPVLoc首先通过跨域图像-全景图匹配，利用语义场景模型在渲染的全景图中估算图像的视口位置。接着根据分类准确度对这些预测结果进行排名，并针对匹配度最高的参考全景图执行相对6D姿态回归。</p>\n<h3 id=\"语义全景视口匹配（Semantic-Panoramic-Viewport-Matching）\"><a href=\"#语义全景视口匹配（Semantic-Panoramic-Viewport-Matching）\" class=\"headerlink\" title=\"语义全景视口匹配（Semantic Panoramic Viewport Matching）\"></a>语义全景视口匹配（Semantic Panoramic Viewport Matching）</h3><h4 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h4><p>本论文的目的是为给定查询的RGB图像，获取其相对于语义三维模型的6D位姿。而本论文对此将整体任务转变为了跨域图像-全景图匹配任务。</p>\n<p>该模型通过一组N个等矩形参考布局 $L&#x3D;{L_i|i&#x3D;1,…N}$ 表示，每个布局单通道中包含语义标签。每个渲染图完整记录了特定视点周围的全景环境。视口 $V$ 是从全景图中截取的独立区域，精确定位了附近相机所能拍摄的实际视野范围。视角 $V_{P_i}$ 的透视畸变包含 $L_i$ 与 $I_q$ 之间的相对姿态偏移 $P_i$ 的信息。通过评分和排序视口有助于选出最优全景图 $L^*$。绝对姿态 $P$ 由 $P^*$ 与 $P_i$ 串联获取。</p>\n<h4 id=\"视口预测\"><a href=\"#视口预测\" class=\"headerlink\" title=\"视口预测\"></a>视口预测</h4><p>视口掩码 $V_P$ 是二值图，用于标示等距柱状投影图像中各点在透视相机视角下的可见性。其计算过程设计将全景图的所有3D点投影至透视图，该投影基于训练阶段一直的底层布局完成。{V_P} 掩码通过以下方式确定：结合3D相机位姿信息，并剔除被遮挡的部分，识别能投影到像素透视图的像素点。</p>\n<p>此外，二维边界框 $V_{bb}$ 是包围 $V_P$ 的最小轴对称矩形框。该边界框在 $360^{\\circ}$ 全景图中采用循环定义的方式，允许右侧溢出的部分从全景图左侧重新进入。每个预测的边界框都关联一个置信度分数 $C_{bb}$，用于量化该边界框作为有效的匹配概率。网络的目标是为每对输入的予以全景图和查询RGB图像预测 $V_P$ 、 $V_{bb}$ 和 $C_{bb}$ 。当给定一组全景图像时，具有最高综合置信度分数 $C_{bb}$ 的预测边界框即为最优观测视点。</p>\n<h4 id=\"神经网络结构\"><a href=\"#神经网络结构\" class=\"headerlink\" title=\"神经网络结构\"></a>神经网络结构</h4><p>神经网络具体结构如下图</p>\n<p><img src=\"/../img/SPVLoc/Network_architecture.png\" alt=\"Network architecture\"></p>\n<p>详细结构分析如下：</p>\n<ol>\n<li><strong>图像编码</strong>：查询图像 $I_q$ 通过 <code>EfficientNet-S</code> 主干网络编码，保留第四层和最后一次下采样层的特征输出。两组特征经空间尺寸归一至 $7 \\times 7$后拼接为张量 $F$。</li>\n<li><strong>全景编码</strong>：语义全景L通过 <code>DenseNet-121</code> 主干编码（延伸至第五下采样块）提取特征 $R$。图像特征通过深度互相关操（标注为 $⋆d$ ）作引导全景分支预测视口信息：<ul>\n<li>特征 $F$ 经两个连续卷积块（含卷积+批归一化+ELU激活）处理，生成 $3\\times3$ 滤波器组 $\\hat{F}$ 用于执行 $R⋆d\\hat{F}$</li>\n<li>对 $F$ 进行全局最大池化压缩后，与R进行相乘和相减运算，生成三重图像信息注入特征.</li>\n</ul>\n</li>\n<li><strong>特征融合</strong>：各注入特征经独立卷积块处理后拼接，最终通过卷积块生成互相关特征 $R*$</li>\n<li><strong>特征解码</strong>：解码器预测全景场景中透视相机视口的投影范围。<ul>\n<li>全景图像中的视口通过边界框进行检测，采用类似 $RetinaNet$ 的设计框架，包含边界框分类头(<code>BBox Class.</code>)和回归头(<code>BBox Reg.</code>) 。</li>\n<li>边界框分类头(<code>BBox Class.</code>) 采用焦点损失函数 $\\mathcal{L}^{VP}_1$ 进行优化，而边界框回归头(<code>BBox Reg.</code>)则使用平滑L1损失函数 ${\\mathcal{L}_{VP2}}$ 。边界框分类头为预测结果提供置信度评分，确保概率最高的边界框即为最优视口估计。</li>\n<li>此外，通过另一个卷积解码器头预测透视相机的精确视窗区域$V_P$，并采用二元交叉熵损失函数 ${\\mathcal{L}_{VP3}}$ 进行优化。</li>\n</ul>\n</li>\n<li><strong>透视监督</strong>：编码后的图像特征IE连接到卷积解码器，输出基础语义图和法线图，以确保提取出与图像内容紧密对应的特征。语义图通过交叉熵损失 $\\mathcal{L}_{VP4}$ 进行学习，法线图则采用余弦损失 ${\\mathcal{L}_{VP5}}$ 进行优化。</li>\n</ol>\n<h3 id=\"基于特征相关性的位姿回归（Feature-Correlation-based-Pose-Regression）\"><a href=\"#基于特征相关性的位姿回归（Feature-Correlation-based-Pose-Regression）\" class=\"headerlink\" title=\"基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）\"></a>基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）</h3><p>关联特征（记为 $R^\\star$）不仅编码了查询图像 $I_q$ 在局部全景图 $L_i$ 视窗内的信息，还应推导出两者之间的相对位姿偏移 $P_i$。由于合成生成的 $L$ 已知 $L_i$ 的精确位置，因此确定 $I_q$ 相对于 $L_i$ 的位置即等价于确定其绝对位置。所有渲染的全景图均具有统一朝向（水平对齐且指向正北），故旋转差值直接对应相机的绝对旋转。</p>\n<p>位姿估计头（Pose Head）通过处理 $R^\\star$ 来预测相机相对于全景渲染的位姿偏移（如下图所示）。</p>\n<p><img src=\"/../img/SPVLoc/Pose_head.png\" alt=\"Pose_head\"></p>\n<p>首先使用三个卷积块（前两个接最大池化层）压缩特征，展平后的特征形成一维嵌入 $r^\\star \\in \\mathbb{R}^C$（对于256×128的全景图，$C&#x3D;640$）。可选地，为处理训练与测试时相机的不同视场角，$r^\\star$ 会经归一化后乘以水平视场角（弧度制）。结果输入四层MLP，输出查询图像位置的3D平移偏移量 $\\hat{t_i} \\in \\mathbb{R}^3$ 以及旋转分量（以单位四元数的对数形式表示 $\\hat{r_i} &#x3D; \\log(\\hat{q})<em>{ij} \\in \\mathbb{R}^3$  [41]）。<br>MLP通过两种损失函数优化：$\\mathcal{L}</em>{P1}$ 评估相对平移精度，$\\mathcal{L}_{P2}$ 评估最终旋转估计准确性。二者均采用L1距离计算。</p>\n<h3 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h3><p>训练损失由两部分组成：MLP的两个损失函数和视口估计的五个损失函数。由于涉及七项损失，我们采用多任务学习策略，通过为每项损失分配不同权重实现平衡。利用指数映射并基于各任务的同方差不确定性（homoscedastic uncertainty）和可学习权重因子进行加权。最终总损失表达为：</p>\n<p>$$\\mathcal{L} &#x3D; \\sum_{i&#x3D;1}^{2} \\mathcal{L}^{P}<em>i e^{-\\beta_i} + \\beta_i + \\sum</em>{i&#x3D;1}^{5} \\mathcal{L}^{VP}_i e^{-\\gamma_i} + \\gamma_i$$</p>\n<p>其中：</p>\n<ul>\n<li>$\\beta_i$ 和 $\\gamma_i$ 是可学习的对数权重参数</li>\n<li>指数项$e^{-\\beta_i}$和$e^{-\\gamma_i}$动态调整各项损失的贡献度</li>\n<li>附加的$\\beta_i$和$\\gamma_i$项用于防止权重过度衰减</li>\n</ul>\n<h3 id=\"推理\"><a href=\"#推理\" class=\"headerlink\" title=\"推理\"></a>推理</h3><p>在全景定位过程中，虚拟相机的位置通过二维固定网格（覆盖于平面图上）确定。网格参数包括：</p>\n<ul>\n<li>高度参数：虚拟相机高度固定为 $h_{\\text{pano}}$</li>\n<li>网格范围：由 $xy_{\\text{pano}}$ 定义<br>为确保有效的参考图像，仅对房间地板以上的区域进行采样。网格范围 $xy_{\\text{pano}}$ 应尽可能扩大，以减少全景参考图数量（因每次推理均需执行特征关联与边界框回归计算）。</li>\n</ul>\n<h4 id=\"位姿估计流程\"><a href=\"#位姿估计流程\" class=\"headerlink\" title=\"位姿估计流程\"></a>位姿估计流程</h4><ol>\n<li>参考帧筛选：选取边界框分类得分最高的前 $n$ 个参考位置</li>\n<li>绝对位姿计算：<ul>\n<li>位姿估计头（Pose Head）输出相对位姿偏移</li>\n<li>叠加所选参考位置的坐标，获得最终绝对平移量</li>\n</ul>\n</li>\n<li>预生成优化：<ul>\n<li>为平面图预渲染全景参考图及其编码特征 $R$</li>\n<li>仅存储 $R$ 可大幅简化推理流程</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"位姿优化（Refinement）\"><a href=\"#位姿优化（Refinement）\" class=\"headerlink\" title=\"位姿优化（Refinement）\"></a>位姿优化（Refinement）</h4><p>在不改动网络架构或训练流程的前提下，SPVLoc 可通过迭代提升位姿精度：</p>\n<ol>\n<li>首轮估计：获取初始位姿结果</li>\n<li>二次渲染：在估计位置生成新参考全景图，重新执行相对位姿回归</li>\n<li>精度增益：由于此时初始估计已接近真实目标，第二轮的相对位姿回归问题更简单，从而输出更精确的优化位姿</li>\n</ol>\n<p>测试<br>$\\mathcal{L}_{VP1}$</p>\n","excerpt":"","more":"<p>本文为对于论文《SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments》 的翻译与精炼，该论文发表于2024年的 <code>ECCV</code>上。</p>\n<h2 id=\"基础摘要\"><a href=\"#基础摘要\" class=\"headerlink\" title=\"基础摘要\"></a>基础摘要</h2><p>通过透视相机拍摄RGB图像，在室内环境全景语义布局表征中，实现对于视口的定位。该全景图仅仅包含室内的近似结构以及门窗标注的无纹理3D参考模型渲染生成。文章中，通过简单的卷积网络结构实现了图像-全景图匹配，并最终达成了图像-模型匹配。基于视口分类打分后对参考全景图进行排序，为查询图片找到最佳匹配项，进而估算选定全景图与查询图像之间的相对位姿。</p>\n<p>整体分析而言，该论文方法分为四个模块：</p>\n<ol>\n<li>语义全景视口匹配（Semantic Panoramic Viewport Matching）</li>\n<li>基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）</li>\n<li>优化</li>\n<li>推理</li>\n</ol>\n<h2 id=\"方法介绍\"><a href=\"#方法介绍\" class=\"headerlink\" title=\"方法介绍\"></a>方法介绍</h2><p><strong>SPVLoc</strong> 方法本质上是通过给定一个二维的RGB平面图在一个无纹理的语义三位建模场景（室内）中推导出相机的六维位姿定位。模型使用的三位模型是 <code>Structured3D</code> 标注格式，其可直接从建筑模型中生成，也可以通过AI工具基于平面图自动推导获取。</p>\n<p>SPVLoc首先通过跨域图像-全景图匹配，利用语义场景模型在渲染的全景图中估算图像的视口位置。接着根据分类准确度对这些预测结果进行排名，并针对匹配度最高的参考全景图执行相对6D姿态回归。</p>\n<h3 id=\"语义全景视口匹配（Semantic-Panoramic-Viewport-Matching）\"><a href=\"#语义全景视口匹配（Semantic-Panoramic-Viewport-Matching）\" class=\"headerlink\" title=\"语义全景视口匹配（Semantic Panoramic Viewport Matching）\"></a>语义全景视口匹配（Semantic Panoramic Viewport Matching）</h3><h4 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h4><p>本论文的目的是为给定查询的RGB图像，获取其相对于语义三维模型的6D位姿。而本论文对此将整体任务转变为了跨域图像-全景图匹配任务。</p>\n<p>该模型通过一组N个等矩形参考布局 $L&#x3D;{L_i|i&#x3D;1,…N}$ 表示，每个布局单通道中包含语义标签。每个渲染图完整记录了特定视点周围的全景环境。视口 $V$ 是从全景图中截取的独立区域，精确定位了附近相机所能拍摄的实际视野范围。视角 $V_{P_i}$ 的透视畸变包含 $L_i$ 与 $I_q$ 之间的相对姿态偏移 $P_i$ 的信息。通过评分和排序视口有助于选出最优全景图 $L^*$。绝对姿态 $P$ 由 $P^*$ 与 $P_i$ 串联获取。</p>\n<h4 id=\"视口预测\"><a href=\"#视口预测\" class=\"headerlink\" title=\"视口预测\"></a>视口预测</h4><p>视口掩码 $V_P$ 是二值图，用于标示等距柱状投影图像中各点在透视相机视角下的可见性。其计算过程设计将全景图的所有3D点投影至透视图，该投影基于训练阶段一直的底层布局完成。{V_P} 掩码通过以下方式确定：结合3D相机位姿信息，并剔除被遮挡的部分，识别能投影到像素透视图的像素点。</p>\n<p>此外，二维边界框 $V_{bb}$ 是包围 $V_P$ 的最小轴对称矩形框。该边界框在 $360^{\\circ}$ 全景图中采用循环定义的方式，允许右侧溢出的部分从全景图左侧重新进入。每个预测的边界框都关联一个置信度分数 $C_{bb}$，用于量化该边界框作为有效的匹配概率。网络的目标是为每对输入的予以全景图和查询RGB图像预测 $V_P$ 、 $V_{bb}$ 和 $C_{bb}$ 。当给定一组全景图像时，具有最高综合置信度分数 $C_{bb}$ 的预测边界框即为最优观测视点。</p>\n<h4 id=\"神经网络结构\"><a href=\"#神经网络结构\" class=\"headerlink\" title=\"神经网络结构\"></a>神经网络结构</h4><p>神经网络具体结构如下图</p>\n<p><img src=\"/../img/SPVLoc/Network_architecture.png\" alt=\"Network architecture\"></p>\n<p>详细结构分析如下：</p>\n<ol>\n<li><strong>图像编码</strong>：查询图像 $I_q$ 通过 <code>EfficientNet-S</code> 主干网络编码，保留第四层和最后一次下采样层的特征输出。两组特征经空间尺寸归一至 $7 \\times 7$后拼接为张量 $F$。</li>\n<li><strong>全景编码</strong>：语义全景L通过 <code>DenseNet-121</code> 主干编码（延伸至第五下采样块）提取特征 $R$。图像特征通过深度互相关操（标注为 $⋆d$ ）作引导全景分支预测视口信息：<ul>\n<li>特征 $F$ 经两个连续卷积块（含卷积+批归一化+ELU激活）处理，生成 $3\\times3$ 滤波器组 $\\hat{F}$ 用于执行 $R⋆d\\hat{F}$</li>\n<li>对 $F$ 进行全局最大池化压缩后，与R进行相乘和相减运算，生成三重图像信息注入特征.</li>\n</ul>\n</li>\n<li><strong>特征融合</strong>：各注入特征经独立卷积块处理后拼接，最终通过卷积块生成互相关特征 $R*$</li>\n<li><strong>特征解码</strong>：解码器预测全景场景中透视相机视口的投影范围。<ul>\n<li>全景图像中的视口通过边界框进行检测，采用类似 $RetinaNet$ 的设计框架，包含边界框分类头(<code>BBox Class.</code>)和回归头(<code>BBox Reg.</code>) 。</li>\n<li>边界框分类头(<code>BBox Class.</code>) 采用焦点损失函数 $\\mathcal{L}^{VP}_1$ 进行优化，而边界框回归头(<code>BBox Reg.</code>)则使用平滑L1损失函数 ${\\mathcal{L}_{VP2}}$ 。边界框分类头为预测结果提供置信度评分，确保概率最高的边界框即为最优视口估计。</li>\n<li>此外，通过另一个卷积解码器头预测透视相机的精确视窗区域$V_P$，并采用二元交叉熵损失函数 ${\\mathcal{L}_{VP3}}$ 进行优化。</li>\n</ul>\n</li>\n<li><strong>透视监督</strong>：编码后的图像特征IE连接到卷积解码器，输出基础语义图和法线图，以确保提取出与图像内容紧密对应的特征。语义图通过交叉熵损失 $\\mathcal{L}_{VP4}$ 进行学习，法线图则采用余弦损失 ${\\mathcal{L}_{VP5}}$ 进行优化。</li>\n</ol>\n<h3 id=\"基于特征相关性的位姿回归（Feature-Correlation-based-Pose-Regression）\"><a href=\"#基于特征相关性的位姿回归（Feature-Correlation-based-Pose-Regression）\" class=\"headerlink\" title=\"基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）\"></a>基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）</h3><p>关联特征（记为 $R^\\star$）不仅编码了查询图像 $I_q$ 在局部全景图 $L_i$ 视窗内的信息，还应推导出两者之间的相对位姿偏移 $P_i$。由于合成生成的 $L$ 已知 $L_i$ 的精确位置，因此确定 $I_q$ 相对于 $L_i$ 的位置即等价于确定其绝对位置。所有渲染的全景图均具有统一朝向（水平对齐且指向正北），故旋转差值直接对应相机的绝对旋转。</p>\n<p>位姿估计头（Pose Head）通过处理 $R^\\star$ 来预测相机相对于全景渲染的位姿偏移（如下图所示）。</p>\n<p><img src=\"/../img/SPVLoc/Pose_head.png\" alt=\"Pose_head\"></p>\n<p>首先使用三个卷积块（前两个接最大池化层）压缩特征，展平后的特征形成一维嵌入 $r^\\star \\in \\mathbb{R}^C$（对于256×128的全景图，$C&#x3D;640$）。可选地，为处理训练与测试时相机的不同视场角，$r^\\star$ 会经归一化后乘以水平视场角（弧度制）。结果输入四层MLP，输出查询图像位置的3D平移偏移量 $\\hat{t_i} \\in \\mathbb{R}^3$ 以及旋转分量（以单位四元数的对数形式表示 $\\hat{r_i} &#x3D; \\log(\\hat{q})<em>{ij} \\in \\mathbb{R}^3$  [41]）。<br>MLP通过两种损失函数优化：$\\mathcal{L}</em>{P1}$ 评估相对平移精度，$\\mathcal{L}_{P2}$ 评估最终旋转估计准确性。二者均采用L1距离计算。</p>\n<h3 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h3><p>训练损失由两部分组成：MLP的两个损失函数和视口估计的五个损失函数。由于涉及七项损失，我们采用多任务学习策略，通过为每项损失分配不同权重实现平衡。利用指数映射并基于各任务的同方差不确定性（homoscedastic uncertainty）和可学习权重因子进行加权。最终总损失表达为：</p>\n<p>$$\\mathcal{L} &#x3D; \\sum_{i&#x3D;1}^{2} \\mathcal{L}^{P}<em>i e^{-\\beta_i} + \\beta_i + \\sum</em>{i&#x3D;1}^{5} \\mathcal{L}^{VP}_i e^{-\\gamma_i} + \\gamma_i$$</p>\n<p>其中：</p>\n<ul>\n<li>$\\beta_i$ 和 $\\gamma_i$ 是可学习的对数权重参数</li>\n<li>指数项$e^{-\\beta_i}$和$e^{-\\gamma_i}$动态调整各项损失的贡献度</li>\n<li>附加的$\\beta_i$和$\\gamma_i$项用于防止权重过度衰减</li>\n</ul>\n<h3 id=\"推理\"><a href=\"#推理\" class=\"headerlink\" title=\"推理\"></a>推理</h3><p>在全景定位过程中，虚拟相机的位置通过二维固定网格（覆盖于平面图上）确定。网格参数包括：</p>\n<ul>\n<li>高度参数：虚拟相机高度固定为 $h_{\\text{pano}}$</li>\n<li>网格范围：由 $xy_{\\text{pano}}$ 定义<br>为确保有效的参考图像，仅对房间地板以上的区域进行采样。网格范围 $xy_{\\text{pano}}$ 应尽可能扩大，以减少全景参考图数量（因每次推理均需执行特征关联与边界框回归计算）。</li>\n</ul>\n<h4 id=\"位姿估计流程\"><a href=\"#位姿估计流程\" class=\"headerlink\" title=\"位姿估计流程\"></a>位姿估计流程</h4><ol>\n<li>参考帧筛选：选取边界框分类得分最高的前 $n$ 个参考位置</li>\n<li>绝对位姿计算：<ul>\n<li>位姿估计头（Pose Head）输出相对位姿偏移</li>\n<li>叠加所选参考位置的坐标，获得最终绝对平移量</li>\n</ul>\n</li>\n<li>预生成优化：<ul>\n<li>为平面图预渲染全景参考图及其编码特征 $R$</li>\n<li>仅存储 $R$ 可大幅简化推理流程</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"位姿优化（Refinement）\"><a href=\"#位姿优化（Refinement）\" class=\"headerlink\" title=\"位姿优化（Refinement）\"></a>位姿优化（Refinement）</h4><p>在不改动网络架构或训练流程的前提下，SPVLoc 可通过迭代提升位姿精度：</p>\n<ol>\n<li>首轮估计：获取初始位姿结果</li>\n<li>二次渲染：在估计位置生成新参考全景图，重新执行相对位姿回归</li>\n<li>精度增益：由于此时初始估计已接近真实目标，第二轮的相对位姿回归问题更简单，从而输出更精确的优化位姿</li>\n</ol>\n<p>测试<br>$\\mathcal{L}_{VP1}$</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cmgt4jot200016d6e9boq76ca","tag_id":"cmgt4jot400036d6eaodk62fz","_id":"cmgt4jot7000f6d6edm6t90sg"},{"post_id":"cmgt4jot200016d6e9boq76ca","tag_id":"cmgt4jot500076d6e02336pe7","_id":"cmgt4jot8000h6d6e5s28165h"},{"post_id":"cmgt4jot200016d6e9boq76ca","tag_id":"cmgt4jot6000a6d6e1okdcdn5","_id":"cmgt4jot8000k6d6egeeccphe"},{"post_id":"cmgzc3rli00007a6efl2chtsj","tag_id":"cmgt4jot400036d6eaodk62fz","_id":"cmgzc3rll00027a6e4p8a7282"},{"post_id":"cmgzc3rli00007a6efl2chtsj","tag_id":"cmgt4jot8000m6d6e6t2h5y3d","_id":"cmgzc3rll00037a6edsdf2mmr"},{"post_id":"cmgzc3rli00007a6efl2chtsj","tag_id":"cmgt4jot6000a6d6e1okdcdn5","_id":"cmgzc3rll00047a6e23brghfw"},{"post_id":"cmgzc3rlk00017a6ehi1y6db9","tag_id":"cmgt4jot400036d6eaodk62fz","_id":"cmgzc3rll00057a6e8kr401o9"},{"post_id":"cmgzc3rlk00017a6ehi1y6db9","tag_id":"cmgt4jot8000m6d6e6t2h5y3d","_id":"cmgzc3rll00067a6e7k85fubn"},{"post_id":"cmgzc3rlk00017a6ehi1y6db9","tag_id":"cmgt4jot6000a6d6e1okdcdn5","_id":"cmgzc3rll00077a6e4vqchfks"}],"Tag":[{"name":"三维视觉","_id":"cmgt4jot400036d6eaodk62fz"},{"name":"SLAM","_id":"cmgt4jot500076d6e02336pe7"},{"name":"学习笔记","_id":"cmgt4jot6000a6d6e1okdcdn5"},{"name":"深度估计","_id":"cmgt4jot8000m6d6e6t2h5y3d"},{"name":"论文阅读总结","_id":"cmgt4jot9000r6d6e1jx45irf"},{"name":"数据库","_id":"cmgt4jota00146d6ecizk6o17"},{"name":"Archlinux","_id":"cmgt4jotb001s6d6e7wwk6wqo"}]}}