---
title: Deepth Anything V3
date: 2025-12-07 11:27:21
tags:
    - 三维视觉
    - 论文阅读
---
# Deepth Anything V3

## 摘要说明

`Depth Anything V3` 是字节在2025年底提出的最新深度估计算法。名义上是深度估计，但是其本身目标是通过一个足够简洁的网络去解决复合的三维重建问题。其本身指出：尽管近期研究探索了能够同时探索多项任务的统一模型，但是这些方案往往存在核心缺陷：它们通常依赖于复杂定制的架构，需要通过端到端联合从头训练，因而难以有效利用大规模预训练模型的优势。
当前模型在结构上一般只能输入单张或者成对的图像， 但 `Depth Anything V3` 跳出了这个框架束缚，接受任意视觉输入（无论是单张图像、场景多视角图像还是视频流）中恢复三维结构。
该模型将多个几何重建任务转化为密集预测任务。对于给定的 $N$ 张输入图像，模型被训练输出对应的 $N$ 张深度图和射线图，每张都与输入图像保持像素级对齐。该架构中采用标准的预训练视觉 `Transformer` 作为主干网络，充分利用其强大的特征提取能力。为处理其中任意视角的输入，该架构中引入了**自适应跨视角自注意力机制** 。该模块在前向传播过程中动态重组选定层的 `token`，实现所有视角之间的高效信息交互。最终预测环节中，该架构中使用了一种新型的双`DPT`头结构，通过采用不同融合参数处理统一组特征，实现深度值与射线值的联合输出。
此模型采用`老师-学生训练范式`来统一多样化训练数据。其本身训练数据包括来源于真实实际的深度相机采集数据、三维重建数据以及深度质量可能较差的合成数据。为解决这一问题，在该工作中采用伪标注策略：先在合成数据上训练出强大的教师单目模型，进而为所有真实数据生成密集、高质量的伪深度标签。关键在于，为保证几何一致性，该工作中会将这些密集伪深度图和原始稀疏（或含噪声） 的深度对齐。该方法在保持几何精度的同时大幅提升了标签细节和完整性。

## 模型设计

模型本身接收多样化的数据输入，包括单张图像、多视角图像集或视频流，并支持在可用时融入相机的位姿信息。

### 模型公式推导

对于每个给定的输入 $\mathcal{I} = \{I_i\}_{i=1}^{N_v}$ 。 其中每个 $I_i \in \mathbb{R}^{H \times W \times 3}$ 。当 $N_v = 1$ 时，输入为单目图像；当 $N_v > 1$ 时，则表示视频或多视角图像集合。每张图像包含深度图 $D_i \in \mathbb{R}^{H \times W}$、相机外参 $\begin{bmatrix} R_i | t_i \end{bmatrix}$ 和内参矩阵 $K_i$。相机参数也可表示为 $v_i \in \mathbb{R}^9$，包含平移 $t_i \in \mathbb{R}^3$、旋转四元数 $q_i \in \mathbb{R}^4$ 和视场角参数 $f_i \in \mathbb{R}^2$。像素点 $p = (u, v, 1)^\top$ 通过下式投影为3D点 $P = (X, Y, Z, 1)^\top$：
$$P = R_i \left( D_i(u, v) \cdot K_i^{-1}p \right) + t_i,$$
基于此，可完整重建出对应的3D视觉空间。

#### 射线映射表示

由于旋转矩阵 $R_i$ 的正交性约束难以直接预测，我们改用逐像素射线映射（per-pixel ray map）隐式表示相机位姿。该射线映射与输入图像和深度图对齐，定义如下：

- 射线参数：每条射线 $r \in \mathbb{R}^6$ 由起点 $t \in \mathbb{R}^3$（相机光心位置）和方向 $d \in \mathbb{R}^3$ 构成：$r = (t, d)$。
- 方向计算：将像素 $p$ 反投影到相机坐标系后，通过旋转矩阵 $R$ 转到世界坐标系：
$$d = R K^{-1} p$$ （不归一化 $d$，以保留投影尺度信息）。
- 射线映射：密集射线映射 $M \in \mathbb{R}^{H \times W \times 6}$ 存储所有像素的 $(t, d)$ 参数。
- 3D点重建：世界坐标系中的3D点通过逐元素运算生成：
$$P = t + D(u, v) \cdot d$$

#### 从射线映射反推相机参数

给定输入图像 $I \in \mathbb{R}^{H \times W \times 3}$，其对应的射线映射为 $M \in \mathbb{R}^{H \times W \times 6}$。该映射的前三通道存储逐像素射线起点 $M(:,:,:3)$，后三通道存储射线方向 $M(:,:,3:)$。

1. 相机中心估计
相机中心 $t_c$ 通过平均所有像素的射线起点得到：
$$t_c = \frac{1}{H \times W} \sum_{h=1}^H \sum_{w=1}^W M(h,w,:3)$$
2. 旋转矩阵 $R$ 与内参 $K$ 估计
将问题转化为单应矩阵 $H$ 的求解：
   1. 定义标准相机：假设一个内参为单位矩阵 $K_I = I$ 的虚拟相机，其像素 $p$ 对应的射线方向为 $d_I = K^{-1}p=p$。
   2. 建立单应关系：目标相机的射线方向 $d_{\text{cam}} = KRd_I$，即 $H = KR$。
   3. 优化求解：通过最小化几何误差求解最优单应矩阵 $H^*$：
$$H^* = \argmin_{\|H\|=1} \sum_{h=1}^H \sum_{w=1}^W \|H p_{h,w} \times M(h,w,3:)\|$$
使用直接线性变换（DLT）算法求解。

3. 参数分解
通过 RQ分解 将 $H^*$ 分解为：
   - 上三角矩阵 $K$（相机内参）
   - 正交矩阵 $R$（旋转矩阵）

#### 精简预测目标与高效相机参数估计

现有方法通常通过多任务学习构建统一3D模型，大致分为两类：

1. 点云映射（Point Maps）：无法保证一致性。
2. 冗余目标组合：联合预测位姿、局部/全局点云和深度，虽能提升位姿精度，但引入目标纠缠（entanglement）反而降低性能。

而在本文中通过**深度-射线**能最小且充分的表示能同时建模场景结构和相机运动。然而，在推理时从射线图恢复相机位姿的计算成本较高。在该论文中通过添加一个轻量级的相机头 $\mathcal D_c$ 来解决这个问题。这个 `Transformer` 用于处理相机 `token` 来预测  视场角（f ∈ ℝ²）、四元数表示的旋转（q ∈ ℝ⁴）和平移量（t ∈ ℝ³）。由于每个视图仅需处理一个token，因此增加的计算开销可以忽略不计，从而实现近似实时建图。

### 模型结构

模型具体结构如下

![模型结构](Pihz-26.github.io/source/img/Deepth-Anything/模型结构.png)

模型由三个主要部分组成：单一Transformer模型作为主干网络，可选配相机编码器（用于位姿条件化）以及双DPT头（Dual-DPT head）用于生成预测结果。

#### 单一Transformer主干网络

在网络设计中，采用 $L$ 块 从大规模单目图像数据上预训练的 `Vision Transformer` 。 模型中，通过输入自适应的自注意力机制，在无需修改结构的情况下，仅重排输入 token 即可实现跨视图的推理。在模型中，将 $Transformer$ 分为两组： $L_s$ 和 $L_g$。前$L_s$层在单图像内进行自注意力计算，后$L_g$层则交替执行跨视图和视图内注意力（通过张量重排联合处理所有token）。实际配置取 $Ls:Lg=2:1（L=Ls+Lg）$，消融实验表明该比例在性能和效率间达到最优平衡。此设计具备输入自适应性：单图像输入时，模型可无缝退化为单目深度估计，无需额外开销。

#### 相机条件注入

为了无缝处理带位置和不带位姿的输入，该模型在每个图像前都添加一个相机$token$。

- 若已知相机参数 $(K_i, R_i, t_i)$ ，则其对应编码通过一个轻量级的 $MLP (\mathcal{E}_c)$ 获取 $c_i=\mathcal{E}_c(f_i, q_i, t_i)$。
- 若相机参数未知，则使用共享可学习的 $token \quad c_i$。

相机$token$与图像块$token$拼接后参与所有注意力计算，提供显式几何上下文或统一的学习占位符。

#### 双DPT头

在最终的预测阶段中，采用了一个新设计的$DPT$头结构，可以联合生成稠密深度值和光线值。

![双DPT头](Pihz-26.github.io/source/img/Deepth-Anything/双DPT头.png)

给定来自主干网络的一组特征，双DPT头部首先通过一组共享的重组模块对其进行处理。随后，处理后的特征通过两套不同的融合层进行融合：一套用于深度分支，另一套用于光线分支。最终，两个独立的输出层分别生成深度图和光线图的预测结果。这种架构确保两个分支基于同一组处理后的特征进行操作，仅在最后的融合阶段有所不同。这样的设计能够促进两个预测任务之间的强交互，同时避免冗余的中间表示。

### 训练

#### 教师-学生训练范式

该模型的训练数据集来源于多种渠道，包括真实场景的深度采集、三维重建数据以及合成数据集。真实世界的深度数据往往存在噪声且不完整，这限制了其作为监督信号的有效性。
为解决这一问题，我们专门使用合成数据训练了一个单目相对深度估计的"教师"模型，用于生成高质量的伪标签。这些伪深度图通过RANSAC最小二乘法与原始稀疏或有噪声的真实标注进行对齐，在保持几何精度的同时，显著提升了标签的细节完整度。该模型命名为$Depth-Anything-3-Teacher$。
该教师模型的训练基于一个大规模的合成数据集，其中涵盖了室内外场景、物体中心视角以及多样化的自然场景，能够有效捕捉精细的几何特征。

#### 训练损失

模型 $F_{\theta}$ 将输入图像 $I$ 映射到一组输出：深度图 $\hat{D}$、光线图 $\hat{R}$ 和可选的相机位姿 $\hat{c}$（可选输出）。在计算损失前，所有真实标注信号均通过统一尺度因子归一化，该尺度定义为有效重投影点图 $P$ 的 $\ell_2$ 范数均值，以确保多模态数据量纲一致并稳定训练过程。
整体训练目标为加权损失之和：

$$
\mathcal{L} = \mathcal{L}_D(\hat{D}, D) + \mathcal{L}_M(\hat{R}, M) + \mathcal{L}_P(\hat{D} \odot d + t, P) + \beta\mathcal{L}_C(\hat{c}, v) + \alpha\mathcal{L}_{\text{grad}}(\hat{D}, D)
$$
其中深度置信损失定义为：
$$
\mathcal{L}_D(\hat{D}, D ; D_c) = \frac{1}{|\Omega|} \sum_{p \in \Omega} m_p \left[ D_{c,p} \left| \hat{D}_p - D_p \right| - \lambda_c \log D_{c,p} \right]
$$
$D_{c,p}$ 表示深度 $D_p$ 的置信度。所有损失基于 $\ell_1$ 范数，权重设为 $\alpha = 1$, $\beta = 1$。梯度损失 $\mathcal{L}_{\text{grad}}$ 惩罚深度梯度差异：
$$
\mathcal{L}_{\text{grad}}(\hat{D}, D) = \|\nabla_x \hat{D} - \nabla_x D\|_1 + \|\nabla_y \hat{D} - \nabla_y D\|_1
$$
$\nabla_x$, $\nabla_y$ 为水平和垂直有限差分算子）。该损失在保持边缘锐度的同时确保平面区域平滑性，实际中取 $\alpha = 1$, $\beta = 1$。

## 教师-学生学习

由于真实世界数据集质量较差，所以在该工作中仅使用合成数据训练教师模型，以提供对真实数据的监督。该教师模型被训练为单目相对深度预测器。在推理或监督过程中，可利用带有噪声的真实深度数据提供尺度和平移参数，从而实现预测的相对深度与绝对深度测量值的对齐。

在数据集上，相比于上一代模型（$Depth-Anything-V2$）进行了极大的扩展，验证了数据规模化的优势。值得注意的是，虽然改进后的深度表征在标准2D评估指标上未呈现显著提升，但能生成质量更优的3D点云——其几何畸变更少、场景结构更逼真。需特别说明的是，教师网络主干直接沿用上述DA3框架，仅包含DINOv2视觉Transformer与DPT解码器，未引入特殊架构修改。

### 训练损失

在几何监督方面，除了标准的深度梯度损失外，文章中还提出了全局-局部损失进行ROE对齐。为了优化局部几何结构，引入了一种距离加权的表面法向损失：
- 对于中心像素，采样四个邻近点并计算非归一化法向量 $n_i$
- 通过以下权重降低较远邻域的贡献：
$w_i = \sum_{j=0}^{4} \|n_j\| - \|n_i\|$
- 计算更接近真实局部表面法向的平均法向：
$n_m = \sum_{i=0}^{4} w_i \frac{n_i}{\|n_i\|}$

最终的表面法向损失定义为：
$L_N = E(\hat{n}_m, n_m) + \sum_{i=0}^{4} E(\hat{n}_i, n_i)$
其中，$E$ 表示法向量之间的角度误差。

在天空区域和仅包含对象的背景区域（如目标检测数据集）中，地面真实深度值是未定义的。为了避免这些区域对深度预测产生负面影响，并方便下游任务的集成，我们同时预测与深度输出对齐的天空掩码和物体掩码，并使用 MSE 损失进行监督。
总体训练目标为：
$L_T = \alpha L_{grad} + L_{gl} + L_N + L_{sky} + L_{obj}, \quad (\alpha = 0.5)$
其中，$L_{grad}$（梯度损失）、$L_{gl}$（全局-局部损失）、$L_{sky}$（天空掩码损失）、$L_{obj}$（物体掩码损失）分别对应各个监督项的贡献。

### 单目模型的教师-学生训练

该方法在教师-学生框架下额外训练了一个单目深度模型。遵循DA2框架，该单目学生模型使用教师生成的伪标签在无标注图像上进行训练。与DA2的关键区别在于预测目标：本模型预测深度图，而DA2预测视差图。我们进一步采用与教师模型相同的损失函数（作用于伪深度标签）监督学生模型。该单目模型同时预测相对深度值，仅通过教师监督的无标注数据训练，其在标准单目深度基准上达到了最先进性能。

### 度量深度模型的教师监督训练

为了进一步证明教师模型可用于训练具有清晰边界的度量深度估计模型。参考Metric3Dv2，通过规范相机空间变换解决焦距变化导致的深度歧义，具体采用尺度因子$f_c/f$对真实深度值进行缩放（$f_c$和$f$分别表示规范焦距和相机焦距）。为保持细节锐度，直接以教师模型的预测作为训练标签，并将其深度预测的尺度和偏移量对齐至真实度量深度标签进行监督。
