---
title: DUSt3R and MONO3R
date: 2025-11-23 13:34:18
tags:
    - 三维视觉
    - 论文阅读
---

# DUSt3R and MONO3R

`DUSt3R` 为全程使用前馈深度学习网络，从二维视图恢复三维结构的开篇之作，`MONO3R` 为其增加先验点云约束的改进作品。学习这两篇论文对于理解该三维视觉流派具有重要意义。

## 背景说明

要理解这两篇论文，首先需要理解传统方法中对于三维结构恢复中的实现方式和局限性，以及三维恢复问题的意义。

### 意义

本质上，通过无结构的多视图恢复三维结构这一核心任务，是少数几个被长期讨论并研究的三维视觉问题，其核心定义在于根据一组图像，获取三维结构和相机参数（内参和外参）。这一问题能够有效运用于诸多问题，包括 建图、导航、建模和机器人。其本质是对于一系列三维视觉问题的完整整合。笔者认为，该问题是三维视觉领域目前的几个最终问题之一。

### 传统方法及其局限性

现代意义上的三维重建方法基本上是源于过去十几年来计算机视觉各个子领域的突破，包括：关键点检测与匹配、鲁棒估计、运动结构恢复和（SAM）、捆绑优化（BA）和密集多视图匹配（MVS）。
最终， SFM 和 MVS 的流程被整体归纳为一系列最小化问题：特征点匹配、求解本质（单应）矩阵、三角化点云、稀疏场景重建、估计相机参数和场景稠密重建。尽管，可以通过替换和优化其中的部分模块来实现性能优化、水平题设个，但是这一复杂的处理链存在明显缺陷———每个子问题的求解误差会反复的传导到下一个环节中，存在的累计误差与噪声会显著影响下一环节，中间需要大量的优化操作。并且，从当前深度学习的角度来看，其各个子模块之间缺乏信息交流互通（例如稠密重建受益于相机位姿估计生成的点云，反之亦然）。
正是由于这些局限性，导致这些方法在诸多场景下容易失效，这一问题或许可以通过在原有框架上提出新方法环节，但是归根到底是传统方法算法框架存在局限性。

## 方法说明

`DUSt3R` 和 `MONO3R` 都是完全基于神经网络的三维结构恢复方法，`MONO3R` 是在 `DUSt3R` 上添加了三维先验估计一致性的方法改进， 如下将先后介绍两个方法，说明其具体流程和异同。

### DUSt3R

![DUSt3R Network](/img/DUSt3R-and-MONO3R/DUSt3R网络结构图.png)

对于该网络，其输入为两张RGB图像 $\mathbf{I}_1, \mathbf{I}_2 \in \mathbb{R}^{W \times H \times 3}$, 其输出为对应的三维点图 $\mathbf{X}^{1,1}, \mathbf{X}^{2,1} \in \mathbb{R}^{W \times H \times 3}$及置信度图$\mathbf{C}^{1,1}, \mathbf{C}^{2,1} \in \mathbb{R}^{W \times H}$。两幅图以$\mathbf{I}_1$的坐标系为基准。此处假设输入图像分辨率为 $W \times H$。

#### 网络架构

该网络中包含两条对称的分支，各自处理一幅图像，每条分支由图像编码器、解码器和回归头组成。输入图像首先通过权重共享的 $VIT$ 编码器进行孪生编码，生成两组特征标记 

$$
\mathbf{F}_1 = \text{Encoder}(\mathbf{I}_1), \quad \mathbf{F}_2 = \text{Encoder}(\mathbf{I}_2)
$$

随后解码器对两组特征进行联合推理。解码器采用带交叉注意力的通用 $Transformer$ 架构：每个解码器一次执行自注意力（同视图标记间交互）、交叉注意力（跨视图标记间交互）和 $MLP$ 前馈。在该过程中，解码过程中双分支持续共享信息，保证了正确对其点图。对于第 $i$ 个解码快的计算过程为

$$
\mathbf{G}_i^1 = \text{DecoderBlock}_i^1(\mathbf{G}_{i-1}^1, \mathbf{G}_{i-1}^2), \quad
\mathbf{G}_i^2 = \text{DecoderBlock}_i^2(\mathbf{G}_{i-1}^2, \mathbf{G}_{i-1}^1)
$$

其中$i=1,...,B$，初始状态$\mathbf{G}_0^1 := \mathbf{F}_1$，$\mathbf{G}_0^2 := \mathbf{F}_2$。$\text{DecoderBlock}_i^v(\cdot)$表示分支$v \in \{1,2\}$的第$i$个模块。最终，各分支的回归头将解码器标记转换为点图与置信图：

$$
\mathbf{X}^{1,1}, \mathbf{C}^{1,1} = \text{Head}^1(\mathbf{G}_0^1, ..., \mathbf{G}_B^1), \quad
\mathbf{X}^{2,1}, \mathbf{C}^{2,1} = \text{Head}^2(\mathbf{G}_0^2, ..., \mathbf{G}_B^2)
$$

#### 特征分析

输出的点图$\mathbf{X}^{1,1}$和$\mathbf{X}^{2,1}$回归时存在未知尺度因子。需注意的是，该通用架构并未显式强加几何约束，因此点图不一定符合物理相机模型。我们让网络从仅包含几何一致点图的训练集中自主学习先验知识，这种通用架构通过利用强预训练技术，最终超越了现有任务专用架构的性能。具体学习过程将在下面训练中详述。

#### 训练

##### 3D 回归损失

该模型的训练目标完全基于三维空间中的回归任务。设真实三维点图分别为 $\overline{\mathbf{X}}^{1,1}$ 和 $\overline{\mathbf{X}}^{2,1}$，其有效像素区域由式(1)定义的两个集合 $\mathcal{D}^1, \mathcal{D}^2 \subseteq \{1...W\} \times \{1...H\}$ 确定。对于视图 $v \in \{1,2\}$ 中的有效像素 $i \in \mathcal{D}^v$，回归损失定义为欧氏距离：

$$
\ell_{\text{regr}}(v,i) = \left| \frac{1}{z}\mathbf{X}^{v,1}_i - \frac{1}{\overline{z}}\overline{\mathbf{X}}^{v,1}_i \right|
$$

为消除预测值与真实值之间的尺度歧义，此处通过缩放因子 $z = \text{norm}(\mathbf{X}^{1,1}, \mathbf{X}^{2,1})$ 和 $\overline{z} = \text{norm}(\overline{\mathbf{X}}^{1,1}, \overline{\mathbf{X}}^{2,1})$ 对点图进行归一化处理。该缩放因子表示所有有效点到原点的平均距离：

$$
\text{norm}(\mathbf{X}^1,\mathbf{X}^2) = \frac{1}{|\mathcal{D}^1| + |\mathcal{D}^2|} \sum_{v\in{1,2}} \sum_{i \in \mathcal{D}^v} ||\mathbf{X}^v_i||
$$

该设计通过尺度归一化实现了以下特性：

1. 对场景绝对尺度具有不变性
2. 确保不同场景样本的损失值具有可比性
3. 使网络专注于学习相对三维几何结构

##### 置信度感知损失

在实际场景中，存在部分无法明确界定的三维点，例如天空或半透明物体上的点。更普遍地说，图像中某些区域通常比其他区域更难预测。因此，该方法中联合学习为每个像素预测一个置信度分数，表征网络对该像素预测结果的确信程度。
最终的训练目标为所有有效像素的置信度加权回归损失

$$
\mathcal{L}{\text{conf}} = \sum_{v\in{1,2}} \sum_{i\in\mathcal{D}^v} \left[ \mathbf{C}^{v,1}_i \ell_{\text{regr}}(v,i) - \alpha \log \mathbf{C}^{v,1}_i \right]
$$

其中 $\mathbf{C}^{v,1}_i$ 表示像素 $i$ 的置信度分数，$\alpha$ 是控制正则化强度的超参数。为保障置信度严格为正，我们通常定义 $\mathbf{C}^{v,1}_i = 1 + \exp(\hat{\mathbf{C}}^{v,1}_i) > 1$。此处的 $- \alpha \log \mathbf{C}^{v,1}_i$ 其本质是一个正则化项，否则 $\mathbf{C}^{v,1}_i$ 会直接趋近于0。

通过该目标函数训练网络 $\mathcal{F}$，可以在无需显式监督的情况下自动学习置信度估计。

#### 下游应用

1. 点匹配
   通过在三维点图空间进行最近邻(NN)搜索，可快速建立两幅图像间的像素对应关系。为提升精度，通常保留双向匹配对：
   $$
   \mathcal{M}^{1,2} = \{(i,j) | i=\text{NN}^{1,2}_1(j) \ \text{and} \ j=\text{NN}^{2,1}_1(i)\}
   $$
   $$
   \text{NN}^{n,m}_k(i) = \mathop{\mathrm{arg\,min}}\limits_{j\in\{0,...,WH\}} \| \mathbf{X}^{n,k}_j - \mathbf{X}^{m,k}_i\|
   $$
2. 内参恢复
   由于点图 $\mathbf{X}^{1,1}$ 处于 $I^1$ 的坐标系中，可通过优化求解相机内参。假设主点居中且像素为方形时，仅需估计焦距
   $$
   f_1^* = \mathop{\mathrm{arg\,min}}\limits_{f_1} \sum_{i,j} \mathbf{C}^{1,1}_{i,j} \left| (i',j') - f_1 \frac{(\mathbf{X}^{1,1}_{i,j,0}, \mathbf{X}^{1,1}_{i,j,1})}{\mathbf{X}^{1,1}_{i,j,2}} \right|
   $$
   其中 $i'=i-\frac{W}{2}$, $j'=j-\frac{H}{2}$。采用Weiszfeld算法等迭代求解器可快速收敛。
3. 相对位姿估计
   有两种实现方式：
   1. 传统方法：先进行二维匹配和內参恢复，再通过本质矩阵计算位姿
   2. 直接法：对点图 $\mathbf{X}^{1,1} \leftrightarrow \mathbf{X}^{1,2}$ 进行Procrustes对齐：
$$
\mathbf{R}, \mathbf{t} = \mathop{\mathrm{arg\,min}}\limits_{\sigma,\mathbf{R},\mathbf{t}} \sum_i \mathbf{C}^{1,1}_i\mathbf{C}^{1,2}_i |\sigma(\mathbf{R}\mathbf{X}^{1,1}_i + \mathbf{t}) - \mathbf{X}^{1,2}_i|^2
$$
为提高鲁棒性，建议最终采用PnP-RANSAC方案。

4. 绝对位姿估计
   设查询图像为 $I^Q$，参考图像 $I^B$ 具有已知2D-3D对应关系：
   - 方案A：估计 $I^Q$ 内参 → 建立 $I^Q$-$I^B$ 的2D匹配 → 通过 $PnP-RANSAC$ 求解
   - 方案B：计算 $I^Q$-$I^B$ 相对位姿 → 根据 $\mathbf{X}^{B,B}$ 与真实点图的尺度关系转换到世界坐标系。

#### 全局对齐方法

当前网络 $\mathcal{F}$ 仅能处理图像对输入。论文中提出一种快速后处理优化方案，可将多幅图像预测的点图对齐到统一三维空间。

 图像关系图构建
给定场景图像集 $\{I^1,I^2,...,I^N\}$：

- 顶点集 $\mathcal{V}$：$N$ 幅图像
- 边集 $\mathcal{E}$：通过图像检索或网络 $\mathcal{F}$（H100 GPU 耗时 ≈40ms/对）检测视觉重叠
- 边筛选：剔除平均置信度 $\mathbf{C}^{n,m}$ 低于阈值的图像对

2. 全局优化框架
对每条边 $e=(n,m)\in\mathcal{E}$ 预测：

成对点图 $\mathbf{X}^{n,e}$ 和 $\mathbf{X}^{m,e}$
置信度图 $\mathbf{C}^{n,e}$ 和 $\mathbf{C}^{m,e}$

引入每边对应的位姿 $\mathbf{P}_e\in\mathbb{R}^{3\times4}$ 和尺度因子 $\sigma_e>0$，建立优化目标：

$$
{\chi^{n*}} = \mathop{\mathrm{arg\,min}}\limits_{\chi,\mathbf{P},\sigma} \sum_{e\in\mathcal{E}} \sum_{v\in{n,m}} \sum_{i=1}^{H \times W} \mathbf{C}^{v,e}_i \left| \chi^v_i - \sigma_e \mathbf{P}_e \mathbf{X}^{v,e}_i \right|
$$

其中约束 $\prod_e \sigma_e = 1$ 避免零解退化。优化关键特性：

对同一边 $e$，$\mathbf{P}_e$ 同时对齐 $\mathbf{X}^{n,e}$ 和 $\mathbf{X}^{m,e}$ 到全局坐标 $\chi^n,\chi^m$
采用三维投影误差（而非传统BA的二维重投影误差）
标准梯度下降即可快速收敛（GPU上仅需数秒）

1. 相机参数恢复拓展
通过替换参数化形式：
$$
\chi^{n}_{i,j} := \mathbf{P}_n^{-1} h\left( \mathbf{K}_n^{-1} [i\mathcal{D}^n_{i,j}; j\mathcal{D}^n_{i,j}; \mathcal{D}^n_{i,j}] \right)
$$

可同步优化所有相机参数：

位姿 $\{\mathbf{P}_n\}$
内参 $\{\mathbf{K}_n\}$
深度图 $\{\mathcal{D}^n\}$

### Mono3R

`Mono3R` 本质上是在  `DUSt3R` 的基础上引入了单目几何先验信息（通过MoGe模型）和单目引导的优化模块，显著提升了弱纹理区域和和低光照条件下的多视角重建质量。
故而，对此部分的介绍注重于模型实现与训练以及下游任务实现。

#### 网络结构

其整体网络结构图如下
![MONO3R Network](/img/DUSt3R-and-MONO3R/MONO3R网络结构图.png)

给定一对图像 $\{I_i\}_{i=1}^2$（$I_i \in \mathbb{R}^{H \times W \times 3}$，其中 $H$ 和 $W$ 为图像尺寸）且未提供预先计算的相机外参和内参，该网络的目标是预测每一帧的稠密点云映射 $\{P_i\}_{i=1}^2$（$P_i \in \mathbb{R}^{H \times W \times 3}$）。点云映射 $P_i$ 将每个像素 $y = (u, v)$ 关联到其对应的三维场景点 $P_i(y) \in \mathbb{R}^3$，这些坐标以 $I_1$ 的坐标系表示。

整体框架包含两个互补分支和一个优化模块：基于 `DUSt3R` 架构，成对分支通过特征匹配处理图像对，同时建立跨视图对应关系并回归成对点图；单目分支采用 `MoGe` 模型 提取鲁棒视觉特征，并从单张图像生成高质量单目点图。单目引导优化模块通过以下方式整合双分支信息：

1. 全局Sim(3)对齐，为单目点图建立统一坐标系；
2. 利用单目先验优化成对点图的迭代程序。

该设计在保证最终点图兼具多视角一致性与单视角鲁棒性的同时，有效解决了固有对齐噪声问题。

#### 成对分支 （Pairwise Branch）

采用 `DUSt3R` 框架作为成对处理模块，该模块能够联合提取跨视图特征并预测一致性点图。其处理流程如下：给定图像对后，权重共享的 $ViT$ 编码器独立处理每张输入图像以提取初始特征；为实现不同视图间的信息交互，采用由12个堆叠 $Transformer$ 模块（交替使用自注意力与交叉注意力层）构成的双向解码器，通过全面跨视图信息交换生成多视图感知特征；随后，$DPT$ 回归头通过分层特征融合聚合来自不同解码器层的中间特征，并将融合后的特征用于预测两帧在 $I_1$ 坐标系下的点图 $\{P_i'\}_{i=1}^2$及置信度$\{w_i'\}_{i=1}^2$ 。我们通过双线性插值简单提升融合特征的空间分辨率以匹配原图尺寸，最终获得成对特征 $\{F_i\}_{i=1}^2$（$F_i \in \mathbb{R}^{H \times W \times C_{\text{pair}}}$ ，其中 $C_{\text{pair}}$ 为成对特征维度）。

![MONO3R 成对分支局限性](/img/DUSt3R-and-MONO3R/MONO3R-成对分支局限性.png)

事实上，仅仅依赖成对分支进行结果预测是不足的，表面上单目点图已经通过全局对齐的方法进行了坐标系的统一，但是对齐后的结果仍然表现出显著的几何不一致性。

#### 单目分支 (Monocular Branch)

尽管当前多视图匹配方法在几何估计方面有所突破，但其在遮挡、无纹理区域和反光表面等挑战性场景中仍存在固有局限。为解决这些问题，我们通过单目视觉线索增强多视图管线，使其在模糊条件下仍能提供鲁棒的场景理解。具体而言，`MONO3R`采用 `MoGe` 模型的预训练单目几何骨干网络——该模型因其在多样化真实场景数据上的卓越表现而被选用。单目编码器采用 `DINOv2` ，其 $14 \times 14$ 像素的切片尺寸可并行输出所有输入图像的特征标记。通过轻量级CNN头部，从这些标记中提取仿射不变的点图$\{\hat{M}_i\}_{i=1}^2$。与成对分支类似，我们将头部中间层特征通过双线性插值恢复至原始空间分辨率，得到单目特征$\{\hat{F}_i\}_{i=1}^2$（$F_i \in \mathbb{R}^{H \times W \times C_{\text{mono}}}$，其中 $C_{\text{mono}}$ 为单目特征维度）。
该单目分支在基于匹配的方法通常失效的挑战性场景中展现出特殊优势，即使面对无纹理表面或复杂光照条件仍能保持稳定预测。这种互补性优势促使我们设计了将单目预测与多视图重建相融合的方案。

#### 基于单目线索的优化模块 （Monocular Cues Guided Refinement）

该模块通过两阶段处理流程增强初始成对点图：首先对单目点图$\{\hat{M}_i\}_{i=1}^2$和特征$\{\hat{F}_i\}_{i=1}^2$进行全局Sim(3)对齐，计算最优相似变换以粗配准单目点图与成对预测结果，从而建立单目与多视表示间的初始几何一致性；随后启动迭代式单目引导优化过程，将已对齐的单目先验作为几何约束逐步优化成对点图。

**全局Sim(3)对齐** 方法通过最小二乘优化计算全局尺度因子 $s^G_i$ 、平移向量 $\mathbf{t}^G_i$ 和旋转矩阵 $\mathbf{R}^G_i$ ，将每个单目点图与对应的成对点图进行粗对齐。
$$
s_i^G, \mathbf{t}_i^G, \mathbf{R}_i^G = \underset{s,\mathbf{t},\mathbf{R}}{\arg\min} \sum_{y} w_i^0(y) \left\| s \cdot \mathbf{R} \hat{M}_i(y) + \mathbf{t} - P_i(y) \right\|^2
$$
$$
M_i = s_i^G (\mathbf{R}_i^G \hat{M}_i + \mathbf{t}_i^G)
$$

其中坐标 $y \in H \times W$ ， $\{M_i\}_{i=1}^2$ 表示对齐后的单目点图，置信度权重$\{w_i^0\}_{i=1}^2$用于排除不可靠预测区域（如天空、极端深度范围和困难区域）。该步骤通过空间变换将单目点图与成对预测置于同一坐标系，为后续精细化处理奠定基础。

![单目先验效果](/img/DUSt3R-and-MONO3R/单目先验效果.png)

深度预测与三维点云的定性对比（基于真实场景图像）
方法与对比：

- 可视化标注：彩色相机锥体示意估计的相机位姿
- 第一组结果：`MONO3R` 准确重建金属管道的细长管状结构（顶部两图），`DUSt3R` 则出现明显形变
- 第二组结果：面对重复纹理的门板（中部两图），`MONO3R`稳定恢复平面结构，`DUSt3R` 产生违背平面先验的虚假深度突变

**精细化调整** 尽管统一的 Sim(3) 变换提供了粗对齐，对齐后的点云图 $M_i$ 仍与成对点云图 $P_i$ 存在残差偏移。这种偏移源于点云估计的预测噪声以及单目与成对估计模型之间固有的域差异。为解决这一问题，`MONO3R` 提出了一种基于单目线索引导的迭代精细化模块来持续提升点云图精度。
首先，我们通过连接多模态输入来编码条件特征：
$$
x^{cond}_i = E_{cond} ([M_i, \hat{F}_i, F_i, w_i, I_i])
$$
其中 $E_{cond}$ 是轻量级卷积特征编码网络。该条件特征 $x^{cond}_i$ 驱动基于 ConvGRU 的更新器来优化隐状态 $h^{j-1}_m$。在步骤 $j$ 时：
$$
z^j = \sigma(Conv([h^{j-1}_M, x^j_S], W_z) + c_k)
$$
$$
r^j = \sigma(Conv([h^{j-1}_M, x^j_S], W_r) + c_r)
$$
$$
\tilde{h}^j_M = \tanh(Conv([r^j \odot h^{j-1}_M, x^j_S], W_h) + c_h)
$$
$$
h^j_M = (1-z^j) \odot h^{j-1}_M + z^j \odot \tilde{h}^j_M,
$$
其中上下文特征 $c_k, c_r, c_h$ 和初始状态 $h^0_m = \tanh(\hat{F}_i)$ 。通过卷积层从隐状态 $h^j_M$ 解码残差点云偏移量 $\Delta p$ 来更新成对点云图：
$$
P^{j+1}_i = P^j_i + \Delta p。
$$
经过 $N$ 次迭代后，最终获得精细化点云图 $P^N_i$。

#### 训练

采用三维回归损失函数来监督成对分支和单目引导精细化模块的输出。将精细化模块经过N次迭代产生的点云图集合记为$\{P_i\}_{i=0}^{N-1}$，对迭代损失采用指数增长权重。总损失函数由成对分支损失 $\mathcal{L}_{pair}$ 与精细化模块损失 $\mathcal{L}_{refine}$ 构成：

$$\begin{aligned}
\mathcal{L}_{refine} &= \sum_{v=1}^{N} \sum_{i=1}^{2} \sum_{k=1}^{H\times W} \gamma^{N-v} \left\| \frac{1}{z}P_{i,k}^v - \frac{1}{\bar{z}}\bar{P}_{i,k}^v \right\| \\
\mathcal{L}_{pair} &= \sum_{i=1}^{2} \sum_{k=1}^{H\times W} \left( w_k^0 \left\| \frac{1}{z}P_{i,k}^0 - \frac{1}{\bar{z}}\bar{P}_{i,k}^0 \right\| - \alpha \log w_k^0 \right)
\end{aligned}$$

其中：

- $\gamma=0.9$ 为衰减系数
- $\bar{P}$ 表示真实点云图
- $z$ 与 $\bar{z}$ 分别为预测值和真值的归一化因子
- $w_k^0$ 为像素级置信度得分，实现基于置信度的自适应损失加权
- $\alpha$ 为平衡超参数

**实现细节** 。单目编码器与解码器分支继承自 `MoGe` 的网络架构和预训练权重。除点云图外，MoGe的输出还包含用于标识点云图有效性的布尔掩码（例如天空区域通常被预测为无效区域）。为避免不合理坐标值影响后续训练，我们将这些无效区域的预测值设为零。`MoGe` 的头部采用多层卷积网络，我们从中特定层提取64通道特征图（ $C_{mono}=64$ ）并通过上采样匹配图像尺寸。
配对编码器、解码器及dpt头部继承DUSt3R[34]架构与预训练权重。解码器包含12个模块，每个模块均由自注意力层和交叉注意力层构成。从dpt头部提取的128通道特征图（$C_{pair}=128$）用于后续处理。
训练过程中，我们冻结单目分支参数，仅优化配对分支末端两个解码模块及头部，实现效率与性能的平衡。消融实验部分将具体分析各模块优化对最终性能与训练时长的影响。
对于Sim(3)对齐，采用 `Umeyama` 算法，其权重由配对分支预测的置信度决定。精炼阶段使用`ConvGRU` 架构，设置迭代次数 $N=2$（该参数的影响参见消融实验）。受计算资源限制，模型仅在 $224$ 分辨率下训练，这已足够验证方法的有效性。

**训练与测试数据**。`MONO3R` 参照`DUSt3R`的数据准备方案，采用混合数据集提供的图像对进行训练，包括：MegaDepth、ARKitScenes、Static Scenes 3D、BlendedMVS、ScanNet++、Co3Dv2和Waymo。这些数据集覆盖室内/室外无界场景以及真实/合成数据，虽规模略小于DUSt3R但仍具有可比性。训练在8块V100 GPU上以224px分辨率持续约3天。

测试集包含三个层次：物体级DTU、室内场景级7Scenes与NRGBD，以及室外无界场景ETH3D和Tanks & Temples，确保与训练集无交集。所有测试均采用固定 $224×224$ 分辨率。尽管`DUST3R`和 `Fast3R` 的推理模型使用512与224混合分辨率训练，这不影响评估公平性。

基线方法。`DUSt3R`作为最接近本工作的基线，在视觉里程计与三维重建基准中表现优异。我们额外对比其改进方案：`Spann3R` 通过外部空间记忆的序列化帧处理替代原耗时的全局对齐阶段（实验采用其offline1模式），以及`Fast3R`——作为 `DUSt3R` 的多视图扩展版本，通过并行处理实现可扩展重建。

## 结果


对于这些在传统通用数据集上的训练结果，个人认为探讨意义不大，故而此处不再赘述，直接截图上表。

- 表1：在两个室内数据集7Scenes和NRGBD上的量化结果，展示了本方法与DUSt3R之间的绝对提升值($\Delta$)。对于标注↑的指标，正值表示改进；而对于标注↓的指标，负值表示改进。
![Table_1](/img/DUSt3R-and-MONO3R/Table_1.png)

- 表2：在DTU物体级数据集上的量化结果表明，本方法在所有评估指标上均一致优于基线方法。
![Table_2](/img/DUSt3R-and-MONO3R/Table_2.png)

- 表3：在ETH3D户外数据集上的量化结果显示：本方法在RRA指标上具有显著优势，RTA指标表现相当，同时整体精度保持竞争力
![Table_3](/img/DUSt3R-and-MONO3R/Table_3.png)

- 表4：在 Tanks & Temples 数据集上的定量结果显示：本方法在RRA指标上具有显著优势，RTA指标达到最佳性能，并以最高mAA分数保持整体精度领先地位。
![Table_4](/img/DUSt3R-and-MONO3R/Table_4.png)

- 表5：在DTU数据集上的消融实验表明：$N=2$ 的配置与主要实验设置相同，仅训练样本对数量存在差异。
![Table_5](/img/DUSt3R-and-MONO3R/Table_5.png)

