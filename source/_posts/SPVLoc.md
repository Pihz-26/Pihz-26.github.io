---
title: SPVLoc：基于语义全景视口匹配的相机定位技术 —— 面向未知环境的六维位姿估计
data:  2025-10-18 
mathjax: true
tags:
    - 三维视觉
    - 深度估计
    - 学习笔记
---
本文为对于论文《SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments》 的翻译与精炼，该论文发表于2024年的 `ECCV`上。

## 基础摘要

通过透视相机拍摄RGB图像，在室内环境全景语义布局表征中，实现对于视口的定位。该全景图仅仅包含室内的近似结构以及门窗标注的无纹理3D参考模型渲染生成。文章中，通过简单的卷积网络结构实现了图像-全景图匹配，并最终达成了图像-模型匹配。基于视口分类打分后对参考全景图进行排序，为查询图片找到最佳匹配项，进而估算选定全景图与查询图像之间的相对位姿。

整体分析而言，该论文方法分为四个模块：

1. 语义全景视口匹配（Semantic Panoramic Viewport Matching）
2. 基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）
3. 优化
4. 推理

## 方法介绍

**SPVLoc** 方法本质上是通过给定一个二维的RGB平面图在一个无纹理的语义三位建模场景（室内）中推导出相机的六维位姿定位。模型使用的三维模型是 `Structured3D` 标注格式，其可直接从建筑模型中生成，也可以通过AI工具基于平面图自动推导获取。

SPVLoc首先通过跨域图像-全景图匹配，利用语义场景模型在渲染的全景图中估算图像的视口位置。接着根据分类准确度对这些预测结果进行排名，并针对匹配度最高的参考全景图执行相对6D姿态回归。

### 语义全景视口匹配（Semantic Panoramic Viewport Matching）

#### 问题定义

本论文的目的是为给定查询的RGB图像，获取其相对于语义三维模型的6D位姿。而本论文对此将整体任务转变为了跨域图像-全景图匹配任务。

该模型通过一组N个等矩形参考布局 $L=\{L_i|i=1,...N\}$ 表示，每个布局单通道中包含语义标签。每个渲染图完整记录了特定视点周围的全景环境。视口 $V$ 是从全景图中截取的独立区域，精确定位了附近相机所能拍摄的实际视野范围。视角 $V_{P_i}$ 的透视畸变包含 $L_i$ 与 $I_q$ 之间的相对姿态偏移 $P_i$ 的信息。通过评分和排序视口有助于选出最优全景图 $L^*$。绝对姿态 $P$ 由 $P^*$ 与 $P_i$ 串联获取。

#### 视口预测

视口掩码 $V_P$ 是二值图，用于标示等矩形全景图中各点在透视相机视角下的可见性。其计算过程设计将全景图的所有3D点投影至透视图，该投影基于训练阶段已知的底层布局完成。$V_P$ 掩码通过以下方式确定：结合3D相机位姿信息，并剔除被遮挡的部分，识别能投影到像素透视图的像素点。

此外，二维边界框 $V_{bb}$ 是包围 $V_P$ 的最小轴对矩形框。该边界框在 $360^{\circ}$ 全景图中采用循环定义的方式，允许右侧溢出的部分从全景图左侧重新进入。每个预测的边界框都关联一个置信度分数 $C_{bb}$，用于量化该边界框作为有效的匹配概率。网络的目标是为每对输入的予以全景图和查询RGB图像预测 $V_P$ 、 $V_{bb}$ 和 $C_{bb}$ 。当给定一组全景图像时，具有最高综合置信度分数 $C_{bb}$ 的预测边界框即为最优观测视点。

#### 神经网络结构

神经网络具体结构如下图

![Network architecture](/img/SPVLoc/Network_architecture.png)

详细结构分析如下：

1. **图像编码**：查询图像 $I_q$ 通过 `EfficientNet-S` 主干网络编码，保留第四层和最后一次下采样层的特征输出。两组特征经空间尺寸归一至 $7 \times 7$后拼接为张量 $F$。
2. **全景编码**：语义全景L通过 `DenseNet-121` 主干编码（延伸至第五下采样块）提取特征 $R$。图像特征通过深度互相关操（标注为 $⋆d$ ）作引导全景分支预测视口信息：
   - 特征 $F$ 经两个连续卷积块（含卷积+批归一化+ELU激活）处理，生成 $3\times3$ 滤波器组 $\hat{F}$ 用于执行 $R⋆d\hat{F}$
   - 对 $F$ 进行全局最大池化压缩后，与R进行相乘和相减运算，生成三重图像信息注入特征.
3. **特征融合**：各注入特征经独立卷积块处理后拼接，最终通过卷积块生成互相关特征 $R*$
4. **特征解码**：解码器预测全景场景中透视相机视口的投影范围。
   - 全景图像中的视口通过边界框进行检测，采用类似 $RetinaNet$ 的设计框架，包含边界框分类头(`BBox Class.`)和回归头(`BBox Reg.`) 。
   - 边界框分类头(`BBox Class.`) 采用焦点损失函数 $\mathcal{L}^{VP}_1$ 进行优化，而边界框回归头(`BBox Reg.`)则使用平滑L1损失函数 ${\mathcal{L}^{VP}_2}$ 。边界框分类头为预测结果提供置信度评分，确保概率最高的边界框即为最优视口估计。
   - 此外，通过另一个卷积解码器头预测透视相机的精确视窗区域$V_P$，并采用二元交叉熵损失函数 ${\mathcal{L}^{VP}_3}$ 进行优化。
5. **透视监督**：编码后的图像特征 $I_E$连接到卷积解码器，输出基础语义图和法线图，以确保提取出与图像内容紧密对应的特征。语义图通过交叉熵损失 $\mathcal{L}^{VP}_4$ 进行学习，法线图则采用余弦损失 $\mathcal{L}^{VP}_5$ 进行优化。

### 基于特征相关性的位姿回归（Feature-Correlation-based Pose Regression）

关联特征（记为 $R^\star$）不仅编码了查询图像 $I_q$ 在局部全景图 $L_i$ 视窗内的信息，还应推导出两者之间的相对位姿偏移 $P_i$。由于合成生成的 $L$ 已知 $L_i$ 的精确位置，因此确定 $I_q$ 相对于 $L_i$ 的位置即等价于确定其绝对位置。所有渲染的全景图均具有统一朝向（水平对齐且指向正北），故旋转差值直接对应相机的绝对旋转。

位姿估计头（Pose Head）通过处理 $R^\star$ 来预测相机相对于全景渲染的位姿偏移（如下图所示）。

![Pose_head](/img/SPVLoc/Pose_head.png)

首先使用三个卷积块（前两个接最大池化层）压缩特征，展平后的特征形成一维嵌入 $r^\star \in \mathbb{R}^C$（对于256×128的全景图，$C=640$）。可选地，为处理训练与测试时相机的不同视场角，$r^\star$ 会经归一化后乘以水平视场角（弧度制）。结果输入四层MLP，输出查询图像位置的3D平移偏移量 $\hat{t_i} \in \mathbb{R}^3$ 以及旋转分量（以单位四元数的对数形式表示 $\hat{r_i} = \log(\hat{q})_{ij} \in \mathbb{R}^3$  [41]）。
MLP通过两种损失函数优化：$\mathcal{L}_{P1}$ 评估相对平移精度，$\mathcal{L}_{P2}$ 评估最终旋转估计准确性。二者均采用L1距离计算。

### 优化

训练损失由两部分组成：MLP的两个损失函数和视口估计的五个损失函数。由于涉及七项损失，论文中采用多任务学习策略，通过为每项损失分配不同权重实现平衡。利用指数映射并基于各任务的同方差不确定性（homoscedastic uncertainty）和可学习权重因子进行加权。最终总损失表达为：

$$\mathcal{L} = {\sum_{i=1}^{2} {\mathcal{L}^{P}_{i}} {e^{-\beta_i}}} + \beta_i + {\sum_{i=1}^{5} \mathcal{L}^{VP}_i e^{-\gamma_i}} + \gamma_i$$

其中：
- $\beta_i$ 和 $\gamma_i$ 是可学习的对数权重参数
- 指数项 $e^{-\beta_i}$ 和 $e^{-\gamma_i}$ 动态调整各项损失的贡献度
- 附加的 $\beta_i$ 和 $\gamma_i$ 项用于防止权重过度衰减

### 推理

在全景定位过程中，虚拟相机的位置通过二维固定网格（覆盖于平面图上）确定。网格参数包括：

- 高度参数：虚拟相机高度固定为 $h_{\text{pano}}$
- 网格范围：由 $xy_{\text{pano}}$ 定义
  为确保有效的参考图像，仅对房间地板以上的区域进行采样。网格范围 $xy_{\text{pano}}$ 应尽可能扩大，以减少全景参考图数量（因每次推理均需执行特征关联与边界框回归计算）。

#### 位姿估计流程

1. 参考帧筛选：选取边界框分类得分最高的前 $n$ 个参考位置
2. 绝对位姿计算：
   - 位姿估计头（Pose Head）输出相对位姿偏移
   - 叠加所选参考位置的坐标，获得最终绝对平移量
3. 预生成优化：
   - 为平面图预渲染全景参考图及其编码特征 $R$
   - 仅存储 $R$ 可大幅简化推理流程

#### 位姿优化（Refinement）

在不改动网络架构或训练流程的前提下，SPVLoc 可通过迭代提升位姿精度：

1. 首轮估计：获取初始位姿结果
2. 二次渲染：在估计位置生成新参考全景图，重新执行相对位姿回归
3. 精度增益：由于此时初始估计已接近真实目标，第二轮的相对位姿回归问题更简单，从而输出更精确的优化位姿

